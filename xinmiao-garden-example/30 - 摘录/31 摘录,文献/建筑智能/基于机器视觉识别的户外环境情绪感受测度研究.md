---

current-status: 待完成
标题: 基于机器视觉识别的户外环境情绪感受测度研究
创建时间: 2023-03-14 14:44
修改时间: <%+ tp.file.last_modified_date() %>
技术: 卷积神经网络
类型1: "文献阅读"
tags: 卷积神经网络
参考价值: 3
用途: 模板
对象:
主张:
意义:
框架:
---

---

```ad-info
title: <u></u>**描述**
collapse: open
color: 233, 244, 240

**◀️ 父节点| ▶️ 子节点** 

🪁 status: #🔖 
🎏 class: #📸  

description :: 

来源:: [全文阅读--XML全文阅读--中国知网](https://kns.cnki.net/KXReader/Detail?invoice=UHHqwpF1zDZbDNl4dyugWdZ6DgQsusBbQE6iM5wsLbDrBMwJCKZr0DIK13CXsf0y7p9o24IBJgmzaKHtIeUNja4p8zsobHobAany%2BB2tCMFEzve05e6CpnqGomUozsW6NpW6m%2BRtc0BZP61RTzHaQv8ViJSC79zSV%2FTLRjNJ1y8%3D&DBCODE=CJFD&FileName=JGSJ202105005&TABLEName=cjfdlast2022&nonce=58A2D202C1314C178FFF174E9716807A&uid=&TIMESTAMP=1678776169761)

论文类型:: 

作者:: 

学科:: 理论物理

导师:: 

学术机构::

时间:: [[2012]]

关键词::

创新:: 

理论意义::

实际意义::

---

研究背景::

研究目的::

研究问题::

研究空白::

假设::

---

研究框架::

研究领域::

研究理论::

研究方法:: [[卷积神经网络]]

统计分析方法:: 

信息分析方法::

指标参数分析方法:: 

结果分析方法:: 

---

研究对象::

条件环境::

方案步骤::

数据获取::

数据特性::

---

研究结果::

研究发现::

研究展望::

研究结论::

局限::

📎
```

### [1. 四川农业大学风景园林学院](https://kns.cnki.net/kcms2/organ/detail?v=3_oUXeZjdDz03Rn1ilmZgOWuD6GFFYkZmhST_xA0PgEXKZCqu4VR8RKZqvMbPSVJL2nYorWie5HmPMH_aGhv27cAIdFW4_IGwbvyXQgNegyaHfSEoHScn37qGlhA1iwzQQQ9afqBn5A=&uniplatform=NZKPT)[2. 华南师范大学华南先进光电子研究院彩色动态电子纸显示技术研究所](https://kns.cnki.net/kcms2/organ/detail?v=3_oUXeZjdDz03Rn1ilmZgOWuD6GFFYkZVGf0j1RIYxalfO5HDqkuH-ZSkz8Tnqwh0oXs5Y0zdwelUPyX5-XFpTYrYE_GEeMqvfCSwlKQbukDCCX1T6mYecrOsQixqrUOAiuMX3mgUr6YbStMXbF16aIDziamGOTeKY7xUAYiBCnhEk7I2-fqzAv9Ib9P5_8oQZhFmsLj-5D1P0R6_qexFQ==&uniplatform=NZKPT)[3. 四川农业大学风景园林学院教授](https://kns.cnki.net/kcms2/organ/detail?v=3_oUXeZjdDz03Rn1ilmZgOWuD6GFFYkZmhST_xA0PgEXKZCqu4VR8RKZqvMbPSVJL2nYorWie5HmPMH_aGhv2578h1lz6qW6HpWn2yuWnQZyAv7MnNMAqPQ58EDmYJ1uBHs7isXRGnVjP5bPMcxZXA==&uniplatform=NZKPT)

摘要：良好的户外环境有利于改善人群身心健康,环境中个体情绪感受的实时测度能客观反映人群对户外环境品质的满意程度,但目前应用于户外环境中的情绪感受测度方法还较少,且无法在场地面积大、样本量较多的情境下实现大范围的高效测度。人工智能领域的机器视觉识别可通过视频数据实现对动态面部表情特征的准确识别,使得开展高精度、长周期的户外环境情绪感受测度成为可能。本文以城市社区广场环境为实证实验场地,基于卷积神经网络算法模型,同步收集人群在户外环境体验时的面部视频数据和皮电数据,训练生成和检验测试可判别户外环境中个体面部情绪的深度学习算法模型——编解码器-SVM优化模型,并在街头绿地环境中展开拓展实验检验模型可靠性。研究结果表明:1)实证实验和拓展实验中的人群情绪感受测度准确度分别达到82.01%、65.08%;2)拓展实验证实该算法模型具备推广应用潜力;3)模型对于人群行为丰富、视野开阔的空间更具适用性。因此,基于机器视觉识别的户外环境情绪感受测度将有助于在场地面积较大、样本量较多的情境下揭示人群在环境体验中的心理状态,提升景观品质优化策略的有效性,同时也将为存在沟通或阅读障碍的特殊人群的情绪感受识别提供技术支撑,有望推动智慧城市建设的精细化转型。 

关键词：

[机器视觉识别;](https://kns.cnki.net/kcms2/keyword/detail?v=3_oUXeZjdDz0sWFvw1N8YsV93pMjO8V6aDl8-88FC2ddLxfORndVmBdwxdkcAUo079uN3bQ29CUO5K91TFGLp59XB20tZzdbi3G3D_dAtElFFh4xfHfdIxHCX15q4QzgyShLxgKpsFPa1mCqYJvCvQ==&uniplatform=NZKPT)[户外环境体验;](https://kns.cnki.net/kcms2/keyword/detail?v=3_oUXeZjdDz0sWFvw1N8YsV93pMjO8V6aDl8-88FC2d4eM5FYWq74ta72VaK3Y3UBqAHZ7g6LNGRKT1RBPjbmhny_2NHuLilZp_kJ6d8ZuH3EvlmYBR3C89dzGa8GGoWVMciCC6_UwA_xyC58M2S3Q==&uniplatform=NZKPT)[情绪感受测度;](https://kns.cnki.net/kcms2/keyword/detail?v=3_oUXeZjdDz0sWFvw1N8YsV93pMjO8V6aDl8-88FC2e11K3-6lqAPKFB3GaDGf_kGRxxE6ft82iqob8gIFg-DoyO9GLq8qPtWyTguiAN7PvvBgpsfqz2Joa75Amb7MPVZq1DnD9Sptg3CJ9pYsVaow==&uniplatform=NZKPT)[卷积神经网络算法模型;](https://kns.cnki.net/kcms2/keyword/detail?v=3_oUXeZjdDz0sWFvw1N8YsV93pMjO8V63H0PWciJD9yolEuzvUoRlqDNdnDs0fgaisPI5oWgesj4O22uLeyU3up_390VhlaV2lbe50q3ko-lzyz3aCJlEuaBD6gEiX8O_TNKwiR6jfBtiKkBLP-60vaXO50agVWkylPXcoA9bK0cCaHGe4VQ3z_daR7CjbNhlwcRpMxjKe8=&uniplatform=NZKPT)[健康促进;](https://kns.cnki.net/kcms2/keyword/detail?v=3_oUXeZjdDz0sWFvw1N8YsV93pMjO8V63H0PWciJD9yOmZAQ06Oeek49lbpGqVaZLtTHBXKyp3TF65xzNI2wbotFdHyrBo1qKxzGQqH1XIRJR3bipVgLehUtw5paMThC&uniplatform=NZKPT)[设计研究;](https://kns.cnki.net/kcms2/keyword/detail?v=3_oUXeZjdDwB3LPuUmAXsCUJFe6LddCmkb9enGt6RZr00jeHf7DjwW10MT0u7y8zWoTAZlBp4CwllL3b3HT3wcZdMhKfutnC0MTi_hlNV2r3fdc6lPdDcUAjQH0P2LSV&uniplatform=NZKPT)

-   专辑：
    
    工程科技Ⅱ辑;信息科技
    
-   专题：
    
    建筑科学与工程;计算机软件及计算机应用
    
-   分类号：
    
    TU984.12;TP391.41

Abstract：

Outdoor environments with quality landscapes can benefit people's physical and mental health. Real-time assessment on individuals' environmental affective experience can improve the scientism in measuring the quality of outdoor environments. Existing measurement methods are often insufficient for the cases of a larger site area or sample size. The machine visual cognition of Artificial Intelligence can realize the recognition of facial expressions and the changes in video images, which supports high-precision and long-cycle measurements on individuals' affective experience in outdoor environments.Taking an urban community square as the study site, this research simultaneously collects participants' facial data from video images and their electrodermal activity data,wherein Convolutional Neural Network algorithm model is trained with a deep learning algorithm, i.e. codec–SVM optimized model, whose reliability is tested through an additional experiment. The research reveals that: 1) The accuracy rate of the main and additional experiments in measuring individuals' affective experience is 82.01% and 65.08%, respectively; 2) The additional experiment verifies the application potential of the codec–SVM optimized model; And 3) the model works more effective for outdoor scenarios with varying usage behaviors and open views.Therefore, machine visual cognition can be used for emotion measurement in a larger site area or sample size and contributes to the effectiveness of landscape optimization efforts, especially as an instrumental tool to study the affective experience of the ones who have communication or reading disability. The findings also demonstrate the model's great potential in building Smart Cities with refined public services.

Keyword：

[Machine Visual Cognition;](https://kns.cnki.net/kcms/detail/knetsearch.aspx?dbcode=CJFD&sfield=kw&skey=Machine%20Visual%20Cognition&code=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!) [Outdoor Environments Experience;](https://kns.cnki.net/kcms/detail/knetsearch.aspx?dbcode=CJFD&sfield=kw&skey=Outdoor%20Environments%20Experience&code=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!) [Affective Experience Measurement;](https://kns.cnki.net/kcms/detail/knetsearch.aspx?dbcode=CJFD&sfield=kw&skey=Affective%20Experience%20Measurement&code=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!) [Convolutional Neural Network Algorithm Model;](https://kns.cnki.net/kcms/detail/knetsearch.aspx?dbcode=CJFD&sfield=kw&skey=Convolutional%20Neural%20Network%20Algorithm%20Model&code=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!) [Health Promotion;](https://kns.cnki.net/kcms/detail/knetsearch.aspx?dbcode=CJFD&sfield=kw&skey=Health%20Promotion&code=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!) [Design Research;](https://kns.cnki.net/kcms/detail/knetsearch.aspx?dbcode=CJFD&sfield=kw&skey=Design%20Research&code=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**Received：** 2021-05-31

### 1 研究背景

情绪是人脑基于主体需要对客观环境与事件刺激的反应[1]，可以通过生理、心理和社会适应等多种途径作用于健康[2]。快节奏、高强度的现代城市生活方式容易诱发焦虑、抑郁、不安等不良情绪，极大地损害了人群身心健康[3][4]，而景观品质较好的户外环境能够缓解人群不良情绪，对于改善人群身心健康具有积极作用[5][6]。近期，环境心理学相关研究证实，针对情绪感受的实时测量能更客观地反映人群对户外环境景观品质的满意程度[7]，有利于增强环境改造/提升措施的有效性。然而，既有的情绪感受测度方法多应用于室内环境，户外环境应用较少，且不适用于在场地面积大、样本量较多的情境下对使用者情绪感受展开大范围测度。因此，如何提高户外环境情绪感受测度的实时性和推广性还有待深入探讨。

### 2 户外环境情绪感受测度方法

#### 2.1 既有户外环境情绪感受测度方法及其局限性

户外环境情绪感受是个体根据户外环境对自身需求的满足程度而产生的综合状态，包括了主观体验、生理反应及外部表现三个方面[8]，其测度方法通常可分为自我报告、生理测量及行为测量三种类型[9][28][46]。自我报告一般为被试者利用情绪评定量表或问卷——常用的情绪评定量表有积极消极情绪量表（PANAS)[10]、心境状态量表（BPOMS)[11]、状态特质焦虑量表（STAI)[12]等——对自身的情绪感受进行描述或等级评价，以此测度其在户外环境中的主观体验。生理测量可通过对人体皮电（EDA）、心率、血压、肌电、脑电波等生理指标[13][14]的监测，反映伴随情绪发生的复杂神经过程和生理变化，常用于测度被试者的瞬时情绪变化。行为测量通常对被试者的声音特性、面部表情、躯体行为等外显行为进行直接观察和测度来衡量个体情绪感受[15]。

为厘清上述方法在户外环境情绪感受测度过程中的优劣势，本研究参考相关文献[16]从适用范围、干扰程度、时间成本、设备成本和人力成本五个方面对三类方法进行比较（图1，表1）。从比对结果看，自我报告适用范围广、测度干扰程度低、设备及人力成本较低的情况，具有简单易行的特点，但其测度周期往往较长，不适用于以提升使用者满意度为目标、需在实践过程中对使用者情绪感受进行实时测度的设计[17,18,19,20]。而生理测量虽然测度周期较短，但存在单人次的测度耗时较长，不适用于大样本量的情绪测度[21,22,23,24]。此外，生理测量还需提供相应的仪器设备，测度成本较高，同时穿戴设备本身也会对被试者被测度时的情绪感受造成一定干扰[25]。行为测量的常用方法有观察者评价[26]、叙事陈述[27]等，虽然可以对被试者的情绪感受进行直观测量，但往往测度耗时较长，且需要大量劳动和培训[28]。鉴于此，随着城市居民对建成环境健康促进功能需求的不断提高，需要引入新的技术手段以开展更高效、更广域、更实时的环境健康情绪测度。

  
  

表1:既有户外环境情绪感受测度方法比较  下载原图

Table 1:The comparison of existing measurement methods on affective experience in outdoor environments

![表1:既有户外环境情绪感受测度方法比较](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_56500.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

注1.评价得分说明：1分表示优势较弱，2分表示优势较强，3分表示优势最强；2.实验周期由单人次实验时间及样本量估算得出。

NOTES1.Score:1 means a slim superiority,2 means a strong superiority,and 3 means an outstanding superiority;2.The measurement time is estimated upon the time spent on each participant and the whole sample size.

![1. 既有户外环境情绪感受测度方法比较雷达图](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_61600.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1. 既有户外环境情绪感受测度方法比较雷达图 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_61600.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1. The rader chart of existing measurement methods on affective experience in outdoor environments

#### 2.2 机器视觉识别在户外环境情绪测度中的应用

机器视觉是机器对人视觉功能的模拟，即通过计算机从图像中提取有效信息进行分析处理，实现对图像内容的识别[29]。该技术具有精度高、实时性强、自动化与智能化程度高[29]等优点，已在公共安全[30][31]、智能驾驶[32][33]、医疗健康[34][35]等领域广泛应用。

在环境情绪感受测度方面，由于面部表情是人们传达情绪最直接的方式[36]，基于机器视觉识别的情绪分析也多集中在面部表情上，主要包括人脸检测、面部特征提取、情绪分类三个步骤[37]。其中，面部特征提取和情绪分类是决定情绪识别结果的关键[37]。目前，具有强大特征学习能力的卷积神经网络（Convolutional Neural Network，简称CNN）算法模型已能通过计算机自动分析出视频中面部表情及其变化，实现对动态图像的面部情绪识别[38]，极大地提升了机器视觉面部情绪识别的精确度和智能化程度[39]。该技术不同于对静止图像的识别，基于视频数据的面部情绪识别可以追踪和提取个体具有时空连续性的多幅图像信息[40]，从而对个体情绪感受进行更加客观全面的识别和测度。在现有的环境情绪感受测度研究中，机器视觉识别主要应用于室内和室外环境。在室内环境方面，维尔达·巴纳·普特拉等人基于机器视觉识别构建了实时面部表情识别系统，用于实时测度课堂中学生的情绪变化，以便及早发现学生的不良情绪[41]；平金元将机器视觉技术和皮电数据相结合，建立了可测度学生心理状态的算法模型，用以实时反馈学生的课堂参与度[42]。在户外环境方面，康宇豪等人从社交媒体网站中收集了80个带有地理坐标的旅游景点照片，利用机器视觉识别分析照片中的面部情绪，探究人群环境情绪感受与环境因素之间的关系[43]。

因此，相较于既有的户外环境情绪感受测度方法，机器视觉识别具有独特优势。从适用范围看，机器视觉识别对于测度场地的限制较小，既能应用于实验室环境[44]，也能应用于不同尺度及景观特征的户外环境[42][43]；从干扰程度看，机器视觉识别通过非接触的方式获取个体面部图像信息[41][45]，可最大限度减少对被试者的干扰；从实验成本看，模型的训练阶段需要技术支持，而模型训练完成后的数据采集过程可以通过摄像头采集等手段实现[46]，设备成本、时间成本及人力成本均较低，有较好的推广应用价值。

但是，机器视觉识别在户外环境的情绪感受测度方面应用还较少；而且，鉴于户外环境具有环境要素复杂、人群姿态多样、光照条件多变等特点，当前的机器视觉面部情绪识别所使用的算法模型的初始识别率通常较低[47]。基于此，本研究最终采用了对于复杂户外环境中的面部情绪识别具有较强适用性的CNN算法模型[48][49]，相较于传统聚类模型、贝叶斯分类模型为代表的机器学习模型[50][51]，该模型具有较强特征提取和分类能力，可以通过卷积层自动完成特征提取，并借助深度学习训练优化模型参数，从而更为精确地实现对个体户外环境情绪感受的识别。

#### 2.3 研究对象与假设

基于以上认识，研究选取了城市居民开展日常户外活动较为频繁的空间载体——城市社区广场为样本，通过开展机器视觉情绪感受测度实验，在CNN算法模型基础上，训练生成可判别面部情绪的深度学习算法模型，实现并验证了基于机器视觉识别的户外环境情绪感受测度的可行性，为引入该技术方法辅助景观设计实践提供了实证支持。

### 3 实证研究

#### 3.1 实验设计与数据获取

本文选取成都市成华区锦绣社区睦邻广场为研究场地，开展基于机器视觉识别的户外环境情绪感受测度实验。如表2所示，锦绣社区睦邻广场北临城市次干道、南临居民区、东侧为锦绣社区服务中心、西侧为社区街头绿地，面积约为1.8hm2。广场内视野开阔，拥有植被、廊架、座椅、铺装等景观要素，是典型的城市开敞空间。为保证全方位获取视频人脸数据，研究者在广场中布置了9个摄像机位。为减少天气波动对数据采集的影响，实验于天气晴朗、温度适宜时段开展（2019年9月19—21日，08:00～12:00）。

参考既有情绪感受测度相关研究，一般实验被试人数为30人以上时即认为数据可靠[52]；为提高情绪识别模型的适用性，还应避免被试者的人口特征过于单一，因此本次实验共招募42位不同性别和年龄的锦绣社区居民作为被试者1。单人次实验步骤包括（图2):

1）准备阶段：被试者食指和中指佩戴便携式PPG-EDA生物反馈装置2并行走适应，其间研究人员向被试者介绍实验流程及注意事项；

2）初始阶段：被试者在等候区静坐休息10分钟3[53][54]，使被试者情绪恢复平静，以测量其初始皮电数据；

3）应激阶段：被试者被要求在封闭的车内边听噪音边做算术题3分钟4（此应激阶段是为了诱发被试者的焦虑情绪，使所有被试者处于同一情绪状态以减少实验误差并增加结果可比性）；

4）体验阶段：被试者进入研究场地按日常习惯进行环境体验5分钟5[55][56];

  
  

表2：研究场地基本环境情况  下载原图

Table 2:The conditions of the Jinxiu Community Square

![表2：研究场地基本环境情况](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_61700.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

![2. 单人次实验流程图](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_61800.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

2. 单人次实验流程图 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_61800.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

2. Experiment procedure for individual paripants

5）实验结束：被试者需填写问卷，并在研究人员协助下解除设备、离开场地。

单人次实验流程需20分钟左右，摄像机从实验初始阶段开始录制视频，直至实验结束。本实验获取被试者在研究场地中进行环境体验的同步皮电数据（Biopac MP150便携式穿戴生理记录分析系统中的PPG-EDA模块检测皮肤电SCR变化值）、视频人脸数据和问卷调查数据共42组。

#### 3.2 实验过程及测度结果

#### 3.2.1 情绪定义与数据集准备

机器视觉识别需要对数据有较为准确的情绪定义才能进行视频情绪分析。既有研究中对情绪定义的方式主要有三种：1）直接激发被试者产生某种特定情绪并以此定义；2）在实验后记录被试者的情绪自我描述完成定义；3）根据被试者在实验中的生理数据反映完成定义[57]。前两种定义方法中分别引入了理想化判断及主观判断，因为被试者在指定实验条件刺激下不一定能激发期望情绪，而人们在情绪自我描述时由于具有先验条件又可能会产生错误描述；且前两种方法只有在持续受激的条件下才能完成情绪定义。因此，本研究以实验获取的视频人脸数据为对象，用同步采集的被试者皮电生理数据（第三种方法）对其进行情绪定义。

![3. 被试者应激阶段和体验阶段皮电数据对比](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_61900.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

3. 被试者应激阶段和体验阶段皮电数据对比 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_61900.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

3. EDA data comparison between the Stressor and Experiencing phases

![4. 被试者应激阶段和体验阶段皮电数据配对样本t检验](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62000.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

4. 被试者应激阶段和体验阶段皮电数据配对样本t检验 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62000.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

4. The matched samples t-test of EDA data at the Stressor and Experiencing phases

![5. 去噪平滑处理前（左图）后（右图）被试者皮电数据波动图示例](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62100.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

5. 去噪平滑处理前（左图）后（右图）被试者皮电数据波动图示例 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62100.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

5. A sample of EDA data fluctuations before (left) and after (right) denoise and smooth

首先，对皮电数据在空间体验前后的均值进行配对样本t检验（应激阶段0.02±0.38，体验阶段0.47±0.13,p<0.01，小于α=0.05），证明本研究选取的样本空间具有显著的情绪缓释效用（图3,4）。然后，使用Savitzky-Golay滤波器，采用就近原则将每位被试者的皮电数据进行去噪平滑处理（图5），并参考相关文献中监督实验所得情绪与皮电数据模型的峰值匹配方法[57]将本研究中的情绪分为三类：将皮电数据几乎无波动的数据模式定义为“平静”情绪；将15秒区间内呈现密集且多段波动幅度适中的数据模式定义为表积极特征的“开心”情绪；而呈现突然性波动的数据模式则被定义为“不开心”情绪（图6）。

![6. 本研究情绪定义模式图样](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62200.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

6. 本研究情绪定义模式图样 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62200.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

6. The samples of emotion labels in this study

最后，设定每15秒长度的视频为一个面部表情，每个面部表情之间有5秒间隔。实验模型训练使用的数据集通过OpenCV软件库中的CascadeClassifier级联分类器获得，该分类器可以获取图像的哈尔特征，而后对图像中的人脸区域进行标定裁剪（在人脸图像角度不超过90°的情况下亦可被裁剪，不存在图像丢失问题）。由此共获得具有情绪定义的面部图片57 600张，作为后续模型训练的数据集。

#### 3.2.2 情绪特征模型训练

(1）模型定义

最初的人脸图像由固定大小的像素矩阵构成，当该矩阵展开为一维矩阵时，会导致矩阵数据量较大，很难直接找到同类与不同类图像之间的关联与区别。本研究选择以CNN为基础的编解码器模型结构——编解码器-SVM优化模型（图7），通过卷积与反卷积配合完成特征的提取与拟合。其中，卷积可利用不同的卷积核提取图像不同的纹理特征进而实现图像的分类，而反卷积则是以恢复原图像为目的进行卷积的逆过程。编码器通过卷积提取的特征向量再输入反卷积构成的解码器，而后通过网络训练，直至输出图像与输入图像之间的交叉熵不断减小至平稳，可认为此时编解码器中间的特征向量最能体现图像特征。该图像特征向量不仅具有代表性还具有生成性，而普通CNN训练得到的图像特征向量则缺少了生成特性。因此，本实验选用编解码器–SVM优化模型进行图像特征提取以及之后的模型训练，以此为基础的模型结构可实现人脸(6)差异较大情况下的情绪识别，更适用于人群特征多样的户外环境情绪测度[58]。

![7.“编解码器-SVM优化模型”网络结构图](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62300.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

7.“编解码器-SVM优化模型”网络结构图 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62300.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

7. The structure of codec–SVM optimized model

(2）模型训练

由于本研究在没有先验标签的情况下进行模型训练，需要利用基于Savitzky-Golay滤波器得到的类别标签与该模型训练得到的标签作验证，以获得最终的识别率。为了保证两项实验者数量一致，模型训练集和测试集的比例为5:5，即从数据集中随机抽取21位被试者的面部图像作为训练集，剩余21位被试者的面部图像为测试集。应用编解码器-SVM优化模型进行图片特征提取和智能识别，期间不断优化算法参数以提高识别率。每次同时输入32张112×112维的图像进行同步训练，设定学习率为1e-3。由于涉及训练解码器所对应的模型生成特征，模型的训练部分耗时较久，过程如图8所示。

本研究模型训练共使用了28 800张图像、耗费了120个时期（epoch），得到具有图像特征的中间向量，之后训练SVM分类器以完成模型的训练过程。SVM分类器利用内积核函数代替向高维空间的非线性映射，可以增强模型的鲁棒性，避免了由于多全连接层带来的训练损失以及过拟合[59]。训练过程如下：首先调整核函数γ的选取，最高识别率0.9771发生在γ为7～8.5范围时，微调后三种分类核函数（即Gauss函数、Poly函数和Sigmoid函数）对应的最佳γ值分别为8.935、8.935和2（图9）。在SVM分类过程中不同的核函数对应了不同的分类模型，在输入数据特征未知的情况下，实验尝试对不同的核函数及其取值进行对比，以寻找具有更高分类准确率的编解码器-SVM优化模型。在此基础上实验调整损失系数c获得最终的情绪识别模型（图10），最终确定在核函数为Gauss函数分布模式下γ值为8.935,c为15，识别准确度为0.9783。

#### 3.2.3 视频情绪识别分析

运用训练好的模型对人脸图像进行表情预测，将数据集中剩余21位被试者的面部图像作为测试集进行视频情绪识别分析[60][61]（共28 800张，未添加情绪标签）。其中图像的特征提取与智能识别借助CascadeClassifier级联分类器完成（延展规模设置为160×160像素，比例系数为7，检测的最小尺寸和最大的尺寸分别100×100像素和1 000×1 000像素），视频的分辨率为每秒30帧，每一帧图像都会进行一次面部情绪识别。将模型对于视频情绪识别的结果与同步获取的皮电数据匹配，最终准确率为82.01%（图11），其中，“平静”“开心”“不开心”三种情绪类别对应的样本数量分别为105、78、6（图12）。

![8. 模型训练过程图](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62400.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

8. 模型训练过程图 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62400.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

8. Model training

![9. S V M分类器在训练集上的识别准确度随不同核函数类型以及γ值的变化](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62500.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

9. S V M分类器在训练集上的识别准确度随不同核函数类型以及γ值的变化 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62500.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

9. Changes of recognition accuracy of SVM classifier on the training set correlating with kernel function types andγvalues

![1 0. SVM分类器在确定了γ值后识别准确度随c值的变化](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62600.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1 0. SVM分类器在确定了γ值后识别准确度随c值的变化 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62600.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1 0. Changes of recognition accuracy of SVM classifier correlating with c values after theγvalue is set

![1 1. 测试集上机器视觉情绪识别准确率](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62700.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1 1. 测试集上机器视觉情绪识别准确率 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62700.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1 1. Emotion recognition accuracy rate of the testing set via machine visual cognition

![1 2. 测试集的情绪分类识别准确率](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62800.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1 2. 测试集的情绪分类识别准确率 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62800.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1 2. Recognition accuracy rate of each emotion labels of the testing set

#### 3.3 拓展实验检验

为了进一步验证此算法模型是否可适应多样的户外环境，2020年9月研究团队选取成都市成华区锦绣社区一处街头绿地进行了拓展实验。该街头绿地面积为0.6hm2，主要景观要素有植被、健身设施和硬质铺装，其中植被以乔木为主，空间的郁闭度较高。为保证全方位获取视频人脸数据，实验共布置了8个摄像机位（图13,14）。招募21位社区居民作为被试者，按照和图2相同的实验步骤获取21组数据。运用本研究得到的编解码器-SVM优化模型对21位被试者的视频数据进行情绪分析，结果显示和同步获取的皮电数据匹配准确率为65.08%（图15）。其中，“平静”“开心”“不开心”三种情绪类别对应的样本数量分别为122、60、7（图16）。

### 4 讨论

#### 4.1 结果分析

本研究提出的编解码器-SVM优化模型在两类不同景观特征的户外环境情绪测度中准确率分别达到82.01%和65.08%，说明机器视觉识别对于测度户外环境中的人群情绪具有较好的应用潜力；相较于既有的情绪感受测度方法，该模型具有实时性强、效率高等优势，可进一步在面积较大、样本量较多的户外环境中测度情绪感受。此外，机器视觉识别提高了情绪测度的智能化程度，未来可考虑将其应用于沟通或阅读障碍人群的情绪识别研究中。

此外，根据实证实验及拓展实验的情绪分类结果（图12,16），可以看出“平静”情绪的识别准确率最高，“不开心”情绪的识别准确率相对较低，这可能和测试集的样本数量有关，也从侧面印证了提高机器视觉识别的可靠性需要进行大量样本的训练与测试。

![1 3. 拓展实验平面及摄像机点位布局](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62900.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1 3. 拓展实验平面及摄像机点位布局 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_62900.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1 3. Site plan and camera locations on the study site of the additional experiment

![1 4. 拓展实验空间现状照片](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_63000.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1 4. 拓展实验空间现状照片 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_63000.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1 4. Photos of the additional experiment site

而拓展实验结果表明，运用编解码器-SVM优化模型的准确率相较于第一次实验有所下降，造成这一现象的原因可能是户外环境的景观特征差异。一方面，该空间郁闭度较高可能导致光线不足，使数据采集受到一定影响；另一方面，这一街头绿地空间面积较为局促，人群行为活动受限，被试者的平静情绪出现频率较其他情绪高，从而导致其他情绪的识别存在着瞬时性及随机性。不过，通过对比两个实验空间的物质要素构成，可以初步推测该模型在人群行为丰富、视野开阔的空间中更为适用。

![1 5. 拓展实验空间机器视觉识别准确率](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_63100.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1 5. 拓展实验空间机器视觉识别准确率 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_63100.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1 5. Emotion recognition accuracy rate of the additional experiment via machine visual cognition

![1 6. 拓展实验空间情绪分类识别准确率](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_63200.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

1 6. 拓展实验空间情绪分类识别准确率 [](https://kns.cnki.net/KXReader/Detail/GetImg?filename=images/JGSJ202105005_63200.jpg&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)  下载原图

1 6. Recognition accuracy rate of each emotion labels of the additional experiment

#### 4.2 研究不足

研究尚存在诸多不足。主要包括：

1）模型泛化能力不足，导致拓展实验中环境改变后模型的识别准确率下降。后续研究可以通过对大量样本进行训练、调整模型参数、优化模型结构等方式进行优化，并可针对不同景观特征的户外环境，训练出相应的机器视觉情绪感受识别模型，从而进行不同户外环境下人群面部情绪的分类识别。

2）本研究将人群情绪分为“平静”“开心”“不开心”三类，情绪分类细粒度水平较低。后续研究可对情绪分类识别进行细化，如增加对厌恶、悲伤、愤怒等情绪的识别。

3）模型测试时偶尔会出现因被试者低头而无法检测面部表情的情况，然而该部分的匹配结果依旧会被归入测试集的匹配准确度结果中的不准确部分，使准确度可信度降低。后续研究可以通过人脸数据筛选规避该情况的影响。

4）数据采集可能会受到光线等环境变化的影响，后续研究可以通过优化数据筛选及拍摄方式规避该影响。

5）模型的实践应用待进一步研讨，虽然该情绪测度模型是基于客观数据构建的，但情绪定义主要依据主观判断，未来研究中可通过与环境感知问卷相结合的方式分析使用者情绪变化的具体原因，从而对环境进行针对性的优化提升。

### 5 结论

在人工智能的大背景下，机器视觉识别能够辅助景观设计实践、提升景观品质优化的有效性和准确性，在追求精细化服务的智慧城市建设中具有极大应用潜力。本文以城市社区广场环境为研究场地，通过同步采集人群在户外环境体验时的面部视频数据和皮电数据，训练生成了可用于户外环境情绪测度的机器视觉算法模型—编解码器-SVM优化模型，并在街头绿地环境中对模型进行了可靠性检验，实现了机器视觉识别在户外环境情绪感受测度中的研究应用，为在场地面积更大、样本量更多的情境下对使用者情绪感受展开大范围测度提供了研究基础。得出如下主要结论：

1）基于机器视觉识别的情绪测度算法模型（编解码器-SVM优化模型）识别个体面部情绪的结果与同步皮电数据匹配的准确率达82.01%，证明了机器视觉能实现对户外环境中人群情绪感受的识别。

2）拓展实验的准确率为65.08%，证实了该算法模型的有效性，具备推广应用潜力。

3）实证实验与拓展实验的户外环境特征差异表明该模型对人群行为丰富、视野开阔的空间具有更好适用性。

4）机器视觉识别弥补了既有情绪测度方法的不足，同时提高了情绪测度的智能化程度，对特殊人群的情绪测度具有一定优势，为探究环境与人群情绪间的复杂关系提供了良好的技术手段。

### 1 Research Background

Emotion is the affect and reaction of human mind,based on the needs of the subject,to certain environmental or event stimuli[1].They affect human health physiologically and psychologically,and on social adaptation as well[2].Fastpaced,busy modern lifestyle often make citizens stressed,anxiety,depression,and uneasiness that is harmful for their physical and mental health[3][4].Existing studies prove that outdoor environments with quality landscapes can help alleviate such negative moods and benefit people’s physical and mental health[5][6].Recent research in Environmental Psychology suggests that real-time assessment on individuals’environmental affective experience can improve the scientism in measuring the quality of outdoor environmental landscapes[7],and effectively contribute to the enhancement of landscape design and environmental performance.Existing assessment methods are mostly applied in scenarios of indoor environments,while those for outdoor environments are often insufficient,especially for the cases of a larger site area or sample size.It thus needs to innovate measurement methods that can generally applied for studying people’s real-time affective experience for outdoor environments.

### 2 Measurement Methods on Affective Experience in Outdoor Environments

#### 2.1 Existing Measurement Methods on Affective Experience in Outdoor Environments and the Limitations

Individuals’affective experience in outdoor environments is an overall emotional state determined by their affected level to the environment,which includes subjective experience,physiological reactions,and external performance[8]and can be assessed with self-report,physiological measurement,and behavioral measurement,respectively[9][28][46].Self-report often in forms of mood scales or questionnaires—mostly Positive and Negative Affect Scale (PANAS)[10],Brief Profile of Mood States (BPOMS)[11],and State-Trait Anxiety Inventory (STAI)[12]—is a means that individuals subjectively describe or rate their affective experience in outdoor environments.Physiological measurement is to reflect individuals’instantaneous emotional changes,which monitors complex neurological processes and physiological changes caused by mood fluctuations and mainly indicated with human electrodermal activity (EDA),heart rate,blood pressure,electromyography,and electroencephalogram[13][14].Behavioral measurement is to assess individuals’affective experience through direct observation of the changes in external behaviors such as voice,facial expression,and body posture[15].

To better understand the pros and cons of each measurement method above,this study reviews them at five aspects,namely application scope,degree of intervention,time cost,device investment,and manpower requirement[16](Fig.1,Table 1).Selfreport is found with a broader application,low degree of intervention,and less device investment and manpower requirement.While,due to its high time cost,self-report is not an ideal choice for real-time measurement of individual’s affectiveexperience in the design practice aiming at improving user satisfaction[17,18,19,20].Physiological measurement is advantageous in measurement time,but it takes a longer time in individual measurement that makes it not applicable to large sample-size research[21,22,23,24].Besides,it often requires high device investment,and the wearing of device would more or less intervene individuals’affective experience during the measurement[25].Although behavioral measurement(observers’ratings[26]and narrative statements[27]mainly) can directly assess individuals’affective experience,but is often time-consuming and require much labor and training[28].As the increase of citizens’health demands in built environment,new technologies and approaches need to be introduced to promote the effectiveness and application scenarios of real-time measurement of affective experience in urban environments.

#### 2.2 The Measurement of Affective Experience in Outdoor Environments via Machine Visual Cognition

Machine vision is a technology that simulates human vision by machines.The computer extracts valid information from images and recognizes the image contents through data analysis and processing[29].This technology sees advantages in high precision,supporting real-time application,and advanced intelligence[29],and has been widely used in fields like public safety[30][31],intelligent driving[32][33],and medical health[34][35].

As facial expressions are people’s the most direct way to convey feelings[36],emotion analysis supported by machine visual cognition focuses on facial expression dissecting,which includes three steps,namely face detection,facial feature extraction,and emotion labeling[37]—the latter two are critical to the emotion recognition results[37].The Convolutional Neural Network (CNN)algorithm model with advanced machine learning capability has greatly elevated the accuracy and intelligence of facial emotion recognition of machine vision[38].This technology can track and extract facial information from images of both time and spatial continuity,which supports a more comprehensive and accurate analysis of human facial expressions and their changes via video monitoring[39][40].At present,emotion recognition by machine vision has been used in both indoor and outdoor environments.Werda Buana Putra et al.built a real-time facial expression recognition system via machine visual cognition which realizes the real-time monitoring of students’emotional changes at class so as to find out their unhealthy emotions as soon as possible[41].By combining machine vision technology with EDAdata,Pyoung Won Kim established an algorithm model that evaluates students’real-time mental states,thereby learning about their engagement level at class[42].Yuhao Kang et al.used machine visual cognition to recognize people’s facial emotions in 80 geotagged photos about tourist attractions collected from social media platforms,aiming to explore the relationship between the environments and individuals’affective experience[43].

Relatively,machine visual cognition is superior over other existing methods in the measurement of affective experience in outdoor environments.First,machine visual cognition is less limited by the site’s conditions and can be applied into both laboratory[44]and outdoor environments at varied scales and with different landscape characteristics[42][43].Secondly,machine visual cognition collects individuals’facial image data in a non-contact manner[41][45],minimizing the intervention over experiment participants.Thirdly,as for the experimental cost,although technical support is required for the model training of machine vision,after that data can be collected via cameras at low cost[46],saving device,manpower,and time cost,which makes the technology widely applicable.

However,machine visual cognition is little applied in measuring affective experience in outdoor environments,and the accuracy of facial emotion recognition by existing algorithm model is often far from satisfactory,due to the complexity and variety of environmental elements,body postures,and lighting conditions in outdoor scenarios[47].Therefore,this research adopts CNN algorithm model[48][49]for its high applicability to complex outdoor environments—Compared with traditional machine learning models (such as traditional clustering models and Bayesian Classifier),CNN algorithm model shows a stronger capability in facial feature extraction and emotion labeling[50][51]via convolutional layers,and optimize the model parameters through deep learning training,recognizing individuals’affective experience in outdoor environments with a higher accuracy.

#### 2.3 Sample Selection and Research Hypothesis

Through an experiment on urban community squares,one of the most frequently used place types for citizens’daily outdoor activities,this research tests the accuracy of machine visual cognition in affective experience measurement,wherein CNN algorithm model is trained with a deep learning algorithm that can discriminate facial emotions.The research demonstrates the effectiveness of measuring affective experience in outdoor environments via machine visual cognition,hoping to provide empirical references for landscape design practices.

### 3 Empirical Research

#### 3.1 Experiment Design and Data Collection

This research selects the Jinxiu Community Square in Chenghua District,Chengdu City as the study site for the experiment of measuring affective experience via machine visual cognition.Showing as Table 2,the 1.8 hm2 study site neighbors a secondary city road,a residential area,and a Community Service Center,and connects with other community green spaces.With an open view,the site is covered with vegetation and pavements and has corridors,seats,and other landscape elements.Nine cameras were placed on the site to ensure an all-round vision for facial image collation.The experiment was conducted on sunny days with a comfort weather (08:00～12:00,from September 19 to 21,2019).

Referring to previous studies on affective experience measurement,the number of experiment participants is reliable when it exceeds 30[52].A total of 42 males and females from the neighboring community were recruited as participants1 that ensured the diversity of demographics.Each participant was asked to follow the experiment steps as below (Fig.2):

1) Preparation:The participant puts on the portable PPG-EDA biofeedback device2 on forefinger and middle finger,and walks around while the researchers introducing them the experimental procedures and precautions;

2) Beginning:The participant sits quietly in the waiting area for 10 minutes3[53][54]to get calm down,then their initial EDA data are collected;

3) Stressor:The participant exposes to a noise in a closed car and does maths for 3 minutes4,which ensures that all participants would be in a similar anxiety status.

4) Experiencing:The participant walks around the site as usual for 5minutes5[55][56].

And 5) Ending:The participant fills in a questionnaire and leaves the site before removing the device with help of the researchers.

Each individual experiment took about 20 minutes,and the cameras recorded videos from the beginning to the ending.A total of 42 sets of real-time EDA data(the SCR value changes detected by PPG-EDA module in Biopac MP150 Data Acquisition and Analysis System),facial image data,and questionnaire data were collected.

#### 3.2 Experiment Process and Measurement Results

#### 3.2.1 Emotion Labeling and Data Set Preparation

Emotion labeling is required for the emotion analysis on the videos via machine visual cognition.There are three emotion labeling ways demonstrated in existing studies:1) directly stimulate participants to generate a specific emotion and label it;2) self-report by participants after experiments;and 3) label participants’emotion according to physiological data during the experiment[57].The first is too ideal since the expected emotion maybe cannot generate under experimental conditions,and the second is poor in credibility because the participants may misrepresent due to priori knowledge.Also,for the both,only constant stressors could emotions be successfully labeled.This research thus adopts the third method,i.e.labeling emotion by the collected real-time EDA data.

To begin with,a matched samples t-test of the mean values of EDA data before and after Experiencing phase is conducted (0.02±0.38 at the Stressor phase and0.47±0.13 at the Experiencing phase,p<0.01 less thanα=0.05),proving a strong significance of the study site to relieve stress (Fig.3,4).Then,the Savitzky-Golay Filter is employed to denoise and smooth each participant’s EDA data under the proximity principle (Fig.5).The peak values are paired between the emotional and EDA data models referring from previous experiments[57].Finally,the emotions are divided into three types in this study:1) Being almost no fluctuations in EDA data,the emotions are labeled as“calm”;2) Being intensive moderate fluctuations in multiple segments within 15 seconds,the emotions are labeled as“happy”;And 3)being an abrupt fluctuation,the emotions are labeled as“unhappy”(Fig.6).

Each 15-second segment is viewed as a facial expression,with a 5-second interval.The dataset used for the model training is obtained with the CascadeClassifier of OpenCV,which can acquire the harr-like features of the images and then perform calibrated cropping of the face areas (all images can be cropped as long as the angle of the input face images is less than 90°).Finally,a total of 57,600 face images with emotional labels are obtained which are used as the dataset for subsequent model training.

#### 3.2.2 Emotional Facial Feature Model Training

(1) Model definition

The original face image consists of a fixed-size pixel matrix.When the matrix is rendered into a one-dimensional matrix,voluminous matrix data would be generated,which makes it difficult to identify the similarity and disparity among images of the same/different classes.The codec–SVM optimized model based on CNN (Fig.7) is introduced into this research to extract and fit the facial features via convolution and deconvolution.Convolution classifies the images by extracting different texture features of images using varied convolutional kernels.In turn,deconvolution is a process of recovering the original images.The characteristic vectors extracted by the encoder via convolution are then fed into the decoder developed via deconvolution.Then the network training repeated until the cross entropy between the output and the input images demonstrate asteady pattern,i.e.when the characteristic vectors in the codec can embody the image characteristic.Combining representation and generation,these characteristic vectors outperform the ones obtained from ordinary CNN training that supports representation only.The codec–SVM optimized model used for image characteristic extraction and subsequent model training[58]can realize the recognition of facial expressions(6) for the measurement of affective experience in varying outdoor environments with diverse population characteristics.

(2) Model training

As the model training is not trained with prior labels from previous studies,the final recognition accuracy rate is calculated as the labels from emotion labels by Savitzky-Golay Filter to those of the trained model in this research.To ensure the same participants sizes of the model training set and the model testing set,the face images of 21 participants are randomized into the training set and the testing set each.The codec–SVM optimized model extracts and recognizes face images while improving its recognition accuracy by iterating algorithm parameters.The learning rate set as 1e-3,a total of 32 112×112-dimensional images are used for the training each time.The model training takes a relative long time due to the model generation characteristics by the decoder (Fig.8).

Taking 120 epochs,a total of 28,800 images are used to obtain the intermediate vectors with image characteristics and to train the SVM classifier.In the SVMclassifier,the nonlinear mapping to high-dimensional space is replaced with the inner kernel function,so as to enhance the robustness of the model and avoid the training loss and overfitting caused by multiple fully connected layers[59].In the training process,the kernel functionγis adjusted:the recognition accuracy of0.9771 occurs whenγranges from 7 to 8.5;and after the fine adjustment,the bestγvalues of the three kernel functions (i.e.Gaussian Function,Poly Function,and Sigmoid Function) are 8.935,8.935,and 2,respectively (Fig.9).During the process of SVM classification,different kernel functions match with varied classification models.As the characteristics of the input data are unknown,the experimentcompares different kernel functions and their values to improve the classification accuracy of the codec–SVM optimized model.The loss coefficient c is then adjusted to obtain the final emotion cognition model (Fig.10).It is concluded that theγvalue is 8.935,c 15,and recognition accuracy 0.9783 under the Gaussian Function.

#### 3.2.3 Emotion Recognition and Analysis of Video Images

To predict facial expressions via the trained model,the 28,800 face images of the rest 21 participants with no emotion labels are used as the testing set[60][61].The feature extraction and recognition of the images are realized with the CascadeClassifier (the scale is set as 160×160 px,the factor 7,the detected minimum size 100×100 px,and 1,000×1,000 px for the maximum).The resolution of the video is 30 fps and the facial expressions in each frame are recognized with emotion labels.The matching rate of the recognition results and EDA data is 82.01%(Fig.11),and the size for the labeled emotion“quiet,”“happy,”and“unhappy”is 105,78,and 6,respectively (Fig.12).

#### 3.3 Additional Experiment Testing

To further test the effectiveness of algorithm model in different outdoor environments,the research team conducted an additional experiment in a street green space in the same district in September 2020.Covering an area of 0.6 hm2,the study site is covered with vegetation (trees mainly),fitness facilities,and pavements,with a high canopy density.To collect all-round facial data,8 cameras were placed on the site (Fig.13,14).Following the same experiment procedure as the main experiment above,the obtained datasets of 21 participants (community residents) are analyzed by the codec–SVM optimized model.The matching rate between the recognition results and EDA data is 65.08%(Fig.15).The size for the labeled emotion“quiet,”“happy,”and“unhappy”is 122,60,and 7,respectively(Fig.16).

### 4 Discussions

#### 4.1 Result Analysis

The accuracy rate of the codec–SVM optimized model developed in this study reaches 82.01%and 65.08%in the measurement of affective experience in two outdoor environments with different landscape characteristics.The results indicate a satisfactory potential of the machine visual cognition in related applications.Compared with existing measurement methods,this technology supports real-time emotion measurement in a larger site area or sample size.In addition,machine visual cognition promotes the intelligence level of affective experience measurement,which would benefit the studies on people facing communication or reading disability.

Also,the results of the main and additional experiments (Fig.12,16) show that the machine visual cognition has a high recognition accuracy of“quiet”emotion but that of“unhappy”is unsatisfactory—This may be due to the sample size of the testing set,which could somehow explain that the reliability of machine visual cognition depends on intensive training based on a considerable sample size.

The recognition accuracy result of the codec–SVM optimized model in the additional experiment is lower than that in the main experiment—This may be down to the disparity of the landscape characteristics of the two sites:the high canopy density of the site may impact data collection to some extent,and the small site area may limit participants’physical activities that often brings a quiet emotion state,as a result,more transient and random in the cognition results of other emotion states.The finding also suggests that the codec–SVM optimized model is more applicable for scenarios with varying usage behaviors and open views.

#### 4.2 Limitations

This research sees limitations as follows.

1) The fact that the model yields a lower recognition accuracy in the additional experiment suggests that its application capacity needs to be promoted by training the model with a larger number of samples,adjusting model parameters,and optimizing model structure.Also,different machine visual cognition models can be developed for outdoor environments with varied landscape characteristics.

2) The emotion labels in this research are“quiet,”“happy,”and“unhappy.”In the future,more emotion labels (such as disgust,sadness,and anger) need to be studied.

3) During the model test,when the subjects lowered heads,the facial expressions cannot be detected but recorded as the inaccuracy data,which affected the matching accuracy of the testing set.Applying face data screening in future studies to avoid such errors is expected.

4) Outdoor data collection is largely subject to environmental conditions (e.g.,sunlight).Such impacts can be minimized by optimizing data screening and camera shooting methods.

And 5) although the codec–SVM optimized model is developed on objective data,the emotion labeling is mostly defined by researchers’subjective assessment.Other methods such as questionnaire surveys on environmental perceptions can be combined to further analyze the factors that cause people’s emotional changes,which would better inform related environmental design.

### 5 Conclusions

As Artificial Intelligence technology advances,machine visual cognition contributes to landscape design and improves the effectiveness and accuracy of landscape optimization efforts,demonstrating a great potential in the building of Smart Cities with refined public services.Taking an urban community square as the study site,this research simultaneously collects participants’facial data from video images and their EDA data in the outdoor environments,and further develops a machine visual algorithm model,i.e.codec–SVM optimized model,whose reliability is tested through an additional experiment.The study explores the application effectiveness of machine visual cognition in outdoor environments,offering references to other cases of a larger site area or sample size.The main research findings include:

1) The matching rate between the EDA data and the emotion labeling results of the codec–SVM optimized model via machine visual cognition is 82.01%,suggesting that machine vision can be used to recognize affective experience in outdoor environments.

2) The matching rate in the additional experiment is 65.08%,verifying the effectiveness and application potential of the algorithm model.

3) Comparing the environmental differences between the main and additional experiments,the model works more effectively for outdoor scenarios with varying usage behaviors and open views.

And 4) machine visual cognition breaks up the limitations of existing measurement methods and increase the intelligence level of recognition.It could be an instrumental tool to explore the complex relationships between the environments and users’affective experience.

### 参考文献

[**[1]** Zhou,W.,Nie,X.,Chi,M.,Zhan,Y.,Yan,C.,Xiu,X.,&Lan,S.(2020).Influence of the Emotional State of Forest Therapy Consumers on the Recovery of Physical and Mental Health.Forestry Economics,(09),53-62.doi:10.13843/j.cnki.lyjj.20201204.001](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=DKFX&filename=LYJJ202009007&v=MDc0MjU3bWZaT2RyRnkzZ1VMckFLVFRCWkxHNEhOSE1wbzlGWTRRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[2]** Dong,Y.,Wang,Q.,&Xin,C.(2012).Progress in Research on the Relationship between Positive Emotion and Physical and Mental Health.Journal of Psychological Science,(02),487-493.doi:10.16719/j.cnki.1671-6981.2012.02.043

[**[3]** Tost,H.,Champagne,F.A.,&Meyer-Lindenberg,A.(2015).Environmental influence in the brain,human welfare and mental health.Nature Neuroscience,18(10),1421.doi:10.1038/nn.4108](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJPD&filename=SJPDA0EDDF8FCF26D89AB553222E78B07DA7&v=MTcwMzE0MlljekY1ME5DZ2d4eG1kaDd6cCtTbjNnMlJVOUM3S1RNY3VZQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnI3VTI1OTFod2IyOHhhQT1OaWZiYXNLNGE2Vw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[4]** Wang,Q.,Wang,C.,Ren,H.,&Li,T.(2013).Urbanized environment and the risk of schizophrenia.Chinese Journal of Nervous and Mental Diseases,(12),758-763.doi:10.3936/j.issn.1002-0152.2013.12.014

[**[5]** Peng,H.,&Tan,S.(2018).Study on the Influencing Mechanism of Restoration Effect of Urban Park.Chinese Landscape Architecture,034(009),5-9.doi:10.3969/j.issn.1000-6664.2018.09.002](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=ZGYL201809002&v=MTk0MzQzZ1VMckFQeXJTWXJHNEg5bk1wbzlGWm9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3bWZaT2RyRnk=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[6]** Jiang,B.,Zhang,T.,William,C.S.,&Wu,X.(2015).Healthy Cities:Mechanisms and Research Questions Regarding the Impacts of Urban Green Landscapes on Public Health and Well-being.Landscape Architecture Frontiers,3(1),24-35.Retrieved from https://journal.hep.com.cn/laf/EN/article/download Article File.do?attach Type=PDF&id=13393

[**[7]** Chen,Z.,&Liu,S.(2018).Real-time Environmental Affective Experience Assessment via Wearable Sensors.Chinese Landscape Architecture,34(3),12-17.doi:10.3969/j.issn.1000-6664.2018.03.003](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=ZGYL201803003&v=MTY5MTN5M2dVTHJBUHlyU1lyRzRIOW5Nckk5Rlo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN21mWk9kckY=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[8]** Meng,Z.(2005).Emotional Psychology.Beijing,China:Peking University Press.

**[9]** Kong,Y.(2014).The research methods of emotion (Master’s thesis).Retrieved from CNKI database.

[**[10]** Watson,D.,Clark,L.A.,&Tellegen,A.(1988).Development and validation of brief measures of positive and negative affect:The panas scales.Journal of Personality and Social Psychology,54(6),1063-1070.doi:10.1037/0022-3514.54.6.1063](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJPD&filename=SJPD13060100219972&v=MTA1MThud1plWnRGU25uVTdySUpWc1hieG89TmlmYmFySzdIdGZNcm85Rlp1b0dCWHM3b0JNVDZUNFBRSC9pclJkR2VycVFUTQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[11]** Mcnair,D.M.,Lorr,M.,&Droppleman,L.F.(1971).EITSManual for the Profile of Mood States.San Diego,CA:Educational&Industrial Testing Services.

**[12]** Zheng,X.,&Li,Y.(1997).State-Trait Anxiety Inventory.Chinese Mental Health Journal,11(4),219-220.

[**[13]** Chen,Z.,Schulz,S.,Qiu,M.,Yang,W.,He,X.,Wang,Z.,&Yang,L.(2018).Assessing affective experience of in-situ environmental walk via wearable biosensors for evidencebased design.Cognitive Systems Research,(52),970-977.doi:10.1016/j.cogsys.2018.09.003](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJES27D0284E774D4824480A6FCE63CAC36B&v=MDY1MDNtYUJ1SFlmT0dRbGZCcjdVMjU5MWh3YjI4eGFBPU5pZk9mYkcvYXRIT3A0c3dZK3dMZUhneHpSSVg0ajhNVGdtUjJSUTJDc1BuUnJ6dENPTnZGU2lXV3I3SklGcA==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[14]** Rad,P.N.,Shahroudi,A.A.,Shabani,H.,Ajami,S.,&Lashgari,R.(2019).Encoding Pleasant and Unpleasant Expression of the Architectural Window Shapes:An ERPStudy.Frontiers in Behavioral Neuroscience,(13),186.doi:10.3389/fnbeh.2019.00186](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJPD&filename=SJPDEDAD22E5A976FDFB2E3CBA72558E9602&v=MDc5NzA3SklGcG1hQnVIWWZPR1FsZkJyN1UyNTkxaHdiMjh4YUE9TmlmYmFzYk1iNlhPcmZwQUZlSUlDZ3BOdVdRUm56d09PZzdscmhjd2NjZWRRN3FkQ09OdkZTaVdXcg==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[15]** Xie,J.,Fang,P.,&Jiang,Y.(2011).Advances in Emotion Measuring Methods.Journal of Psychological Science,(02),488-493.doi:10.16719/j.cnki.1671-6981.2011.02.048](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=XLKX201102046&v=MzEwMDlPZHJGeTNnVUxyQVBTSEFkckc0SDlETXJZOUJZb1FLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdtZlo=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[16]** Xu,X.,Zhao,W.,&Liu,H.(2018).Research on Application and Model of Emotional Analysis in Blended Learning Environment:From Perspective of Meta-analysis.E-education Research,(08),70-77.doi:10.13811/j.cnki.eer.2018.08.011

[**[17]** Chen,C.,&Zhang,H.(2018).Using emotion to evaluate our community:Exploring the relationship between the affective appraisal of community residents and the community environment.Architectural Engineering and Design Management,14(3-4),1-16.doi:10.1080/17452007.2018.1457942](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=STJD&filename=STJDEEDB3781ED97174AC72329976FF7CB89&v=MjM5NTNuYnJxeFJERDdYbk43S1dDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcjdVMjU5MWh3YjI4eGFBPU5qbkJhc2JOYXFQUHFJZEVFWjhHQzMwK3kyZGc3VDErUw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[18]** Li,D.,Deal,B.,Zhou,X.,Slavenas,M.,&Sullivan,W.C.(2018).Moving beyond the neighborhood:Daily exposure to nature and adolescents’mood.Landscape and Urban Planning,(173),33-43.https://doi.org/10.1016/j.landurbplan.2018.01.009](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJES452D44101C2673290C4A71406AD7537D&v=MjI1MzJ3YjI4eGFBPU5pZk9mYmU5SEtYSXE0NUZaWmdOQ25zNnpSOFRtVHNNVDM3bXJCUkVEYldSUnIzckNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyN1UyNTkxaA==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[19]** Chen,Z.,Dong,N.,Liu,S.,Zhang,Y.,&Ding,Q.(2017).Study on the Impacts of Urban Park Use on Public Health in Shanghai.Landscape Architecture,(09),99-105.doi:10.14085/j.fjyl.2017.09.0099.08](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=FJYL201709028&v=MTM2NjdmWk9kckZ5M2dVTHJBSXlmU1lyRzRIOWJNcG85SGJJUUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN20=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[20]** Liu,C.,Li,S.,&Chen,S.(2018).Study on Behavior of Visiting Campus Green Space’Role in Emotion Regulation under the Influence of Multi-factors:Take 3 Universities in Beijing for Example.Landscape Architecture,(03),46-52.doi:10.14085/j.fjyl.2018.03.0046.07](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=FJYL201803008&v=MjE1NTJySTlGYklRS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3bWZaT2RyRnkzZ1VMckFJeWZTWXJHNEg5bk0=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[21]** Lin,W.,Chen,Q.,Jiang,M.,Zhang,X.,&Zeng,Q.(2019).The effect of green space behaviour and per capita area in small urban green spaces on psychophysiological responses.Landscape and Urban Planning,(192),103637.https://doi.org/10.1016/j.landurbplan.2019.103637](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJESDECAEA598CFD048CC8CEC6083F557893&v=MDkxOTNiMjh4YUE9TmlmT2ZjZk5iYUM1M29wTWJKaDVlSHc5eDJWZzRrd0lPM25pcEJGRGZMZVRUYk9jQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnI3VTI1OTFodw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[22]** Lin,W.,Chen,Q.,Jiang,M.,Tao,J.,&Zeng,Q.(2020).Sitting or Walking?Analyzing the Neural Emotional Indicators of Urban Green Space Behavior with Mobile EEG.Journal of Urban Health,97(4).191-203.https://doi.org/10.1007/s11524-019-00407-8](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJPD&filename=SJPD54E1152AFB4204A526170E78319B1257&v=MTM0NjZybHBCRTBjTUNWUjcrWUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyN1UyNTkxaHdiMjh4YUE9TmlmYmFyYThhOUROcW8wMEVwa0xEbnc5dmhNUjdENTZTQQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[23]** Liu,B.,&Xu,Y.(2016).Study on the Effects of Different Landscapes on Elderly People’s Body-Mind Health.Landscape Architecture,(7),113-120.doi:10.14085/j.fjyl.2016.07.0113.08](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=FJYL201607013&v=MzI3MjlqNTRPM3pxcUJ0R0ZyQ1VSN21mWk9kckZ5M2dVTHJBSXlmU1lyRzRIOWZNcUk5RVo0UUtESDg0dlI0VDY=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[24]** Weng,Y.,Zhu,Y.,Dong,J.,Wang,M.,&Dong,J.(2021).Effects of Soundscape on Emotion and Attention on Campus Green Space-A Case Study of Fujian Agriculture and Forestry University.Chinese Landscape Architecture,37(2),88-93.doi:10.19775/j.cla.2021.02.0088](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=ZGYL202102016&v=MDc4NjlORE1yWTlFWW9RS0RIODR2UjRUNmo1NE8zenFxQnRHRnJDVVI3bWZaT2RyRnkzZ1VMckFQeXJTWXJHNEg=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[25]** Ge,Y.,Chen,Y.,Liu,Y.,Li,W.,&Sun,X.(2014).Electrophysiological Measures Applied in User Experience Studies.Advances in Psychological Science,22(6),959-967.doi:10.3724/SP.J.1042.2014.00959

[**[26]** Ekman,P.,Freisen,W.V.,&Ancoli,S.(1980).Facial signs of emotional experience.Journal of Personality and Social Psychology,39(6),1125-1134.](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SBQK&filename=SBQK039DDED0FE5441D5FD118CB7B914B07D&v=MzE5MDJPR1FsZkJyN1UyNTkxaHdiMjh4YUE9TmkvYVpiTzdGNlc0MnZ0RkVwNEtDSGc0dXhObG5qNThRQXlRcTJBOGVMYm1SYjNyQ09OdkZTaVdXcjdKSUZwbWFCdUhZZg==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[27]** Lengen,C.(2015).The effects of colours,shapes and boundaries of landscapes on perception,emotion and mentalising processes promoting health and well-being.Health&Place,(35),166-177.https://doi.org/10.1016/j.healthplace.2015.05.016](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJESEC1661551DEDCA7075E0D8D81087D8B0&v=MjE2NDVCWVU3MHA5UEhlV3BCTTFjYlhnVGNpZkNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyN1UyNTkxaHdiMjh4YUE9TmlmT2ZjYkxIOWZLcm9wQVpaOTZlQTlJeQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[28]** Sinclair,R.R.,Wang,M.,&Tetrick,L.E.(2014).Research Methods in Occupational Health Psychology:Measurement,Design and Data Analysis.Nanjing,China:Southeast University Press.

**[29]** Zhu,Y.,Lin,Z.,&Zhang,Y.(2020).Research progress and prospect of machine vision technology.Journal of Graphics,41(6),871-890.doi:10.11996/JG.j.2095-302X.2020060871

[**[30]** Hu,W.,&Hu,H.(2020).Disentangled Spectrum Variations Networks for NIR-VIS Face Recognition.IEEE Transactions on Multimedia,22(5),1234-1248.doi:10.1109/TMM.2019.2938685](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=XJAZ&filename=XJAZ7AEFD1DA4B48F5DE32E04E7913AFE0A4&v=MzA3MDVmT0dRbGZCcjdVMjU5MWh3YjI4eGFBPVBTZktkTFRKYTZlNHJ2czBZSmtMQkFvOHUyTVE2RXA5VEFybHBSTTJDTVRoUmN1YkNPTnZGU2lXV3I3SklGcG1hQnVIWQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[31]** Grant,J.M.,&Flynn,P.J.(2017).Crowd Scene Understanding from Video:A Survey.ACM Transactions on Multimedia Computing,Communications,and Applications,13(2),1-23.https://doi.org/10.1145/3052930

**[32]** Gerónimo,D.,López,A.M.,Sappa,A.D.,&Graf,T.(2010).Survey of Pedestrian Detection for Advanced Driver Assistance Systems.IEEE Transactions on Pattern Analysis and Machine Intelligence,32(7).1239-1258.doi:10.1109/TPAMI.2009.122.

[**[33]** Wei,J.,He,J.,Zhou,Y.,Chen,K.,Tang,Z.,&Xiong,Z.(2019).Enhanced Object Detection with Deep Convolutional Neural Networks for Advanced Driving Assistance.IEEE Transactions on Intelligent Transportation Systems,21(4),1572-1583.doi:10.1109/TITS.2019.2910643](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJIE&filename=SJIEFD972B4C0B239287579A8E5CDA02BB26&v=MDM3NTc3VTI1OTFod2IyOHhhQT1OaWZDYThYTUY5Yk8zWXMyWkprTkQzVTd4eEVXN1RZTVFBcm4zMlpFZWJEbU43aVpDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcg==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[34]** Parmar,C.,Barry,J.D.,Hosny,A.,Quackenbush,J.,&Aerts,H.J.(2018).Data Analysis Strategies in Medical Imaging.Clinical Cancer Research,24(15).3492-3499.doi:10.1158/1078-0432.CCR-18-0385

[**[35]** Yu,Z.,Jiang,X.,Zhou,F.,Qin,J.,Ni,D.,Chen,S.,…Wang,T.(2019).Melanoma Recognition in Dermoscopy Images via Aggregated Deep Convolutional Features.IEEE Transactions on Biomedical Engineering,66(4),1006-1016.doi:10.1109/TBME.2018.2866166](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJPD&filename=SJPDF22CD3D3CB8C221B8383EA852FFCC885&v=MTc4Mzc5MWh3YjI4eGFBPU5pZmJhc1c2SEtLNHJQdEdGNWtIZjM0N3ptUWI2VGQrUFE3cXFSQkREOEhuVGJLYUNPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyN1UyNQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[36]** Mehrabian,A.(1968).Communication Without Words.In C.D.Mortensen (Ed.),Communication Theory (p.8).New York,NY:Routledge.

**[37]** Li,G.,Liu,B.,Li,K.,&Huang,S.(2019).Research on Face Emotion Recognition Based on Machine Learning.Computer Technology and Development,29(5),27-31.doi:10.3969/j.issn.1673-629X.2019.05.006

[**[38]** Schmidt,T.,Newcombe,R.,&Fox,D.(2017).Self-Supervised Visual Descriptor Learning for Dense Correspondence.IEEERobotics and Automation Letters,2(2),420-427.doi:10.1109/LRA.2016.2634089](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=XJAZ&filename=XJAZ4CFD56EB36693E900BB9846389265938&v=MTgxNDVYOU14aFlUbUUxMFFIdmtyeG84ZTdTUlRMbVhDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcjdVMjU5MWh3YjI4eGFBPVBTZktkTGZMYUtYSnFmbzNaKzBKQg==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[39]** Xu,L.,Zhang,S.,&Zhao,J.(2017).Summary of facial expression recognition methods based on image.Journal of Computer Applications,37(12),3509-3516,3546.doi:10.11772/j.issn.1001-9081.2017.12.3509](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=JSJY201712053&v=MjgxMjdmWk9kckZ5M2dVTHJBTHo3QmQ3RzRIOWJOclk5QVo0UUtESDg0dlI0VDZqNTRPM3pxcUJ0R0ZyQ1VSN20=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[40]** Yan,Y.,&Zhang,Y.(2009).State-of-the-Art on Video-Based Face Recognition.Chinese Journal of Computers,32(5),878-886.doi:10.3724/SP.J.1018.2009.00878

[**[41]** Putra,W.B.,&Arifin,F.(2019).Real-Time Emotion Recognition System to Monitor Student’s Mood in a Classroom.Journal of Physics:Conference Series.Advance online publication.doi:10.1088/1742-6596/1413/1/012021](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=XJAZ&filename=XJAZ72393057FF68BF094C56B7A91E8FE089&v=MDE3MzRVMjU5MWh3YjI4eGFBPVBTZktkTFM2SGRqUHI0cENFcDBKQkE1UHp4OFhtVHA3T25pVHBSTkFjY1RoUmJLV0NPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyNw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[42]** Kim,P.W.(2018).Real-time bio-signal-processing of students based on an intelligent algorithm for internet of things to assess engagement levels in a classroom.Future Generation Computer Systems,(86),716-722.doi:10.1016/j.future.2018.04.093](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJES705DC107DACCE21152DC044679FD21C5&v=MDQzODFyN0pJRnBtYUJ1SFlmT0dRbGZCcjdVMjU5MWh3YjI4eGFBPU5pZk9mYlM0RzZXL3JvOUNFSnA4ZndrN3poY1c2RXNPU0h2bXFoVThEOGFXUk1tYUNPTnZGU2lXVw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[43]** Kang,Y.,Jia,Q.,Gao,S.,Zeng,X.,Wang,Y.,Angsuesser,S.,…Fei,T.(2019).Extracting human emotions at different places based on facial expressions and spatial clustering analysis.Transactions in GIS,23(3),450-480.doi:10.1111/tgis.12552](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJWD&filename=SJWD1E1D5A9E143B1D42CAD5F27720559056&v=MDM3NzdyTE5INlhKM29Zd1plOE1mbjFOeXhSZ20wdDRQbjNscXhBMWZMZWRSYitaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnI3VTI1OTFod2IyOHhhQT1OaWZjYQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[44]** Qiao,L.,Zhuang,J.,Zhang,X.,Su,Y.,&Xia,Y.(2021).Assessing Emotional Responses to the Spatial Quality of Urban Green Spaces through Self-Report and Face Recognition Measures.International Journal of Environmental Research and Public Health,18(16),8526.doi:10.3390/ijerph18168526](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJMD&filename=SJMD14B9DE414CDD7CB840022D0A6ED51E24&v=MjYzOTQxaHdiMjh4YUE9TmlmR2FySzhiTmk0Mm90RVlKaDdlSHRLdlI0WDZqOS9TZ3ZpM1JSQURiZVZNTGliQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnI3VTI1OQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[45]** Zeng,Z.,Pantic,M.,Roisman,G.I.,&Huang,T.S.(2009).A Survey of Affect Recognition Methods:Audio,Visual,and Spontaneous Expressions.IEEE Transactions on Pattern Analysis and Machine Intelligence,31(1),39-58.doi:10.1109/TPAMI.2008.52](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJPD&filename=SJPD12102101658477&v=MjY5MzNySzZIOUhPcm85RVl1NEhDSHMrb0JNVDZUNFBRSC9pclJkR2VycVFUTW53WmVadEZTbm5VN3JJSlZzWGJ4bz1OaWZiYQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[46]** Chen,Z.,&Zhu,X.(2021).Research on Automatic Emotion Recognition for Learners based on Facial Expression:Relevance,Research Situation,Existing Problems and Development Paths.Journal of Distance Education,37(4),64-72.doi:10.15881/j.cnki.cn33-1304/g4.2019.04.008](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=YCJY201904008&v=MDI5MzVnVUxyQVBDN0JkN0c0SDlqTXE0OUZiSVFLREg4NHZSNFQ2ajU0TzN6cXFCdEdGckNVUjdtZlpPZHJGeTM=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[47]** Shen,J.(2019).Research on Driver’s Emotion Recognition Algorithm Based on Machine Vision in Complex Illumination Environment (Doctoral dissertation).Retrieved from CNKIdatabase.

[**[48]** Kumar,N.,&Sukavanam,N.(2019).An improved CNNframework for detecting and tracking human body in unconstraint environment.Knowledge-Based Systems,(193),105198.doi:10.1016/j.knosys.2019.105198](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJESBEB4F61C40971010E83FBE377FCFE746&v=MTU4NjVzVGhRcjZaQ09OdkZTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnI3VTI1OTFod2IyOHhhQT1OaWZPZmNITmJOVzZxWTQyWU9zR0MzMDV6aFptNGp3TE9ncmhxeFZEQw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[49]** Tang,L.,Gao,C.,Chen,X.,&Zhao,Y.(2019).Pose detection in complex classroom environment based on improved faster R-CNN.IET Image Processing,13(3),451-457.doi:10.1049/iet-ipr.2018.5905](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJGI&filename=SJGICBFA7AF057A0E4FCBF90D7B5D853210F&v=MDMyOTFHUWxmQnI3VTI1OTFod2IyOHhhQT1OaWZNWjhES2FLREwzdmxGWWV4K0RBazl1V1ZobkRaOVBIaVFxV1k5ZkxHV1JMcnBDT052RlNpV1dyN0pJRnBtYUJ1SFlmTw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[50]** Chen,T.,Li,X.,Grosse,R.,&Duvenaud,D.(2018).Isolating Sources of Disentanglement in Variational Autoencoders.Co RR.Advance online publication.Retrieved from https://arxiv.org/pdf/1802.04942.pdf](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=DBLP&filename=DBLPE12BE38F38749D8D429324BDE8ECAACD&v=MzEyNDFTaVdXcjdKSUZwbWFCdUhZZk9HUWxmQnI3VTI1OTFod2IyOHhhQT1JUy9IZnNhNUhLTzVySWN6WitNSUNIVk54MklYNkRaK1NudVEyR2M5RE1IbE5NbnJDT052Rg==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[51]** Burgess,C.P.,Higgins,I.,Pal,A.,Matthey,L.,Watters,N.,Desjardins,G.,&Lerchner,A.(2018).Understanding disentangling inβ-VAE.Co RR.Advance online publication.Retrieved from https://arxiv.org/pdf/1804.03599.pdf

**[52]** Guo,S.,Zhao,N.,Zhang,J.,Xue,T.,Liu,P.,Xu,S.,&Xu,D.(2017).Landscape visual quality assessment based on eye movement:College student eye-tracking experiments on tourism landscape pictures.Resources Science,39(6),1137-1147.doi:10.18402/resci.2017.06.13

[**[53]** Tsunetsugu,Y.,Lee,J.,Park,B.J.,Tyrvainen,L.,Kagawa,T.,&Miyazaki,Y.(2013).Physiological and psychological effects of viewing urban forest landscapes assessed by multiple measurements.Landscape and Urban Planning,(113),90-93.https://doi.org/10.1016/j.landurbplan.2013.01.014](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJESB913780123D2AB6ED628F5ED89B37C31&v=MjkyNTNhQT1OaWZPZmNHeEg5TExwNDlFWnVoN0RnMUx5V05uN0QxMVBucVgyQm84QzdHVE5ybWVDT052RlNpV1dyN0pJRnBtYUJ1SFlmT0dRbGZCcjdVMjU5MWh3YjI4eA==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[54]** Joung,D.,Kim,G.,Choi,Y.,Lim,H.,Park,S.,Woo,J.-M.,&Park,B.-J.(2015).The Prefrontal Cortex Activity and Psychological Effects of Viewing Forest Landscapes in Autumn Season.International Journal of Environmental Research and Public Health,12(7),7235-7243.doi:10.3390/ijerph120707235](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJMD&filename=SJMD15081200001826&v=MDgwNTlpclJkR2VycVFUTW53WmVadEZTbm5VN3JJSlZzWGJ4bz1OaWZHYXJLOUh0bk5yWTlGWk9zT0JINC9vQk1UNlQ0UFFILw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[55]** Barton,J.O.,&Pretty,J.(2010).What is the best dose of nature and green exercise for improving mental health?Amulti-study analysis.Environmental Science&Technology,44(10),3947.doi:10.1021/es903183r](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJPD&filename=SJPD12102101007384&v=MTA5MTNIL2lyUmRHZXJxUVRNbndaZVp0RlNublU3cklKVnNYYnhvPU5pZmJhcks2SDlIT3JvOUVaT3NJRDNROW9CTVQ2VDRQUQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

[**[56]** Ulrich,R.S.,Simons,R.F.,Losito,B.D.,Fiorito,E.,Miles,M.A.,&Zelson,M.(1991).Stress recovery during exposure to natural and urban environments.Journal of Environmental Psychology,11(3),201-230.https://doi.org/10.1016/S0272-4944(05)80184-7](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=SJES&filename=SJESJ2DF70106976E405827099109B8BF8147&v=MjA5MTdYYmpyQnRIY2NEaVRidWJZSWxxRmlybElMYkpJVjRRR0JDTGJJbWFMMVhEcnJZMTVkaGt4TDI2dzYySk5pZk9mY202YXFmTHI0NUZZdUlJQ2drOXp4TWI2RGg5UQ==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[57]** Zhou,Y.(2012).The Research of the Emotional Feature Extraction and Recognition Classification of GSR (Master’s thesis).Retrieved from CNKI database.

[**[58]** Gao,K.,Sun,S.,Yao,G.,&Zhao,H.(2017).Semantic segmentation of night vision images for unmanned vehicles based on deep learning.Journal of Applied Optics,38(3),421-428.doi:10.5768/JAO201738.0302007](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&filename=YYGX201703013&v=MDY4MTA1NE8zenFxQnRHRnJDVVI3bWZaT2RyRnkzZ1VMckFQRFRNZHJHNEg5Yk1ySTlFWjRRS0RIODR2UjRUNmo=&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[59]** Gayathri,N.,&Mahesh,K.(2020).Improved Fuzzy-Based SVM Classification System Using Feature Extraction for Video Indexing and Retrieval.International Journal of Fuzzy Systems,22(5),1716-1729.https://doi.org/10.1007/s40815-020-00884-z

[**[60]** Shan,L.,&Deng,W.(2018).Deep Facial Expression Recognition:A Survey.IEEE Transactions on Affective Computing.Advance online publication.doi:10.1109/TAFFC.2020.2981446](https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=XJAZ&filename=XJAZ82C04E783025F96FBC2DFF6AE6C0569B&v=MjkxNTBVMjU5MWh3YjI4eGFBPVBTZktkTHU2YmRISTJvaE5aK3NOQ1Fvd3lXQmhtVDBKUGduazNXY3pDcktSUTdQdENPTnZGU2lXV3I3SklGcG1hQnVIWWZPR1FsZkJyNw==&uid=WEEvREcwSlJHSldSdmVqM1BLY1lQTDIvQVVyNWRCaFNEMi94RDFZTEtvWT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4IQMovwHtwkF4VYPoHbKxJw!!)

**[61]** Lucey,P.,Cohn,J.F.,Kanade,T.,Saragih,J.,&Matthews,I.(2010).The Extended Cohn-Kanade Dataset (CK+):A complete dataset for action unit and emotion-specified expression.2010IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops (pp.94-101).New York,NY:IEEE.doi:10.1109/CVPRW.2010.5543262







### LX笔记标题

关键词：

标签：

---

```ad-note
title: Literature Note
collapse: open
color: 88, 88, 124
这是关于 的笔记。
```

```ad-note
title: Reference Note
collapse: open
color: 152, 178, 143
原文如下：

```

```ad-note
title: Relavant Note
collapse: open
color: 142, 106, 120
- 其他笔记也有提及到：
- 其他链接：
```


