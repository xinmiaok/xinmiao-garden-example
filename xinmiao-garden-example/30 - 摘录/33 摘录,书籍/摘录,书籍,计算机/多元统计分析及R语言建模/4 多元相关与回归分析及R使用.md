---
书名: 多元统计分析及R语言建模
---

```ad-info
title: <u></u>**描述**
collapse: open
color: 233, 244, 240

**◀️ 父节点| ▶️ 子节点** 

🪁 status: #🔖 
🎏 class: #📸  

description ::  【目的要求】要求学生在已具有的（一元）线性相关分析与回归分析的基础知识上, 掌握和应用多元线性相关分析与回归分析。【教学内容】变量间的关系分析; 简单相关分析与回归分析; 多元相关分析与回归 分析的目的和基本思想; 多元回归分析的数学模型; 基本假定和最小二乘求法; 回归系数的假设检验; 变量选择及逐步回归分析方法; 非线性回归模型的计算。

来源::

📎

```

变量间的关系分析; 简单相关分析与回归分析; 多元相关分析与回归 分析的目的和基本思想; 多元回归分析的数学模型; 基本假定和最小二乘求法; 回归系数的假设检验; 变量选择及逐步回归分析方法; 非线性回归模型的计算。

## 4.1 变量间的关系分析
变量间的关系有两类, 
- 一类是变量间==存在着完全确定的关系==, 这类变量间的关系称为**函数关系**; 
- 另一类是变量间关系==不存在完全的确定性, 不能用精确的数学公式来表示==, 
	- 这些变量间都存在着<span style="background:rgba(240, 200, 0, 0.2)">十分密切的关系</span>, 
	- 但<span style="background:rgba(240, 200, 0, 0.2)">不能</span>由一个或几个变量的值<span style="background:rgba(240, 200, 0, 0.2)">精确地求出</span>另一个变量的值, 
	- 这些变量间的关系称为**相关关系**, 存在相关关系的变量称为<span style="background:rgba(240, 107, 5, 0.2)">相关变量</span>。

**相关变量**间的关系有两种：
- 一种是==平行关系==，
	- 即两个或两个以上变量之间<span style="background:rgba(136, 49, 204, 0.2)">相互影响</span>; 
- 另一种是==依存关系==, 即一个<span style="background:rgba(74, 82, 199, 0.2)">变量的变化受另一个或几个变量的影响</span>。
- [[相关分析]]是研究呈==平行关系==的相关变量之间的关系, 
- 而[[回归分析]]是研究呈==依存关系==的相关变量间的关系。 
- 表示<span style="background:rgba(5, 117, 197, 0.2)">原因</span>的变量称为**自变量** (independent variable), 
- 表示<span style="background:rgba(5, 117, 197, 0.2)">结果</span>的变量称为**因变量**( dependent variable)。

变量间的关系及分析方法如下:
变量间的关系
- 函数关系 (确定性关系)一一有精确的数学表达式 (数学模型)
- 相关关系 (非确定性关系)
	- ==平行关系==
		- 一元相关分析
		- 多元相关分析
	- ==依存关系==
		- [[一元回归分析]]
		- [[多元回归分析]]


### 4.1.1 简单相关分析的 R 计算


```ad-todo
title: **P56P4相关关系**
collapse: open
color: 233, 243, 242
来源:: [[多元统计分析及R语言建模]] 王斌会

标签：#笔记/论述参考/统计 #多元统计分析 

**◀️ 父节点| ▶️ 子节点** 

description ::  前言

**Literature Note**

这是关于[[相关关系]]的笔记。

*经济方面的论述也适用于园林中，很多变量也是[[相关关系]]，可以用[[相关分析]]加以研究*

---
**Reference Note**

>[[相关分析]]
- 就是通过对大量数字资料的观察, 
- 消除偶然因素的影响, 
- 探求现象之间==相关关系==的密切程度和表现形式。
- 研究现象之间相关关系的理论方法称为相关分析法。

在经济系统中, 各个经济变量常常存在密切的关系, 
- 例如经济增长与财政收人、人均收人与消费支出等。
- 在这些关系中，有一些是==严格的函数关系==，这类关系可以用==数学表达式==表示出来。
	- 例如，在价格一定的条件下，<span style="background:rgba(240, 107, 5, 0.2)">商品销售额与销售量的依存关系</span>。
- 还有一些是==非确定的关系==, 一个变量产生变动会影响其他变量, 使其产生变化。
	- 其变化具有随机的特性, 但是仍然遵循一定的规律。
- 对此, 函数关系很容易解决, 而那些非确定的相关关系才是我们所关心的问题。
	- 因为在经济系统中, 绝大多数经济变量之间的关系是==非严格的、不确定的==。
--- 

<font color="#5f497a">Relavant Note</font>
- 其他笔记也有提及到：
- 其他链接：
```



相关分析
- 以现象之间<span style="background:rgba(240, 200, 0, 0.2)">是否相关</span>、<span style="background:rgba(240, 107, 5, 0.2)">相关的方向</span>和<span style="background:rgba(163, 67, 31, 0.2)">密切程度</span>等为**主要研究内容**, 
- 它不区别自变量与因变量, 也不关心各变量的构成形式。
- 其主要**分析方法**有
	- ==绘制相关图==、
	- ==计算相关系数==和
	- ==检验相关系数==。
#### 1.两变量线性相关系数的计算
在所有[[相关分析]]中, 
- 最简单的是两个变量之间的线性相关, 它只涉及两个变量。
- 而且一变量数值发生变动, 另一变量的数值也随之发生大致均等的变动, 
- 其各点的分布在平面图上近似地表现为一<span style="background:rgba(240, 200, 0, 0.2)">直线</span>, 这种相关关系就称为<span style="background:rgba(240, 200, 0, 0.2)">直线相关</span> (也叫<span style="background:rgba(240, 200, 0, 0.2)">线性关系</span>)。

[[线性相关分析]]是用[[相关系数]]来表示两个变量间相互的线性关系, 并判断其密切程度的统计方法。==总体相关系数==通常用  $\rho$  表示。其计算公式为:

$$\rho=\frac{\operatorname{cov}(x, y)}{\sqrt{\operatorname{var}(x) \operatorname{var}(y)}}=\frac{\sigma_{x y}}{\sqrt{\sigma_{x}^{2} \sigma_{y}^{2}}}$$

- 式中,  
	- $\sigma_{x}^{2}$  为变量  x  的总体方差,  
	- $\sigma_{y}^{2}$  为变量  y  的总体方差, 
	 - $\sigma_{x y}$  为变量  x  与变量  y  的总体协方差。
	 - [[相关系数]]  $\rho$  没有单位, 
		 - 在 -1 至 +1 范围内波动, 
		 - 其绝对值愈接近 1 , 两个变量间的直线相关性愈密切；
		 - 愈接近 0 ，相关性愈不密切。

在实际中, 我们通常要计算==样本的线性相关系数== (Pearson 相关系数), 其计算 公式为:

$$r=\frac{s_{x y}}{\sqrt{s_{x}^{2} \cdot s_{y}^{2}}}=\frac{l_{x y}}{\sqrt{l_{x x} \cdot l_{y y}}}=\frac{\sum(x-\bar{x})(y-\bar{y})}{\sqrt{\sum(x-\bar{x})^{2} \sum(y-\bar{y})^{2}}}$$

- 式中,  
	- $s_{x}^{2}$  为变量  x  的[[样本方差]],  
	- $s_{y}^{2}$  为变量  y  的[[样本方差]],  
	- $s_{x y}$  为变量  x  与变量  y  的[[样本协方差]],  
	- $l_{x x}$  为  x  的[[离均差平方和]],  
	- $l_{y y}$  为  y  的[[离均差平方和]],  
	- $l_{x y}$  为  x  与  y  的离均差乘积之和, 简称为[[离均差积和]], 其值可正可负。
	- 实际计算时可按下式简化:
		- $$\left\{\begin{array}{l}l_{x x}=\sum(x-\bar{x})^{2}=\sum x^{2}-\frac{\left(\sum x\right)^{2}}{n} \\l_{y y}=\sum(y-\bar{y})^{2}=\sum y^{2}-\frac{\left(\sum y\right)^{2}}{n} \\l_{x y}=\sum(x-\bar{x})(y-\bar{y})=\sum x y-\frac{\left(\sum x\right)\left(\sum y\right)}{n}\end{array}\right.$$

##### 【例 4-1】( (续例 2-2) 身高与体重的相关关系分析
下面以例  2-2  的身高与体重数据分析。首先通过散点图看身高与体重的关系，见下图。
为了使大家进一步熟悉  $\mathrm{R}$  语言编程, 我们先建立一个离均差积和函数  $l_{x y}$  :

$$\begin{array}{l}
l_{x x}=556.9, l_{y y}=813, l_{x y}=645.5 \\
r=\frac{l_{x y}}{\sqrt{l_{x x} \cdot l_{y y}}}=\frac{645.5}{\sqrt{556.9 \times 813}}=0.9593
\end{array}$$

```
 x1=c(171,175,159,155,152,158,154,164,168,166,159,164)  #身高  
 x2=c(57,64,41,38,35,44,41,51,57,49,47,46)  #体重 
 plot(x1, x2)  #作散点图
```

```
lxy<- function(x,y)\{n=length(x);sum(x*y)-sum}(x) * sum(y) / n}  #离均差乘积和函数  
lxy(x1,x1) # x1  的离均差平方和
```
[1] 556.9

```
lxy(x2, x2) # x2  的离均差平方和
```
 [1] 813 

 ```
 lxy(x1,x2) # x1  和x2  的离均差乘积和
 ```
[1] 645.5

```
 (r=lxy(x1,x2) /sqrt(lxy(x1,x}1) * lxy(x2, x2)))  #显示用离均差乘积和计算的相关系数
 ```
[1] 0.9593

这里  r  为正值, 说明该组人群的身高与体重之间呈现正的线性相关关系。至于相关 系数  r  是否显著, 尚需进行假设检验。下面是  $\mathrm{R}$  语言中自带的求相关系数的函数。

相关系数计算函数[[cor]]()的用法

```
cor(x, y=NULL, method =c(" pearson" , "kendall" , " spearman"))
```
 
- x: 数值向量、矩阵或数据框;
- y: 空或数值向量、矩阵或数据框 
- method : 计算方法,包括" pearson"," kendall" 或" spearman" 三种, 默认 pearson

```
cor(x1, x2)  #计算相关系数
```

 [1] 0.9593 
 
#### 2.相关系数的假设检验
 r  与其他统计指标一样, 也有==抽样误差==。
 - 从同一总体内抽取若干大小相同的样本, 各样本的相关系数总有波动。
 - 要判断不等于 0 的  r  值是来自
	 - 总体相关系数  $\rho=0$  的总体
	 - 还是来自  $\rho \neq 0$  的总体, 
	 - 必须进行[[显著性检验]]。
##### 方法：t检验
由于来自  $\rho=0$  的总体的所有样本相关系数呈对称分布, 
- 故  r  的显著性可用  [[t检验]]来进行。
- 根据例  4-1  的资料, 对  r  进行  [[t检验]]的步骤为:

###### (1) 建立检验假设
$H_{0}: \rho=0, H_{1}: \rho \neq 0(\alpha=0.05)$
###### (2) 计算相关系数  r  的  t  值:
$$t_{r}=\frac{r-0}{\sqrt{\frac{1-r^{2}}{n-2}}}=\frac{0.9593 \sqrt{12-2}}{\sqrt{1-0.9593^{2}}}=10.74$$

```
n=length  (x1)  #向量的长度
tr=r/sqrt((1-r^2) /n-2))  #相关系数假设检验 t统计量
tr
```

 [1] 10.74 
###### (3) 计算t值和P值，作结论
$\text { 相关系数检验函数 cor.test() 的用法}$
```
cor. test  (x, y , alternative  =c(  "two. sided" ," less" ," greater"  ) ,method=c( "pearson" ,"kendall" ,"spearman" ) ,...) 
```

- x,y: 数据向量(长度相同)
- alternative: 备择假设,
	- " two. sided"（双侧），
	- "greater"（右侧)
	- 或"less"（左侧） 
- method:计算方法,
	- 包括" pearson", "kendall" 或" spearman"三种

```
cor. test  (x1, x2)  #相关系数假设检验
```

- - - 

Pearson's product-moment correlation

data:  x  and  y 

t=10.74, df=10, p- value  =8.21e-07 

alternative hypothesis: true correlation is not equal to 0

95 percent confidence interval :

 0.8575 0.9888 

sample estimates :

cor

0.9593
- - - 
由于  $P=8.21 \mathrm{e}^{-07}<0.05$ , 
- 于是在==显著性水平==  $\alpha=0.05$  上拒绝  $H_{0}$ , 接受  $H_{1}$ , 
- 可认为该人群身高与体重呈现==正的线性关系==。

注意: [[相关系数]]的显著性与<span style="background:rgba(240, 200, 0, 0.2)">自由度</span>有关, 
- 如  n=3, n-2=1  时, 虽然  r=-0.9070 , 却为不显著; 
- 当  n=400  时, 即使  r=-0.1000 , 亦为显著。
- 因此不能只看  r  的值就下结论, 还需看其<span style="background:rgba(136, 49, 204, 0.2)">样本量</span>的大小。

### 4.1.2 简单回归分析的  R  计算
#### 一、简单回归模型的描述
[[简单回归模型]]是通过回归分析研究两变量之间的依存关系, 将变量区分出自变量和因变量, 并研究确定自变量和因变量之间的具体关系的方程形式。
- 分析中所形成的这种关系式称为回归模型, 
	- 其中以一条直线方程表明两变量依存关系的模型叫**单变量（一元）线性回归模型**。

其**主要步骤**包括：
- ==建立回归模型==、
- ==求解回归模型中的参数==、
- ==对回归模型进行检验==等。

#### 二、简单回归模型的参数估计
在对因变量和自变量所作的散点图中, 如果趋势大致呈直线, 则可拟合一条直线方程。

直线方程的模型为:  $\hat{y}=a+bx$ 
- 式中,  $\hat{y}$  为因变量  y  的估计值,  
- x  为自变量的实际值,  
- a 、 b  为待估参数。
	- 其几何意义是: 
		-  a  是直线方程的截距,  b  是斜率。
	- 其经济意义是:  
		- a  是当  x  为 0 时  y  的估计值,  
		- b  是 当  x  每增加一个单位时  y  增加的数量。  
	- b  也叫回归系数。

配合回归直线的目的
- 是要找到一条==理想的直线==, 用直线上的点来代表所有的相关点。 
- 数理统计证明, 用==最小平方法==配合的直线最理想, 最具有代表性。
- 计算  a  与  b  常用[[最小二乘估计]] (least square estimate) 的方法。

由前面的散点图可见, 虽然  x  与  y  间有直线趋势存在, 但并不是一一对应的。
- 每一 个值  $x_{i}$  与对  $y_{i}(i=1,2, \cdots, n)$  用回归方程估计的  $\hat{y}_{i}$  值（即直线上的点）或多或少存在一 定的差距。
	- 这些差距可以用  $\left(y_{i}-\hat{y}_{i}\right)$ 来表示, 称为**估计误差或残差**（residual)。
- 要使回归方程比较 “理想”, 很自然会想到应该使这些估计误差尽量小一些, 也就是使**估计误差平方和**

$$Q=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left[y_{i}-\left(a+b x_{i}\right)\right]^{2}$$

达到最小。对  Q  求关于  a  和  b  的偏导数, 并分别令其等于零, 可得:

$$b=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}}=\frac{l_{x y}}{l_{x x}}, a=\bar{y}-b \bar{x}$$

式中,  
- $l_{x x}$  表示  x  的==离差平方和==,  
- $l_{x y}$  表示  x  与  y  的==离差积和==。

#### 三、建立直线回归方程的步骤
由散点图观察实测样本资料是否存在一定的协同变化趋势, 
- 这种趋势是==否是直线==的, 
- 然后根据是否有直线趋势确定==应拟合直线还是曲线==。

由本例资料绘制的散点图可见, 
- 身高与体重之间存在明显的线性趋势，
- 所以可考虑建立直线回归方程。

要考察  x  与  y  之间的数量关系, 需建立线性回归方程, 以便进行分析、估计和预测。 

##### 【例 4-2】下面仍以例 2-2 的数据来介绍建立直线回归方程的步骤

```
x=x1  #自变量,数据来自例  2-2 
y=x2  #因变量,数据来自例  2-2 
b=lxy(x, y) / lxy(x,x)  #线性回归方程斜率
a=mean(y)-b * mean(x)  #线性回归方程截距
c(a=a, b=b)  #显示线性回归方程估计值
 ```
 
 $$\begin{array}{ll}\text { a } & \text { b } \\ -140.364 & 1.159\end{array} $$

于是得到回归方程:  $\hat{y}=-140.364+1.159x$ 

建立回归方程后, 一般应将回归方程在散点图上表示出来, 也就是作回归直线。
- 作图时可在自变量  x  的实测范围内任取两个相距相对较远的数值  $x_{1}$ 、 $x_{2}$  代入回归方程, 
- 计算得到  $\hat{y}_{1}, \hat{y}_{2}$ , 用  $\left(x_{1}, \hat{y}_{1}\right) 、\left(x_{2}, \hat{y}_{2}\right)$两点即可作出回归直线, 如下图所示。

```
plot(x, y)  #作散点图
lines  (x,a+b*x)  #添加估计方程线
```

简单相关分析与回归分析; 多元相关分析与回归 分析的目的和基本思想; 多元回归分析的数学模型; 基本假定和最小二乘求法; 回归系数的假设检验; 变量选择及逐步回归分析方法; 非线性回归模型的计算。

#### 四、回归系数的假设检验
由样本资料建立回归方程的目的
- 是对两个变量的回归关系进行推断, 
- 也就是对总体回归方程作估计。

由于抽样误差, 样本回归系数  b  往往不会恰好等于总体回归系数  $\beta$  。
- 如果总体回归系数  $\beta=0$ , 
	- 那么当  $\hat{y}$  是常数, 
		- 无论  x  如何变化, 都不会影响  $\hat{y}$ , 
		- 回归方程就没有意义。
- 当总体回归系数  $\beta$=0  时, 
	- 由样本资料计算得到的样本回归系数  b  不一定为 0 , 
	- 所以有必要对估计得到的样本回归系数  b  进行检验。
- 检验一般用[[方差分析]]或  [[t 检验]], 
	- 两者的检验结果是等价的。
	- 方差分析主要是针对整个模型的, 
	- 而  [[t 检验]]是关于回归系数的

##### 1.方差分析
经回归分析, 因变量  y  实测值的离均差平方和  $S S_{T}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}=l_{y y}$ , 被分解成两个部分。
- 第一部分为  $S S_{E}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}$ , 
	- 其本质是==估计误差的平方和==, 这部分反映了这组实测值  $y_{i}$  扣除了  x  对  y  的线性影响后==剩下的变异==。
- 另一部分为  $S S_{R}=\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}$ , 
	- 反映了  ==x  对  y  的线性影响==，
	- 称为**回归平方和或回归贡献**，
- 不难证明  $S S_{T}=S S_{R}+S S_{E}$  。

根据方差分析的原理, 
- 判断回归贡献是否有意义可以用[[方差分析]]进行检验。
	- 这时总变异的自由度为  $d f_{T}=n-1$ ; 
	- 由于只有一个自变量, 
		- 所以回归自由度  $d f_{R}=1$ ; 
		- 误差自由度  $d f_{E}=d f_{T}-d f_{R}=n-2$  。
	- 有了离差平方和与自由度, 即可分别计算回归均方与误差均方, 进而得到  F值。计算公式如下:

$$M S_{R}=\frac{S S_{R}}{d f_{R}}, M S_{E}=\frac{S S_{E}}{d f_{E}}, F=\frac{M S_{R}}{M S_{E}}$$

其中,

$$\begin{array}{l}
S S_{R}=\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}=b \sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)\left(x_{i}-\bar{x}\right)=b l_{x y} \\
S S_{E}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}-\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}
\end{array}$$

对例 4-2 作方差分析:
- $H_{0}$  : 模型无意义, 即  $\beta=0$ 
- $H_{1}$  : 模型有意义, 即  $\beta \neq 0$ 
取  $\alpha=0.05$  。

$$\begin{array}{l}S S_{T}=l_{y y}=813.0 \\S S_{R}=b l_{x y}=1.159 \times 645.533=748.17 \\S S_{E}=S S_{T}-S S_{R}=813.0-748.17=64.83 \\M S_{R}=\frac{748.17}{1}=748.17, M S_{E}=\frac{64.83}{10}=6.483, F=\frac{748.17}{6.483}=115.4\end{array}$$
$F_{1-\alpha}(1, n-2)=F_{0.95}(1,22)=4.3$,
由于  F=115.4>4.3 , 
- 所以有  P<0.01 , 
- 于是在  $\alpha=0.05$  水平处拒绝  $H_{0}$ , 
- 即本例回归系数有统计学意义,  x  与  y  间存在直线回归关系。

$$\begin{array}{ll}
\mathrm{SST}=\operatorname{lxy}(\mathrm{y}, \mathrm{y}) & \text { \#因变量的离均差平方和 } \\
\mathrm{SSR}=\mathrm{b} * \operatorname{lxy}(\mathrm{x}, \mathrm{y}) & \text { \#回归平方和 } \\
\mathrm{SSE}=\mathrm{SST}-\mathrm{SSR} & \text { \#误差平方和 } \\
\mathrm{MSR}=\mathrm{SSR} / 1 & \text { \#回归均方 } \\
\mathrm{MSE}=\mathrm{SSE} /(\mathrm{n}-2) & \text { \#误差均方 } \\
\mathrm{F}=\mathrm{MSR} / \mathrm{MSE} & \text { \#F 统计量 } \\
\mathrm{c}(\mathrm{SST}=\mathrm{SST}, \mathrm{SSR}=\mathrm{SSR}, \mathrm{SSE}=\mathrm{SSE}, \mathrm{MSR}=\mathrm{MSR}, \mathrm{MSE}=\mathrm{MSE}, \mathrm{F}=\mathrm{F}) \quad \text { \#显示结果 }
\end{array}
$$

- - -
$$\begin{array}{lllclc}\text { SST } & \text { SSR } & \text { SSE } & \text { MSR } & \text { MSE } & \text { F } \\813.000 & 748.173 & 64.827 & 748.173 & 6.483 & 115.412\end{array}$$
- - -

##### 2.t  检验
当  $\beta=0$  成立时, 样本回归系数  b  服从正态分布。
- 所以也可用  [[t 检验]]的方法检验  b  是 否有统计学意义。
- 检验时用的统计量为:

$$\begin{array}{l}
t=\frac{b-\beta}{s_{b}} \sim t(n-2) \\
s_{b}=\frac{s_{y \cdot x}}{\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}_{i}\right)^{2}}}=\frac{s_{y \cdot x}}{\sqrt{l_{x x}}} \\
s_{y \cdot x}=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}}{n-2}}=\sqrt{\frac{S S_{E}}{n-2}}=\sqrt{M S_{E}}
\end{array}$$

上式中,  
- $s_{y} \cdot x$  称为==剩余标准差或标准估计误差== (standard error of estimate), 
	- 是误差的均方根, 它反映了因变量  y  在扣除自变量  x  的线性影响后的离散程度。  
- $s_{y} \cdot{ }_{x}$  可以与  y  的标准差  $s_{y}$  比较, 
	- 从而可看出自变量  x  对  y  的线性影响的大小。
- 上式中,  $s_{b}$  被称为样本回归 系数  b  的标准误差。

对例 4-2 作  t  检验:  
$H_{0}: \beta=0, H_{1}: \beta \neq 0, \alpha=0.05$ 

$$\begin{array}{l}
s_{y \cdot x}=\sqrt{M S_{E}}=\sqrt{6.483}=2.5462 \\
s_{b}=\frac{2.5462}{\sqrt{556.9}}=0.1079 \\
t_{b}=\frac{1.159}{0.1079}=10.74
\end{array}$$

 $|t|>t_{(1-\alpha / 2, n-2)}=t_{(1-0.05 / 2,10)}=2.2281, P<0.05$  。
 - 于是, 在  $\alpha=0.05$  水平处拒绝  $H_{0}$ , 
 - 接受  $H_{1}$ , 
 - 即本例回归系数有统计学意义,  x  与  y  间存在回归关系。

$$\begin{array}{ll}
\text { sy. } x=\operatorname{sqrt}(\text { MSE }) & \text { \#估计标准差 } \\
\mathrm{sb}=\mathrm{sy} \cdot \mathrm{x} / \mathrm{sqrt}(\operatorname{lxy}(\mathrm{x}, \mathrm{x})) & \text { \#离均差平方和 } \\
\mathrm{t}=\mathrm{b} / \mathrm{sb} & \text { \# } \mathrm{t} \text { 统计量 } \\
\mathrm{ta}=\mathrm{qt}(1-0.05 / 2, \mathrm{n}-2) & \text { \# } \text { 分位数 } \\
c(\mathrm{sy} \cdot \mathrm{x}=\mathrm{sy} \cdot \mathrm{x}, \mathrm{sb}=\mathrm{sb}, \mathrm{t}=\mathrm{t}, \mathrm{ta}=\mathrm{ta}) & \text { \#显示结果 }
\end{array}$$

- - -
$$\begin{array}{lccl}\text { sy. x } & \text { sb } & \text { t } & \text { ta } \\2.5462 & 0.1079 & 10.7430 & 2.2281\end{array}$$
- - -

上面我们通过  R  语言编程的方式对两变量进行了回归分析, 
- 目的是使大家熟悉  R  语 言的编程技巧。
- 实际上, 在进行[[线性回归分析]]时, 可直接应用  R  语言自身的拟合线性模
型的函数  $1 \mathrm{m}$  进行，下面我们就用  $l\mathrm{m}$  函数进行线性回归分析。

##### 【例 4-3】财政收入与税收
我们知道, 财政收入与税收有着密切的依存关系。
- 以下收集了我国自 1978 年改革开放以来到 2008 年共 31 年的税收 (  x , 百亿元) 和财政收入 (y , 百亿元  )  数据, 
- 见表 4-1, 试分析税收与财政收入之间的依存关系。
- ![[Snipaste_2023-04-06_08-54-49.png]]

要考察它们之间的数量关系, 需**建立线性回归方程**, 以便进行分析、估计和预测 步骤如下:
###### (1) 读入数据
```
yx=read. table ("clipboard", header=T) #加载例4-3数据 
```
###### (2) 拟合模型
```
fm=lm(y-x, data=yx) #一元线性回归模型 
fm
```

$$\begin{array}{l}
\text { Call : } \operatorname{lm}(\text { formula }=\mathrm{y} \sim \mathrm{x}) \\
\text { Coefficients: } \\
\begin{array}{ll}
\text { (Intercept) } & \mathrm{x} \\
-1.197 & 1.116
\end{array}
\end{array}$$

于是得到回归方程:  $\hat{y}=-1.197+1.116 x$  。

###### (3) 作回归直线
 ```
 plot(yx, data=yx) #作散点图 
 abline(fm)  #添加回归线
 ```
 ![[Pasted image 20230406093646.png]]

###### (4) 回归方程的假设检验。
**1)模型的[[方差分析]] (ANOVA)**
[[fm]]()
```
anova(fm) #模型方差分析 
```

$$Analysis of Variance Table$$
$Response: y$
$$\begin{array}{ccccccc} & \text { Df } & \text { Sum Sq } & \text { Mean Sq } & \text { F value } & \operatorname{Pr}(>\mathrm{F}) \\ \mathrm{x} & 1 & 712076.834 & 712076.834 & 27428.1326 & <2.22 \mathrm{e}-16 & * * * \\ \text { Residuals } & 29 & 752.885 & 25.962 & & \end{array} 
Signif. codes: 0 '***'  0.001^{\prime} * *  '  0.01^{\prime} *  '  0.05^{\prime} . 0.1$$
$\text { Signif. codes: } 0 \text { '***' } 0.001^{\prime} * * \text { ' } 0.01^{\prime} * *^{\prime} 0.05 \text { ' } 0.1$

由于  P<0.05 , 
- 于是在  $\alpha=0.05$  水平处拒绝  $H_{0}$ , 
- 即本例回归系数有统计学意义,  x  与  y  间 存在==直线回归关系==。

**2)回归系数的 [[t检验]]**
[[summary]]()
```
summary(fm) #回归系数的t检验
```

$\operatorname{lm}(\text { formula }=y \sim x)$

$Residuals :$

$$\begin{array}{crrrl}
{\text { Min }} & 1 \mathrm{Q} & \text { Median } & 3 Q & {\text { Max }} \\
-6.6295697 & -3.6919399 & -1.5350531 & 5.3382063 & 11.4319756 \\
\text { Coefficients : } & & & & \\
& \text { Estimate } & \text { Std. Error } & \mathrm{t} \text { value } & \operatorname{Pr}(>|\mathrm{t}|) \\
(\text { Intercept) } & -1.196562984 & 1.161245228 & -1.03041 & 0.31133 \\
\mathrm{x} & 1.116225390 & 0.006739905 & 165.61441 & <2 \mathrm{e}-16
\end{array}$$
$Signif. codes:  0 *^{*} * * ' 0.001 *^{*} *^{\prime} 0.01  '* 0.05 '. '0.1' '1$

$Residual standard error: 5.0952478 on 29 degrees of freedom$

$Multiple R -squared: 0.99894381, Adjusted R -squared: 0.99890739$

$F - statistic: 27428.133  on 1 and  29 \mathrm{DF}, \quad \mathrm{p}  - value:  <2.22045 \mathrm{e}-16$


由于  P<0.05 , 
-  于是在 $\alpha=0.05$  水平处拒绝  $H_{0}$ , 接受  $H_{1}$ , 
- 即本例回归系数有统计学意义, x  与  y  间存在回归关系。

注意: 本例  $t^{2}=F\left(165.61441^{2}=27428.1328\right)$ , 
- 当  $d f_{R}=1$  时,  
	- t  值的平方等于  F  值 (  $d f_{E}$  即为  t  的自由度  n-2  ) 。
- 所以说当==自变量只有一个==时, <span style="background:rgba(240, 200, 0, 0.2)">方差分析与  t  检验的结果是等价的</span>。
- 但在下面的多元分析中, 方差分析与  t  检验的结果并不等价。

## 4.2 多元线性回归分析
[[回归分析]]研究
- 的**主要对象**
	- 是客观事物==变量间的统计关系==。
- 它是建立在对客观事物进行大量实验和观察的基础上, 用来寻找隐藏在看起来<span style="background:rgba(240, 200, 0, 0.2)">不确定的现象中的统计规律的统计方法</span>。
- 它**与[[相关分析]]的主要区别**为: 
	- 一是
		- 在回归分析中, 
			- 解释变量称为自变量, 
			- 被解释变量称为==因变量==, 处于被==解释的特殊地位==; 
		- 在[[相关分析]]中, 
			- 并<span style="background:rgba(240, 200, 0, 0.2)">不区分自变量和因变量</span>, 
			- 各变量处于<span style="background:rgba(240, 200, 0, 0.2)">平等地位</span>。
	- 二是
		- 在[[相关分析]]中
			- 所涉及的<span style="background:rgba(240, 200, 0, 0.2)">变量</span>全是[[随机变量]]; 
		- 在[[回归分析]]中, 
			- ==因变量==是[[随机变量]], 
			- 而==自变量==可以是[[随机变量]], 也可以是[[非随机变量]]。
	- 三是
		- [[相关分析]]研究主要
			- 是为刻画两类变量间的<span style="background:rgba(240, 200, 0, 0.2)">线性相关的密切程度</span>; 
		- 而[[回归分析]]
			- 不仅可以揭示==自变量对因变量的影响大小==, 
			- 还可以==用回归方程进行预测和控制==。

### 4.2.1 多元线性回模型建立
上一节已经介绍了[[简单回归分析]], 
- 它研究的是一个因变量与一个自变量间呈直线趋势的数量关系。
- 在实际中, 常会遇到==一个因变量与多个自变量==数量关系的问题。
	- 如在例 4-3 中考察的是 1978-2008 年我国<span style="background:rgba(240, 200, 0, 0.2)">财政收人与税收之间的线性关系</span>, 
	- 如果我们想进一步考察<span style="background:rgba(240, 107, 5, 0.2)">财政收人和国民生产总值、税收、进出口贸易总额、经济活动人口之间的依存关系</span>, 就需要建立[[多元回归模型]]。
- 与简单回归 (直线回归) 类似, 
	- 一个<span style="background:rgba(3, 135, 102, 0.2)">因变量与多个自变量间的这种线性数量关系</span>可以用==多元线性回归方程==来表示。

$$\hat{y}=b_{0}+b_{1} x_{1}+b_{2} x_{2}+\cdots+b_{p} x_{p}$$

- 式中,  
	- $b_{0}$  相当于直线回归方程中的常数项  a, 
	- $b_{i}(i=1,2, \cdots, p)$  称为[[偏回归系数]]（partial regression coefficient), 
		- 其意义与直线回归方程中的[[回归系数]]  b  相似。
		- 当其他自变量对因变量的线性影响固定时,  
			- $b_{i}$  反映了==第  i  个自变量 $x_{i}$  对因变量  y  线性影响的大小==。
				- 这样的回归称为<span style="background:rgba(240, 107, 5, 0.2)">因变量  y  在这一组自变量  x  上的回归,</span> 
					- 习惯上称之为[[多元线性回归模型]]。
#### 1.多元线性回归模型的一般形式
==[[随机变量]]==  y  与==[[一般变量]]==  x  的**线性回归模型**为:
$$y=\beta_{0}+\beta_{1} x_{1}+\beta_{2} x_{2}+\cdots+\beta_{p} x_{p}+\varepsilon$$

当我们得到  n  组观测数据  $\left(x_{1}, x_{2}, \cdots, x_{p}, y_{i}\right)$  时,  $i=1,2, \cdots, n$ , 线性回归模型可表示为:

$$\left\{\begin{array}{c}
y_{1}=\beta_{0}+\beta_{1} x_{11}+\beta_{2} x_{12}+\cdots+\beta_{p} x_{1 p}+\varepsilon_{1} \\
y_{2}=\beta_{0}+\beta_{1} x_{21}+\beta_{2} x_{22}+\cdots+\beta_{p} x_{2 p}+\varepsilon_{2} \\
\vdots \\
y_{n}=\beta_{0}+\beta_{1} x_{n 1}+\beta_{2} x_{n 2}+\cdots+\beta_{p} x_{n p}+\varepsilon_{n}
\end{array}\right.$$

将其写成矩阵形式  $y=X \beta+\varepsilon$ , 其中,

$$y=\left[\begin{array}{c}
y_{1} \\
y_{2} \\
\vdots \\
y_{n}
\end{array}\right], X=\left[\begin{array}{ccccc}
1 & x_{11} & x_{12} & \cdots & x_{1 p} \\
1 & x_{21} & x_{22} & \cdots & x_{2 p} \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & x_{n 1} & x_{n 2} & \cdots & x_{n p}
\end{array}\right], \beta=\left[\begin{array}{c}
\beta_{0} \\
\beta_{1} \\
\vdots \\
\beta_{p}
\end{array}\right], \varepsilon=\left[\begin{array}{c}
\varepsilon_{1} \\
\varepsilon_{2} \\
\vdots \\
\varepsilon_{n}
\end{array}\right]$$

- 通常称  X  为[[设计阵]],  
- $\beta$  为[[回归系数向量]]。

#### 2.线性回归模型的基本假设
- 由于一元线性回归比较简单, 
	- 其趋势图可用==散点图==直观显示,
	-  所以, 我们对其性质和假定并末作详细探讨。
- 实际上, 我们在建立线性回归模型前, 需要**对模型作一些假定**。 
	- 如**经典线性回归模型的基本假设前提**为:

##### (1) ==解释变量==一般来说是[[非随机变量]]。

##### (2) ==误差等方差及不相关假定== ( G-M  条件):

$$\left\{\begin{array}{l}
E\left(\varepsilon_{i}\right)=0, i=1,2, \cdots, n \\
\operatorname{cov}\left(\varepsilon_{i}, \varepsilon_{j}\right)=\left\{\begin{array}{l}
\sigma^{2}, i=j \\
0, i \neq j
\end{array} \quad i, j=1,2, \cdots, n\right.
\end{array}\right.$$

##### (3) 误差正态分布的假定条件为:

$$\varepsilon_{i} \stackrel{i i d}{\sim} N\left(0, \sigma^{2}\right), i=1,2, \cdots, n$$

##### (4)  n>p , 即要求样本容量个数多于解释变量的个数

#### 3.多元回归参数的最小二乘估计

从多元线性模型的矩阵形式  $y=X \beta+\varepsilon$  可知, 
- 若模型的参数  $\beta$  的估计量  $\hat{\beta}$  已获得, 
	- 则  $\hat{y}=X \hat{\beta}$ , 
- 于是残差  $\varepsilon_{i}=y_{i}-\hat{y}_{i}$ , 
	- 根据[[最小二乘法]]的原理, 
	- 所选择的估计方法应使估计值  $\hat{y}_{i}$  与观察值  $y_{i}$  之间的残差  $\varepsilon_{i}$  在所有样本点上达到最小，即
		- $$Q=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}=\varepsilon^{\prime} \varepsilon=(y-X \hat{\boldsymbol{\beta}})^{\prime}(y-X \hat{\boldsymbol{\beta}})$$
		- 达到最小, 根据微积分求极值的原理,  Q  对  $\hat{\beta}$  求导且等于 0 , 
		- 可求得使  Q  达到最小的  $\hat{\beta}$ , 这就是所谓的==最小二乘  (LS)  法==。

$$\frac{\partial Q}{\partial \hat{\beta}}=\frac{\partial(y-X \hat{\boldsymbol{\beta}})^{\prime}(y-X \hat{\boldsymbol{\beta}})}{\partial \hat{\boldsymbol{\beta}}}
$$
$$\begin{aligned}
& =\frac{\partial}{\partial \hat{\beta}}\left(y^{\prime}-\hat{\beta}^{\prime} X^{\prime}\right)(y-X \hat{\boldsymbol{\beta}}) \\
& =\frac{\partial}{\partial \hat{\beta}}\left(y^{\prime} y-\hat{\beta}^{\prime} X^{\prime} y-y^{\prime} X \hat{\beta}+\hat{\beta}^{\prime} X^{\prime} X \hat{\beta}\right) \\
& =\frac{\partial}{\partial \hat{\beta}}\left(y^{\prime} y-2 \hat{\beta}^{\prime} X^{\prime} y+\hat{\beta}^{\prime} X^{\prime} X \hat{\beta}\right) \\
& =-2 X^{\prime} y+2 X^{\prime} X \hat{\beta} \\
& =0 \\
X^{\prime} X \hat{\beta} & =X^{\prime} y \\
\hat{\beta}_{L S} & =\left(X^{\prime} X\right)^{-1} X^{\prime} y
\end{aligned}$$

另外还可证明, 
- 在正态性假定下, 
	- 回归参数  $\beta$  的  LS  估计与==极大似然==  (ML)  估计完全相同，即  $\hat{\beta}_{M L}=\hat{\beta}_{L S}$  ，
	- 关于回归系数的极大似然估计参见有关文献。

#### 【例 4-4】财政收入多元分析。
财政收入
- 是指一个国家政府凭借政府的特殊权力, 
- 按照有关的法律和法规在一定时期内（一般为一年）取得的各种形式收人的总和，
	- 包括税收、企事业收人、国家能源交通重点建设基金收人、债务收人、规费收人及罚没收人等。
- 财政收入水平是反映一国经济实力的重要标志。
- 本例共取五个变量进行分析, 分析财政收人和国内生产总值、税收、 进出口贸易总额、经济活动人口之间的关系。
	- 其中,  
		- t  为年份,  
		- y  为财政收人 (百亿元), 
		-  $x_{1}$  为国内生产总值 (百亿元),  
		- $x_{2}$  为税 收 (百亿元),  
		- $x_{3}$  为进出口贸易总额 (百亿元),  
		- $x_{4}$  为经济活动人口 (百万人)。

本案例的样本数据来自中国统计出版社出版的 《中国统计年鉴》及海关总署（以 2008 年的经济活动人口为测算值), 数据时限为 1978-2008 年, 数据详见表 4-2。

在例 4-3 中我们发现 1978-2008 年我国财政收人与税收之间的确存在线性回归关系, 为了进一步考察财政收人和其他变量之间的数量关系, 需建立多元线性回归方程, 以便进行分析与预测, 步骤如下:
![[Pasted image 20230406111859.png]]

```
yX = read. table ( "clipboard", header=T) #加载例 4-4 数据
(fm=lm(y~x1+x2+x3+x4, data =yX)) #显示多元线性回归模型
```


$\operatorname{lm}(\text { formula }=y \sim x 1+x 2+x 3+x 4)$

$Coefficients :$

$$\begin{array}{rcrrr}\text { ( Intercept) } & x 1 & x 2 & x 3 & x 4 \\ 23.532109 & -0.003387 & 1.164115 & 0.000292 & -0.043742\end{array}$$

于是得到多元线性回归方程:

\hat{y}=23.532109-0.003387 x_{1}+1.164115 x_{2}+0.000292 x_{3}-0.043742 x_{4}

#### 4.标准化偏回归系数
##### 标准化原因
由于自变量  $x_{j}(j=1,2, \cdots, p)$  与因变量都是有单位的, 
- 从数值上来看, 
	- 它们样本取值的极差会有很大的差异, 
	- 均数与标准差也各不相同, 
- 所以不能由偏回归系数的大小直接说明对因变量线性影响的大小。
	- 对于这个问题常用==变量标准化==与==计算标准化偏回归系数==的方法来处理。

对每一个变量（包括因变量) 标准化后, 
- 再计算方程的偏回归系数, 可得到==标准化偏回归系数==，
	- 常用  $\hat{\beta}_{i}^{*}$  表示：

$$\hat{\beta}_{i}^{*}=\hat{\beta}_{i} \frac{s_{i}}{s_{y}} \quad(i=1,2, \cdots, p)$$

- 式中,  
	- $s_{i}(i=1,2, \cdots, p)$  与  $s_{y}$  分别是各自变量和因变量的标准差。

由于标准化后各变量的均值为 0 , 方差为 1 , 
- 所以标准化后的多元回归方程一定是通过原点的, 
	- 也就是常数项  $\hat{\beta}_{0}=0$  。 
- 由于各变量的标准差  $s_{i}(i=1,2, \cdots, p)$  变得相同, 
	- ==各标准化偏回归系数的值==
		- 可以反映各个自变量在其他自变量固定时==对因变量线性影响的大小==, 
		- 也可相互间==进行比较==。

常用的统计软件都能给出标准化偏回归系数, 
- 但  $\mathrm{R}$  语言中并不包含计算标准回归系数的函数, 
- 因此我们编写了 coef. sd 计算之。
	- 例  4-4  的  $\mathrm{R}$  软件给出标准化偏回归系数如 下:
	-   $$\hat{\beta}_{1}^{*}=-0.01745, \hat{\beta}_{2}^{*}=1.0424, \hat{\beta}_{3}^{*}=0.00096, \hat{\beta}_{4}^{*}=-0.03711 $$
	- 由标准化偏回归系数可见, ==税收对财政收入的线性影响最大==。

```
library(mvstats)
coef.sd(fm) #标准化偏回归系数结果
```

$$\begin{array}{l}\text { \$coef. sd }\\\begin{array}{cccc}x 1 & x 2 & x 3 & x 4 \\-0.017451 & 1.042352 & 0.000963 & -0.037105\end{array}\end{array}$$

### 4.2.2 多元线性回归模型检验
#### 1.回归方程的假设检验 
由样本计算得到的这些偏回归系数  $\hat{\beta}_{j}$  是总体偏回归系数  $\beta_{j}$  的估计值。
- 如果这些==总体偏回归系数等于 0== , 多元回归<span style="background:rgba(240, 107, 5, 0.2)">方程就没有意义</span>。
- 所以与直线回归一样, 在建立起方程后有必要对这些==偏回归系数作检验==。
- 对多元回归方程作假设检验也可以用**[[方差分析]]**。 
	- 因变量  y  的[[离均差平方和]]经[[回归分析]]后被分解成两个部分。

$$S S_{T}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}=\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}+\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2}=S S_{R}+S S_{E}$$

这与单变量回归是一样的。
- 同时, 自由度也被分解成两个部分。
- 其中, [[回归自由度]]就是==自变量的个数==。

$$d f_{R}=p, d f_{E}=d f_{T}-d f_{R}=(n-1)-p=n-p-1$$

由此可==分别计算两部分的均方==:

$$\begin{array}{l}
M S_{R}=S S_{R} / d f_{R}=\sum_{i=1}^{n}\left(\hat{y}_{i}-\bar{y}\right)^{2} / p \\
M S_{E}=S S_{E} / d f_{E}
\end{array}$$

方差分析的检验假设
- 是  $H_{0}: \beta_{1}=\beta_{2}=\cdots=\beta_{p}=0$ , 
	- 这就意味着因变量  y  与所有的自变量 $x_{j}$  都不存在回归关系，多元回归方程没有意义。
- 相应的备择假设$H_{1}: \beta_{1}, \beta_{2}, \cdots, \beta_{p}$不全为  0, $H_{0}$  成立时，有:

$$F=\frac{M S_{k}}{M S_{E}} \sim F(p, n-p-1)$$

即  F 服从 [[F分布]]。
- 这样就可以用  F  统计量来检验回归方程是否有意义。
#### 2.回归系数的假设检验
##### t检验的原因
多元回归方程有统计学意义并不说明每一个偏回归系数都有意义, 
- 所以有必要对每个偏回归系数作检验。
	- 在  $\beta_{j}=0$  时, 
		- 偏回归系数  $\hat{\beta}_{j}(j=1,2, \cdots, p)$  服从==正态分布==, 
		- 所以可用 t统计量对偏回归系数作检验。
##### t检验的公式
**检验假设公式**  
- $H_{0 j}: \beta_{j}=0, H_{1 j}: \beta_{j} \neq 0$  。
	- 当  $H_{0 j}$  成立时, 
		- 而  $\hat{\beta} \sim N\left(\beta, \sigma^{2}\left(X^{\prime} X\right)^{-1}\right)$ , 
		- 记  $\left(X^{\prime} X\right)^{-1}=\left(c_{i j}\right)$  。则我们构造的  t  统计量为:

$$t_{j}=\frac{\hat{\beta}_{j}-\beta_{j}}{s_{\beta_{j}}} \quad j=1,2, \cdots, p$$

- 式中,  $s_{\beta j}^{-}$ 是==第  j  个偏回归系数的标准误差==, 其计算比较复杂。

$$\begin{array}{l}
s_{\beta_{j}}=\sqrt{c_{j j}} s_{y \cdot x} \\
s_{y \cdot x}=\sqrt{\frac{\sum_{i=1}^{n} \varepsilon^{2}}{n-p-1}}=\sqrt{\frac{\sum_{i=1}^{n}\left(y_{i}-\hat{y}_{i}\right)^{2}}{n-p-1}}=\sqrt{\frac{S S_{E}}{d f_{E}}}=\sqrt{M S_{E}}
\end{array}$$

与单变量情形一样,  
- $s_{y} \cdot x_{x}$  称为==剩余标准差或标准估计误差==, 
	- 也反映了因变量  y  在扣除各自变量  x  的线性影响后的<span style="background:rgba(240, 107, 5, 0.2)">变异程度</span>。  
- $s_{y} \cdot{ }_{x}$  可以与  y  的标准差  $s_{y}$  比较, 
	- 从而可看出所有自变量  x  对  y  的线性影响大小。


##### 检验假设判断
当
- 原假设  $H_{0 j}: \beta_{j}=0$  成立时, 
	- 上面的  t  统计量服从自由度为  n-p-1  的  t  分布。
	- 给定显著性水平  $\alpha$ , 查出双侧检验的临界值  $t_{1-\alpha / 2}$  。
- 当  $\left|t_{j}\right| \geqslant t_{1-\alpha / 2}$  时, 
	- 拒绝零假设  $H_{0 j}: \beta_{j}=0$ , 
	- 认为  $\beta_{j}$  显著不为零, 
	- 自变量  $x_{j}$  对因变量  y  的线性效果显著; 
- 当  $\left|t_{j}\right|<t_{1-\alpha / 2}$  时, 
	- 接受零假设  $H_{0 j}: \beta_{j}=0$ , 
	- 认为  $\beta_{j}$  为零, 自变量  $x_{j}$  对因变量  y  的线性效果不显著

一般统计软件在完成多元回归分析的同时都会输出方差分析与  t  检验的结果。
- 其中，  t  检验结果给出了
	- 每个偏回归系数和
	- 常数项的值、
	- 标准误差、  
	- t  值与相应的  P  值。

```
summary(fm) #多元线性回归系数t检验
```

$$\begin{array}{l}
\text { Call: } \\
\operatorname{lm}(\text { formula }=y \sim x 1+x 2+x 3+x 4) \\
\text { Residuals: } \\
\begin{array}{ccccc}
\text { Min } & 1 Q & \text { Median } & 3 Q & \text { Max } \\
-5.02 & -2.14 & 0.33 & 1.26 & 6.97
\end{array} \\
\end{array}$$
$$\begin{array}{l}
\text { Coefficients : }\\
\begin{array}{rrrrrr} 
& \text { Estimate } & \text { Std. Error } & \text { t value } & \operatorname{Pr}(>|\mathrm{t}|) & \\
\text { (Intercept) } & 23.532109 & 4.599071 & 5.12 & 2.5 \mathrm{e}-05 & * * * \\
\text { x1 } & -0.003387 & 0.008075 & -0.42 & 0.68 & \\
\text { x2 } & 1.164115 & 0.040489 & 28.75 & <2 \mathrm{e}-16 & * * * \\
\text { x3 } & 0.000292 & 0.008553 & 0.03 & 0.97 & \\
\text { x4 } & -0.043742 & 0.009264 & -4.72 & 7.0 \mathrm{e}-05 & * * *
\end{array}
\end{array}$$
$Signif. codes:  0 * * * *, 0.001^{*} * * *^{\prime} 0.01^{\prime} * *^{\prime} 0.05^{\circ} ., 0.1^{\prime}, \quad 1$

Residual standard error: 2.79 on 26 degrees of freedom

$Multiple  \mathrm{R} -squared:  \quad 1, \quad  Adjusted  \mathrm{R} -squared: 1$

$\mathrm{F} -statistic:  2.29 \mathrm{e}+04  on 4 and  26 \mathrm{DF}, \mathrm{p}  - value:  <2 \mathrm{e}-16$ 

由方差分析结果可见 (见表  4-3  ), 
- 模型的  F  值为  22893, P<0.0001 , 
- 故本例回归模型是有意义的。

![[Pasted image 20230406120745.png]]

由  [[t检验]]结果可见, 
- 偏回归系数  $b_{2}$ 、 $b_{4}$  的  P  值都小于 0.01 , 
	- 可认为解释变量税收  $x_{2}$  和经济活动人口  $x_{4}$  显著;  
- $b_{1}$ 、 $b_{3}$  的  P  值大于 0.50 , 
	- 不能否定对  $\beta_{1}=0$ 、 $\beta_{3}=0$  的假设, 
	- 可认为国内生产总值  $x_{1}$  和进出口贸易总额  $x_{3}$  对财政收入  y ==没有显著的影响==。
- 我们可以看到, 
	- 国内生产总值、经济活动人口所对应的偏回归系数都为负, 
		- 这与经济现实是不相符的。
		- 出现这种结果的原因可能是这些解释变量之间存在==高度的共线性==。

## 4.3 多元线性相关分析
在相关分析中, 
- 研究较多的是两个变量之间的关系, 称为[[简单相关]]。
- 当涉及的变量为三个或三个以上时, 称为[[偏相关]]或复相关。
	- 实际上, 偏相关 (复相关) 是==对简单相 的一种推广==。

在有些情况下，我们只想了解两个变量之间有==无线性相关关系==，
- 并不需要建立它们之间的回归模型, 
- 也不需要区分自变量和因变量, 
- 这时, 就可用较为方便的[[相关分析]]方法。

### 4.3.1 矩阵相关分析
设  $x_{1}$, $x_{2}$, $\cdots, x_{n}$  来自正态总体  $N_{p}(\mu, \Sigma)$  容量为  n  的样本, 样本资料矩阵为:

$$X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \cdots & x_{1 p} \\
x_{21} & x_{22} & \cdots & x_{2 p} \\
\vdots & \vdots & \vdots & \vdots \\
x_{n 1} & x_{n 2} & \cdots & x_{n p}
\end{array}\right]$$

此时，任意两个变量间的相关系数构成的矩阵为:

$$R=\left[\begin{array}{cccc}
r_{11} & r_{12} & \cdots & r_{1 p} \\
r_{21} & r_{22} & \cdots & r_{2 p} \\
\vdots & \vdots & \vdots & \vdots \\
r_{p 1} & r_{p 2} & \cdots & r_{p p}
\end{array}\right]=\left[\begin{array}{cccc}
1 & r_{12} & \cdots & r_{1 p} \\
r_{21} & 1 & \cdots & r_{2 p} \\
\vdots & \vdots & \vdots & \vdots \\
r_{p 1} & r_{p 2} & \cdots & 1
\end{array}\right]=\left(r_{i j}\right)_{p \times p}$$

其中,  $r_{i j}$  为任意两个变量之间的简单相关系数, 即

$$r_{i j}=\frac{\sum_{i j}\left(x_{i}-\bar{x}\right)\left(y_{j}-\bar{y}\right)}{\sqrt{\sum_{i}\left(x_{i}-\bar{x}\right)^{2}} \sum_{j}\left(y_{j}-\bar{y}\right)^{2}}$$

#### 【例 4-5】(续例 4-4) 财政收人与其他变量间的相关分析。
计算财政收入和国民生产总值、税收、进出口贸易总额、经济活动人口两两之间的相关系数时, 表 4-4 给出了相关系数的假设检验统计量。

表  4-4  相关系数的假设检验统计量
![[Pasted image 20230406121918.png]]

##### 1.计算变量两两间的相关系数

```
cor(yX)  #多元数据相关系数矩阵 
```

$$\begin{array}{rrrrrr} & \mathrm{y} & \mathrm{x} 1 & \mathrm{x} 2 & \mathrm{x} 3 & \mathrm{x} 4 \\\mathrm{y} & 1.0000 & 0.9871 & 0.9995 & 0.9912 & 0.6957 \\\mathrm{x} 1 & 0.9871 & 1.0000 & 0.9907 & 0.9868 & 0.7818 \\\mathrm{x} 2 & 0.9995 & 0.9907 & 1.0000 & 0.9917 & 0.7154 \\\mathrm{x} 3 & 0.9912 & 0.9868 & 0.9917 & 1.0000 & 0.7074 \\\mathrm{x} 4 & 0.6957 & 0.7818 & 0.7154 & 0.7074 & 1.0000\end{array}$$

##### 2.给出变量两两间的矩阵散点图
```
pairs (X, ...)
X为数值矩阵或数据框
```

```
pairs (yX) #多元数据散点图
```

![[Pasted image 20230406122514.png]]

由于没有现成的进行相关系数矩阵的假设检验, 下面编写计算相关系数的  t  值和  P  值的函数 corr. test () 。

相关矩阵检验函数corr. test()的用法

```
library (mvstats)
corr. test (yX) #多元数据相关系数检验
```

$$\begin{array}{l}
\begin{array}{rrrrrr} 
& \mathrm{y} & \mathrm{x} 1 & \mathrm{x} 2 & \mathrm{x} 3 & \mathrm{x} 4 \\
\mathrm{y} & 0.000 & 0.000 & 0.000 & 0.000 & 0 \\
\mathrm{x} 1 & 33.267 & 0.000 & 0.000 & 0.000 & 0 \\
\mathrm{x} 2 & 165.614 & 39.214 & 0.000 & 0.000 & 0 \\
\mathrm{x} 3 & 40.336 & 32.772 & 41.560 & 0.000 & 0 \\
\mathrm{x} 4 & 5.215 & 6.752 & 5.514 & 5.389 & 0
\end{array}\\
\text { 左下角为 } \mathrm{t} \text { 值, 右上角为 } \mathrm{P} \text { 值 }
\end{array}$$

从结果可以看出，
- 财政收入和国民生产总值、税收、进出口贸易总额、经济活动人口之间的关系都非常密切  (r>0.8, P<0.001) , 
- 财政收入与税收之间的关系最为密切  (r=   0.9995, P<0.001)  。

### 4.3.2 复相关分析
以上都是在把其他变量的影响完全排除在外的情况下研究两个变量之间的相关关系。 
- 但是在实际分析中, 一个变量的变化往往要受到多种变量的综合影响, 这时就需要采用[[复相关分析]]方法。
- 所谓复相关, 就是研究多个变量同时与某个变量之间的相关关系, 度量复相关程度的指标是复相关系数。

#### 1.复相关系数的计算
设因变量为  y , 自变量为  $x_{1}$, $x_{2}$, $\cdots, x_{p}$ , 假定回归模型为:

$$\begin{array}{l}
y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+\cdots+b_{p} x_{p}+\varepsilon \\
\hat{y}=b_{0}+b_{1} x_{1}+b_{2} x_{2}+\cdots+b_{p} x_{p}
\end{array}$$

对  y  与  $x_{1}$, $x_{2}$, $\cdots, x_{p}$  作相关分析
- 就是对  y  与  $\hat{y}$  作相关分析, 
- 记 $r_{y} \cdot x_{1} x_{2} \cdots x_{p}$  为  y  与 $x_{1}, x_{2}, \cdots ,  x_{p}$  的复相关系数, 
- 而  $r_{y} . j$  可以看作  y  与  $\hat{y}$  的简单相关系数。
- 于是  y  与  $x_{1}, x_{2}, \cdots, x_{p}$  的复相关系数计算公式为:

$$R=\operatorname{corr}\left(y, x_{1}, x_{2}, \cdots, x_{p}\right)=\operatorname{corr}(y, \hat{y})=\frac{\operatorname{cov}(y, \hat{y})}{\sqrt{\operatorname{var}(y) \operatorname{var}(\hat{y})}}=\sqrt{\frac{\sum\left(\hat{y}_{i}-\bar{y}\right)^{2}}{\sum\left(y_{i}-\bar{y}\right)^{2}}}$$

#### 2.决定系数
在类似多元回归分析这类问题中, 
- 研究者常希望知道因变量与一组自变量间的相关程度, 即复相关。
- 如例  4-4  的资料, 研究者希望分析财政收人与国民生产总值和税收等 指标间的相关程度。为此可计算复相关系数  R  :

$$R=\sqrt{\frac{\sum\left(\hat{y}_{i}-\bar{y}\right)^{2}}{\sum\left(y_{i}-\bar{y}\right)^{2}}}=\sqrt{\frac{S S_{R}}{S S_{T}}}$$

[[复相关系数]]
- 反映了一个变量与另一组变量关系密切的程度。
- 复相关系数的==假设检验等价于多元回归的方差分析结果==，所以不必再作假设检验。

公式  R  根号里的分式
- 实际上就是回归[[离差平方和]]与[[总离差平方和]]的比值, 
- 反映了回归贡献的百分比值, 
- 所以常把  $R^{2}$  称为==决定系数或相关指数==。
- 本例中,  $R^{2}=0.9999^{2}=0.9998$  。  
- $R^{2}$  在评价<span style="background:rgba(240, 107, 5, 0.2)">多元回归方程、变量选择、曲线回归方程拟合的好坏程度</span>中常会用到。

```
( R2 = summary (fm)$r. sq) #显示多元线性回归模型决定系数
```

[1] 0.9998
 ```
 (R=sqrt(R}2)  #显示多元数据复相关系数
 ```
 
 [1] 0.9999 

## 4.4 回归变量的选择方法
[[多元回归分析]]在实际中有广泛的应用, 由 4.2 节分析可知, 其**主要用途**有: 
-  (1)==用于描述、解释现象==, 
	- 这时希望回归方程中所包含的<span style="background:rgba(92, 92, 92, 0.2)">自变量尽可能少一些</span>; 
- (2)用于==预测==, 
	- 这时希望预测的<span style="background:rgba(163, 67, 31, 0.2)">均方误差较小</span>; 
- (3)用于==控制==, 
	- 这时希望<span style="background:rgba(240, 200, 0, 0.2)">各回归系数</span>具有<span style="background:rgba(240, 200, 0, 0.2)">较小的方差和均方误差</span>。
- 在实际问题中, 可以提出许多对因变量有影响的自变量, 
	- <span style="background:rgba(136, 49, 204, 0.2)">变量</span>选择<span style="background:rgba(136, 49, 204, 0.2)">太少</span>或不恰当, 
		- <span style="background:rgba(136, 49, 204, 0.2)">会使建立的模型与实际有较大的偏离</span>; 
	- 而变量选得<span style="background:rgba(136, 49, 204, 0.2)">太多</span>则使用不便, 
		- 并且有时也会<span style="background:rgba(136, 49, 204, 0.2)">削弱估计和预测的稳定性</span>, 
	- 所以变量选择问题是一个十分重要的问题。
		- 也就是说, 在多元回归分析中, 并不是变量越多越好。
			- ==变量太多, 容易引起以下四个问题==: 
				- (1)增加了模型的<span style="background:rgba(240, 200, 0, 0.2)">复杂度</span>
				- (2)<span style="background:rgba(240, 200, 0, 0.2)">计算量增大</span>; 
				- (3)估计和预测的<span style="background:rgba(240, 200, 0, 0.2)">精度下降</span>; 
				- (4)模型<span style="background:rgba(240, 200, 0, 0.2)">应用费用增加</span>。

### 4.4.1 变量选择准则
为解决以上问题, 人们提出了许多变量选择的准则, 
- 如[[全部子集法]]、[[向后删除法]]、 [[向前引入法]]及[[逐步筛选法]]等。
#### 一、全局择优法
这需要根据一些准则 (criterion) 建立 “最优” 回归模型。

从理论上说, 自变量选择最好的方法
- 是所有可能回归法, 
- 即建立<span style="background:rgba(240, 107, 5, 0.2)">因变量和所有自变量全部子集组合的回归模型</span>，也称<span style="background:rgba(240, 107, 5, 0.2)">全部子集法</span>。

对于含有  p  个自变量的<span style="background:rgba(240, 107, 5, 0.2)">回归模型</span>来说, 
- 含有 0 个自变量 (仅有常数项) 的子集有  $C_{p}^{0}$  个; 
- 含有 1 个自变量的子集有  $C_{p}^{1}$  个; 
- 含有2个自变量的子集有  $C_{p}^{2}$  个,  $\cdots \cdots$ , 
- 含有  p  个 自变量的子集有  $C_{p}^{p}$  个, 
- 因此, ==共有  $C_{p}^{0}+C_{p}^{1}+C_{p}^{2}+\cdots+C_{p}^{p}=2^{p}$  个模型==。

求出所有可能的回归模型（共有  $2^{p}-1$  个）对应的<span style="background:rgba(74, 82, 199, 0.2)">准则值</span>，按一定准则<span style="background:rgba(74, 82, 199, 0.2)">选择最优模型</span>。

对于每个模型, 在实用上, 从数据与模型拟合优劣的直观考虑出发, 
- 基于==残差（误差) 平方和RSS== (residual sum of squares, 即方差分析表中  $S S_{E}$  ) 的变量选择准则使用得最多。
	- ==误差平方和越小, 回归方程的拟合越理想==。
- 而且, 复相关系数的平方（决定系数)  $R^{2}=1-R S S / S S_{T}$ , 
	- 对一个确定的问题, 即  $S S_{T}$  确定, 
	- 基于残差（误差）平方和  RSS  的变量选择准则与基于决定系数  $R^{2}$  的变量选择准则意义是等价的, 
	- ==决定系数  $R^{2}$  越大, 回归方程的拟合越理想==。

下面以残差平方和 RSS  与复相关系数的平方  $R^{2}$  为准则介绍变量选择的过程。

##### 【例 4-6】(续例 4-4) 
在 “财政收入” 数据中, 有四个自变量:  $x_{1}$ 、 $x_{2}$ 、 $x_{3}$  和  $x_{4}$  。
- 所有可能的模型可分为以下五组子集:
- 子集  $A: y=b_{0} \Rightarrow C_{4}^{0}=1$  种可能模型。
- 子集  $B: y=b_{0}+b_{i} x_{i}, i=1,2,3,4 \Rightarrow C_{4}^{1}=4$  种可能模型。
- 子集  $C: y=b_{0}+b_{i} x_{i}+b_{j} x_{j}, i \neq j, i, j=1,2,3,4 \Rightarrow C_{4}^{2}=6$  种可能模型。
- 子集  $D: y=b_{0}+b_{i} x_{i}+b_{j} x_{j}+b_{k} x_{k}, i \neq j \neq k, i, j, k=1,2,3,4 \Rightarrow C_{4}^{3}=4$  种可 能模型。
- 子集  $E: y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+b_{3} x_{3}+b_{4} x_{4} \Rightarrow C_{4}^{4}=1$  种可能模型。
- 总共有  $C_{4}^{0}+C_{4}^{1}+C_{4}^{2}+C_{4}^{3}+C_{4}^{4}=2^{4}=16$  种模型。

###### 1.RSS  和  $R^{2}$  准则变量的选取
对每组子集, 挑出  RSS  最小、  $R^{2}$  最大的变量, 见表 4-5, 得出下列模型:
$$表  4-5 例 4-4 数据的  R S S  与  R^{2}  准则回归子集$$
$$\begin{array}{llcc}
\text { 子集 } & Models  & R S S & R^{2} \\
\hline \text { 子集 } B & y=b_{0}+b_{2} x_{2} & 752.88 & 0.99894 \\
\text { 子集 } C & y=b_{0}+b_{2} x_{2}+b_{4} x_{4} & 203.88 & 0.99971 \\
\text { 子集 } D & y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+b_{4} x_{4} & 202.35 & 0.99972 \\
\text { 子集 } E & y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+b_{3} x_{3}+b_{4} x_{4} & 202.34 & 0.99972
\end{array}$$
$\text { 注意: 在本书中残差平方和用 } S S_{E} \text { 表示, 等同于 } \mathrm{R} \text { 中的 } R S S_{\text {。 }}$

```
library(leaps)  #加载 leaps 包
varsel  =  regsubsets  (y~x1+x2+x3+x4 , data =yX)  #多元数据线性回归变量选择模型
result  =  summary(varsel) #变量选择方法结果
data. frame(result\$outmat, RSS=result$rss, R2=result$rsq)，#RSS 和决定系数准则结果展示
```

$$\begin{array}{cccccccc} & & \mathrm{x} 1 & \mathrm{x} 2 & \mathrm{x} 3 & \mathrm{x} 4 & \mathrm{RSS} & \mathrm{R} 2 \\1 & (1) & * & & & & 752.88 & 0.99894 \\2 & (1) & * & * & & & 203.88 & 0.99971 \\3 & (1) & * & * & * & & 202.35 & 0.99972 \\4 & (1) & * & * & * & * & 202.34 & 0.99972\end{array}$$

###### 2.RSS  和  $R^{2}$  准则的优点
具有较大的  $R^{2}$  值对于较少自变量的模型应该是好的选择, 
- 因为较大的  $R^{2}$  意味着有较好的拟合效果, 
- 而较少的变量个数有利于信息的收集和控制。
###### 3.RSS  和  $R^{2}$  准则的缺点
对于有  p  个自变量的回归模型来说, 
- 当==自变量子集在扩大==时, 
	- 残差平方和随之减少 ( 可以证明  $R S S_{p} \leqslant R S S_{p-1}$ , 进而  $R_{p}^{2} \geqslant R_{p-1}^{2}$  ), 
	- 因此，如果按 “  RSS  愈小愈好” 和按 “  $R_{p}^{2}$  愈大愈好” 的原则来选择自变量子集, 
		- 则毫无疑问, 应该选全部自变量。
	- 所以说, 在实际 中, ==“RSS 愈小愈好”和 “  $R_{p}^{2}$  愈大愈好”不能作为选择自变量的准则==。

另外, 在上述  $R^{2}$  准则的选择中, 
- 本案例的两个模型  $y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+b_{4} x_{4}$  和  $y=b_{0}+b_{2} x_{2}+b_{3} x_{3}+b_{4} x_{4}$  就很难选取。
- 这主要是因为  <span style="background:rgba(240, 107, 5, 0.2)">$x_{1}$  和  $x_{3}$  高度相关</span>, 
	- 其相关系数为 0.9868 , 
		- 因而它们的  <span style="background:rgba(240, 107, 5, 0.2)">$R^{2}$  一样</span>就不奇怪了。

#### 二、变量选择的常用准则
由于在实际的变量选择问题中, 
- 我们的==主要目的就是设法防止选取过多的自变量==, 
- 而基于直观考虑的[[残差平方和]]准则、复相关系数平方准则最终都将选取所有自变量, 
- 所 以常用的做法是在残差平方和 RSS  上添加对变量的惩罚因子  n-p  。
##### 1.平均残差平方和最小准则

$$RMS_{p}=\frac{R S S_{p}}{n-p}$$

这里, 
- p  为所选模型的变量个数 (每个模型皆包括常数项), 
- 因  $(n-p)^{-1}$  随着自变量个数  p  的增加而增加, 
	- 它体现了变量个数增加对 RSS  增加的惩罚, 
- 于是有平均残差平方和最小准则：按 “  ==RMS  愈小愈好==” 选取自变量。
##### 2.误差均方根 MSE 最小准则

$$MSE_{p}=\sqrt{R M S_{p}}$$

- $MSE_{p}$  
	- 实际上就是模型的剩余标准差  $s_{y \cdot x}$, 
	- $MSE_{p}$  越小, 说明模型拟合得越好。
	- 当然, 模型中最小的  $MSE_{p}$  所对应的模型就是最好的模型, 
	- 所得结论同  $RMS_{p}$  准则等价。

##### 3.校正复相关系数平方 $(Adjusted  R^{2}  )$ 准则

$$\begin{array}{l}
\operatorname{adj} R^{2}=1-\frac{n-1}{n-p}\left(1-R^{2}\right) \\
\operatorname{adj} R^{2}=1-\frac{n-1}{n-p}\left(1-R^{2}\right)=1-\frac{R S S_{p} /(n-p)}{S S_{T} /(n-1)}=1-\frac{n-1}{S S_{T}} R M S_{p}
\end{array}$$

由于对一个具体问题  $S S_{T}$  不变, 所以这个准则也就等价于  $RMS_{p}$  准则。  $\operatorname{adj} R^{2}$  越大,说明模型拟合得越好。

##### 4.$C_{p}$  准则
近年来, 一个得到广泛重视的变量选择准则是基于 1964 年 C. Mallows 提出的  $C_{p}$  统计 量,  $C_{p}$  统计量是从预测的角度出发, 基于残差平方和的一个准则。

$C_{p}=\frac{R S S_{p}}{s^{2}}-(n-2 p)=\frac{R S S_{p}}{R M S}-(n-2 p)=\frac{(n-p) R M S_{p}}{R M S}-(n-2 p)$

这里,  
- C  即 criterion,  
- p  为所选模型中变量的个数,  
- $C_{p}$  接近  p  模型为最优。
- 其中,  $s^{2}$  为全模型的==均方误差  RMS==  。
 
$C_{p}$  法则为：选择对应点  $\left(p, C_{p}\right)$  最接近第一象限角平分线, 且  $C_{p}$  最小的模型。

##### A I C  准则和  B I C  准则
 ==AIC  (Akaike information criterion)== 和  ==BIC  (Bayesian information criterion)== 是多元回归中选择模型的两条重要准则。
 - 在多元回归分析中, 为了防止过度拟合等问题（既要使模型的解释性强, 又要有一点张力), 
 - Akaike (1978) 和 Schwarz (1978) 分别提出了 AIC 和  BIC  作为回归模型选择的标准。
 - 在回归模型中, 这==两个值都是越小越好==。
- 它们不仅可用于[[回归分析]]的变量选择中，还可用于[[时间序列分析]]的[[自回归模型]]的定阶上。

回归分析中选择变量的  A I C  和  B I C  准则分别为:
$$\begin{array}{l}
A I C=n \ln \left(\frac{R S S_{p}}{n}\right)+2 p \\
B I C=n \ln \left(\frac{R S S_{p}}{n}\right)+p \ln (n)
\end{array}$$

 AIC 和  BIC  选择变量按 “ ==AIC  或  BIC  愈小愈好==” 的准则选取。
 
对每组子集, 挑出  $C_{p}$  和  B I C  最小的变量, 见表 4-6, 得出下列模型:

$$表 4-6   例 4-4 数据的  C_{p}  与  B I C  准则回归子集$$
$$\begin{array}{clccc}
\hline \text { 子集 } & Models & \operatorname{adj} R^{2} & C_{p} & B I C \\
\hline \text { 子集 } B & y=b_{0}+b_{2} x_{2} & 0.9989 & 69.745 & -205.6 \\
\text { 子集 } C & y=b_{0}+b_{2} x_{2}+b_{4} x_{4} & 0.9997 & 1.199 & -242.6 \\
\text { 子集 } D & y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+b_{4} x_{4} & 0.9997 & 3.001 & -239.4 \\
\text { 子集 } E & y=b_{0}+b_{1} x_{1}+b_{2} x_{2}+b_{3} x_{3}+b_{4} x_{4} & 0.9997 & 5.000 & -236.0 \\
\hline
\end{array}$$

对例 4-4, 上面给出了所选模型的  $C_{p}$  值,  
- $C_{p}$  的最小值对应的变量子集为  $\left(x_{0}, x_{2}, x_{4}\right)$ ,  
- $C_{p}=1.199,\left(x_{0}, x_{2}, x_{4}\right)$  对应的  (1+2,1.199)=(3,1.199)  最接近第一象限角平分线。
- 另外一些较小的  $C_{p}$  统计量分别对应于  $\left(x_{0}, x_{1}, x_{2}, x_{4}\right)$ , 
	- 对这个变量子集, 其对应的  (1+3,3.001)=(4,3.001)  也接近第一象限角平分线, 
	- 如果没有别的附加考虑, 在  $C_{p}$  准 则下,  
		- $\left(x_{0}, x_{2}, x_{4}\right)$  是 “最优” 子集。

而按  B I C  准则选择的 “最优” 子集也是  $\left(x_{0}, x_{2}, x_{4}\right)$  。

```
data. frame (result$outmat, adjR2= result$adjr2 ,  Cp=  result$cp, BIC=  result$bic  )  
#调整决定系数, Cp 和 BIC 准则结果展示
```

$$\begin{array}{ccccccccc} 
& & \text { x1 } & \text { x2 } & \text { x3 } & \text { x4 } & \text { adjR2 } & \text { Cp } & \text { BIC } \\
1 & (1) & & * & & & 0.9989 & 69.745 & -205.6 \\
2 & (1) & & * & & * & 0.9997 & 1.199 & -242.6 \\
3 & (1) & * & * & & * & 0.9997 & 3.001 & -239.4 \\
4 & (1) & * & * & * & * & 0.9997 & 5.000 & -236.0
\end{array}$$

### 4.2.4 逐步回归分析
#### 一、逐步回归分析的概念
在作实际多元线性回归时常有这样的情况: 
- 变量  $x_{1}, x_{2}, \cdots, x_{p}$  相互之间常常是线性相关的, 
	- 即在  $x_{1}, x_{2}, \cdots, x_{p}$  ==中任何两个变量是完全线性相关的, 其相关系数为 1 ==, 
	- 则 
		- 矩阵  $X^{\prime} X$  的秩小于  p,
		- $\left(X^{\prime} X\right)^{-1}$  就无解。
- 当变量  $x_{1}, x_{2}, \cdots, x_{p}$  中任有两个变量存在较大的相关性时, 
	- 矩阵  $X^{\prime} X$  处于==病态==, 
	- 会给模型带来很大误差。
- 因此作回归时, 应选变量  $x_{1} ,  x_{2}, \cdots, x_{p}$  中的一部分, 剔除一些变量。
- 逐步回归法就是寻找较优子空间的一种变量选择方法。

在前面的章节中, 我们给出了一般多元线性回归方程的求法, 
- 但是细心的读者也许会注意到, 
	- 在那里不管自变量  $x_{i}$  对因变量  y  的影响是否显著, 均可进入回归方程, 
	- 这样就使误差的自由度变小, 
		- 而误差的<span style="background:rgba(240, 200, 0, 0.2)">自由度变小, 就使得误差的均方增大</span>, 
			- 即估计的<span style="background:rgba(240, 200, 0, 0.2)">精度变低</span>。
	- 另外, 在许多实际问题中, 往往自变量  $x_{1}, x_{2}, \cdots, x_{p}$  之间并不是完全独立的, 而是有一定的相关性存在的。
		- 如果回归模型中的某两个自变量  $x_{i}$  和  $x_{j}$  的相关系数比较大, 
			- 就可使得正规方程组的系数矩阵出现==病态==, 
			- 也就是所谓==多重共线性==的问题, 
			- 将导致<span style="background:rgba(240, 200, 0, 0.2)">回归系数的估计值的精度不高</span>。

在例 4-4 中, 虽然回归方程的检验是高度显著的, 
- 但是回归系数的检验结果只有  $x_{2}$  和  $x_{4}$  是显著的, 
- 而  $x_{1}$  和  $x_{3}$  却不显著, 
- 这样的回归方程不能称为最佳回归方程。
- 在实际计算中, 我们总是希望
	- 不但求得的==回归方程是显著的==, 
	- 而且在回归方程中的==自变量也都是尽可能显著==的, 
	- 也就是要<span style="background:rgba(240, 200, 0, 0.2)">选择最佳的回归模型</span>。
		- 选择最佳回归模型的方法很多, 而逐步回归分析方法就是其中的一种。
#### 二、逐步变量选择的方法
在后面的讨论中, 
- 如果对回归方程增加自变量  $x_{i}$ , 则称为 “==引入==” 变量  $x_{i}$ ; 
- 如果要 将已在回归方程中的自变量  $x_{i}$  从回归方程中删掉, 则称为 “==剔除==” 变量  $x_{i}$  。
- 无论引入变量或剔除变量, 都要利用  [[F检验]], 
	- 将显著的变量引入回归方程, 
	- 而将不显著的变量从回归方程剔除。
	- 记
		- 引人变量的  [[F检验]]的临界值为  $F_{\text {进 }}$  
		- 剔除变量的  [[F检验]]的临界值为  $F_{\text {出}}$ , 
	- 方程的变量为  m  个  ($m \leqslant p$) , 
		- ==则对给定的显著性水平  $\alpha$ , 确定  F  值, 记为  $F^{*}$, 则可取==
			-  $F_{\text {进}}=F_{\text {出}}=F^{*}$  。
			- 一般来说, 也可以直接取  $F_{\text {进}}=F_{\text {出 }}=$ ==3.84  或 2.71== 。
			- 当然, 为了回归方程中还能多进入一些自变量, 甚至也可以取为 ==2.0 或 2.5== 。

##### 1.向前引入法 (forward)
- 首先对全部  p  个自变量, 
	- 分别对因变量  y  建立一元回归方程, 
	- 并分别计算这  p  个一元 回归方程的  p  个回归系数的  F  检验值，
		- 记为  $\left\{F_{1}^{1}, F_{2}^{1}, \cdots, F_{p}^{1}\right\}$ , 
		- 选其最大的记为 $F_{j}^{1}=\max \left\{F_{1}^{1}, F_{2}^{1}, \cdots, F_{p}^{1}\right\}$
- 接着考虑将  $\left(x_{1}, x_{2}\right),\left(x_{1}, x_{3}\right), \cdots,\left(x_{1}, x_{p}\right)$  
	- 分别与因变量  y  建立二元回归方程, 
	- 对这  p-1  个回归方程中  $x_{2}, x_{3}, \cdots, x_{p}$  的回归系数进行  [[F检验]], 计算得到的  F  值, 
		- 记为  $F_{2}^{2}, F_{3}^{2}, \cdots$ , 不失一般性，
		- 然后设  $x_{k}$  就是  $x_{2}$  。
- 对已经引入回归方程的变量  $x_{1}$  和  $x_{2}$ , 用前面的方法做下去, 
	- 直到所有未被引入方程的变量F值均小于$F_{\text {进}}$为止。
		- 这时的回归方程就是最终选定的回归方程。
		- 换种说法, 向前引入法即从一个变量开始, 每次引入一个对y 影响显著的变量, 直到无法引入为止。
	- 这种方法的要点是
		- 从一个变量开始, 
		- 将回归变量逐个引人回归方程, 
		- 它要先计算  y  同各个变量的相关系数, 
			- 对于相关系数绝对值最大的变量, 
				- 对其偏回归平方和 (复相关系数) 作显著性检验, 
					- 如果显著就引人方程。
	- 这种方法只是对变量的引入把关, 变量引入之后，不论其以后是否会变成不显著，概不剔除。

显然, 这种增加法有一定的缺点, 主要是它==不能反映后来的变化情况==。
- 因为对于某个自变量, 它可能开始时是显著的, 
	- 即将其引入回归方程以后, 随着其他自变量的引人, 它可能又变为不显著的了, 而并没有将其及时从回归方程中剔除。
	- 也就是说，==增加变量法只考虑引入而不考虑剔除==。
##### 2.向后剔除法 (backward)
与向前引入法相反, 向后剔除法
- 是首先建立==全部自变量==  $x_{1}, x_{2}, \cdots, x_{p}$  对因变量  y  的==回归方程==, 
- 然后对  p  个回归系数进行  [[F检验]], 
	- 记求得的  ==F值==为  $\left\{F_{1}^{1}, F_{2}^{1}, \cdots, F_{p}^{1}\right\}$ , 
	- 选=其==最小值===, 记为  $F_{j}^{1}=\min \left\{F_{1}^{1}, F_{2}^{1}, \cdots, F_{p}^{1}\right\}$ , 
- 若有  $F_{j}^{1} \leqslant F_{\text {出}}$ , 
	- 则可以考虑将自变量  $x_{j}$  从回归方程中剔除，不妨设  $x_{j}$  就取为  $x_{1}$  。
- 再对  $x_{2}, x_{3}, \cdots, x_{p}$  对因变量  y  建立的回归方程中的回归系数进行  F  检验, 
	- 记求得 的  F  值为  $\left\{F_{2}^{2}, F_{3}^{2}, \cdots, F_{p}^{2}\right\}$  。
	- 再取其中最小值, 记为  $F_{k}^{2}=\min \left\{F_{2}^{2}, F_{3}^{2}, \cdots, F_{p}^{2}\right\}$ , 
	- 若有  $F_{k}^{2} \leqslant\leqslant F_{\text {出}}$  ，
		- 则接着将$x_{k}$也从回归方程剔除。
		- 不妨设$x_{k}$就是$x_{2}$。
- 重复前面的做法，直至在回归方程中的变量的 F检验值均大于 $F_{\text {出 }}$，
	- 即没有变量可剔除为止, 这时的回归方程就是最终的回归方程。

总之, 向后剔除法
- 即从包含全部  p  个变量的回归方程中, 根据判断, 每次剔除一个 对  y  影响不显著的变量, 直到无法剔除为止。
- 即从包含全部变量的回归方程中逐步剔除不显著变量。
	- 先建立==全部变量的回归方程==, 
	- 然后对每一变量作==显著性检验==, 剔除不显著变量中==偏回归平方和最小==的一个变量, 
	- 重新建立方程, 重复上面的过程, 直至方程中每 个变量都显著为止。
- 许多文献观点都认为这种方法
	- 在<span style="background:rgba(240, 200, 0, 0.2)">变量不多且不显著变量也不多</span>时可以采用。
	- 而<span style="background:rgba(240, 107, 5, 0.2)">当变量较多, 特别是不显著变量很多</span>时, <span style="background:rgba(240, 107, 5, 0.2)">计算工作量</span>是相当大的, 
		- 因为每剔除一个因子后就得重新计算回归系数。

这种剔除法有一个明显的缺点, 就是一开始把全部自变量都引入回归方程, 这样使得==计算量比较大==。若对一些不重要变量, 一开始就不引人, 这样便可以减少一些计算量。
##### 3.逐步筛选法 (stepwise)
- 前面的变量引入法, 
	- 只考虑增加变量, 不考虑剔除, 
		- 也就是对任何一个变量, 一旦将其引入回归方程, 不管其以后在回归方程中的作用发生什么变化（即使变得不显著 了), 也不考虑将其剔除。
- 反之, 变量剔除法, 
	- 只考虑剔除, 而不考虑增加。
	- 如果自变量  $x_{1}, x_{2}, \cdots, x_{p}$  是完全独立的, 那么利用这两种方法所求得的两个回归模型之间是完全没有显著差异的。
	- 然而, 在许多实际问题的数据中, 
		- 自变量  $x_{1}, x_{2}, \cdots, x_{p}$  之间往往并不是独立的, 
			- 而是有一定的相关性存在, 这就使得随着回归方程中变量的增加和减少, 某些自变量对回归方程的贡献也会发生变化。
- 因此一种很自然的想法是将前两种方法结合起来, 
	- 也就是
		- 对每一个自变量, 随着其对回归方程贡献的变化,== 随时将其引入回归方程或剔除出去==, 
		- 最终的回归模型是, 
			- ==在回归方程中的自变量均为显著的变量==, 不在回归方程中的自变量均为不显著的变量。
- 也就是说, [[逐步筛选法]]是综合上述两种方法的特点而建立的一种新方法, 其**基本思想**是：
	- 在所考虑的全部变量中, 按其对预报变量  y  作用的显著程度大小, 挑选一个最重要变量, 建立只包含<span style="background:rgba(240, 107, 5, 0.2)">这个变量的回归方程</span>; 
	- 接着对其他变量计算偏回归平方和, 引入一个显著性的变量, 建立具有两个变量的回归方程; 
	- 从此之后, 逐步回归的每一步（引人一个变量或从回归方程中剔除一个变量都算作一步）前后都要作显著性检验, 即反复进行两个步骤。
		- 第一, 对已在回归方程中的变量作显著性检验, 使得显著者保留, 最不显著者剔除; 
		- 第二, 对不在回归方程中的其余变量, 挑选最重要的那一个进入回归方程, 直至最后回归方程中再也不能剔除任一变量, 同时也不能再引入变量为止, 保证最后所得的回归方程中所有变量都为显著变量。
	- 这种方法和所谓选择全部回归子集的方法在一般情况下是很好的, 特别是==当整个模型满足线性回归的基本假定时效果较好==。

**逐步回归的计算步骤**是从一个变量开始做：
- (1)每次选人一个对  y  影响显著的变量, 直到无法选入时转到 (2); 
- (2)每次剔除一个对  y  影响不显著的变量, 直到无法剔除时转到(1)。
- 当无法选人也无法剔除时停止筛选, 以使最后回归方程只保留重要的变量。

```
 fm=lm(y~x1+x2+x3+x4 , data = yX  )  #多元数据线性回归模型
fm. step=step(fm, direction= "forward"  )  #向前引人法变量选择结果
```

$Start :  \mathrm{AIC}=68.15$ 

$y \sim x_{1}+x_{2}+x_{3}+x_{4}$

```
fm. step= step(fm, direction= "backward") #向后剔除法变量选择结果
```

$$\begin{array}{l}
\text { Start }: \mathrm{AIC}=68.15\\
\begin{array}{rrrrr}
\mathrm{y} \sim \mathrm{x} 1+\mathrm{x} 2+\mathrm{x} 3+\mathrm{x} 4 & & \\
& \text { Df } & \text { Sum of Sq } & \text { RSS } & \text { AIC } \\
-\mathrm{x} 3 & 1 & 0.009 & 202 & 66 \\
-\mathrm{x} 1 & 1 & 1 & 204 & 66 \\
\langle\text { none }\rangle & & & 202 & 68 \\
-x 4 & 1 & 174 & 376 & 85 \\
-x 2 & 1 & 6433 & 6635 & 174
\end{array}
\end{array}$$
$$\begin{array}{l}
\begin{array}{rrrrr}
\text { Step: } & \text { AIC }=66.16 \\
\mathrm{y} \sim \mathrm{x} 1+\mathrm{x} 2+\mathrm{x} 4 & & & \\
& \text { Df } & \text { Sum of Sq } & \text { RSS } & \text { AIC } \\
-\mathrm{x} 1 & 1 & 2 & 204 & 64 \\
<\text { none }> & & & 202 & 66 \\
-\mathrm{x} 4 & 1 & 197 & 400 & 85 \\
-\mathrm{x} 2 & 1 & 7382 & 7585 & 176
\end{array}\\
\text { Step: } \quad \mathrm{AIC}=64.39\\
\begin{array}{rrrrr}
\mathrm{y} \sim \mathrm{x} 2+\mathrm{x} 4 & & & \\
& \text { Df } & \text { Sum of Sq } & \text { RSS } & \text { AIC } \\
\text { <none > } & & & 204 & 64 \\
-\mathrm{x} 4 & 1 & 549 & 753 & 103 \\
-\mathrm{x} 2 & 1 & 367655 & 367859 & 295
\end{array}
\end{array}$$

```
fm. step= step (fm, direction= "both" }) #逐步筛选法变量选择结果
```

![[Pasted image 20230406225720.png]]


