---
书名: 多元统计分析及R语言建模
---

```ad-info
title: <u></u>**描述**
collapse: open
color: 233, 244, 240

**◀️ 父节点| ▶️ 子节点** 

🪁 status: #🔖 
🎏 class: #📸  

description ::  【目的要求】要求学生理解聚类分析的目的和意义及其统计思想, 了解变量类型的几种尺度定义; 热悉 Q 型和 R 型聚类分析常用的距离和相似系数的定义, 特别是 Minkowski 距离; 了解教材中介绍的六种系统聚类方法以及它们的统一公式; 熱悉软件中最长（短）距离法、重心法和离差平方和（Ward）法的具体使用步骤。【教学内容】聚类分析的目的和意义; 聚类分析中所使用的几种尺度的定义; 六种系统聚类方法的定义其基本性质; 计算程序中有关聚类分析的算法基础；在理解系统聚类方法基本性质的基础上, 初步掌握在实际问题中选用聚类方法与对应的测量距离的原则。

来源::

📎

```



## 7.1 聚类分析的概念和类型
### 1.聚类分析法的概念
[[聚类分析]]法 (cluster analysis) 是研究 “物以类聚” 的一种现代统计分析方法, 在社会生活的众多领域中, 都需要采用聚类分析作分类研究。
#### 聚类分析原因
- 过去人们主要靠经验和专业知识作定性分类处理, 很少利用数学方法, 
	- 致使许多分类都带有主观性和任意性, 不能很好地揭示客观事物内在的本质差别和联系, 
		- 特别是==对于多因素、多指标的分类问题, 定性分类更难以实现准确分类==。
	- 为了克服定性分类的不足, 多元统计分析逐渐被引入数值分类学，形成了聚类分析这个分支。
#### 聚类分析应用领域
聚类分析方法近十年来发展很快, 
- 并且在==经济、管理、地质勘探、天与㑔报、生物分类、考古学、医学、心理学以及制定国家标准和区域标准==等许多方面的应用都卓有成效, 因而成为目前国外较为流行的多变量统计分析方法之一
#### 聚类分析目的
**聚类分析的目的**
- 是把分类对象按一定规则分成若干类, 
	- 这些类不是事先设定的, 而是根据数据的特征确定的。
- 在同一类中这些对象在某种意义上趋向于彼此相似, 而在不同类中的对象趋向于不相似。

$$\text { 聚类分析方法 }\left\{\begin{array}{l}
\text { 系统聚类法 } \\
\text { 快速聚类法 }
\end{array}\right.$$

### 2.聚类分析法的类型
在实际问题中, 经常要对一些东西进行分类。
- 例如, 
	- 在古生物研究中, 
		- 通过挖掘出来的一些骨蹩的形状和大小对它们进行科学的分类; 
	- 在地质勘探中, 
		- 通过矿石标本的物探、化探指标对标本进行分类; 
	- 在经济区域的划分中, 
		- 根据各主要经济指标将全国各省划分成几个区域。
- 这里, 
	- 骨骼的形状和大小, 标本的物探、化探指标以及经济指标是我们==用来分类的依据==, 称为==指标 (或变量)==, 
		- 用  $X_{1}, X_{2}, X_{3}, \cdots, X_{p}$  表示, 
			-  p  是变量的个数; 
	- 需要进行分类的骨骼、矿石和地区称为<span style="background:rgba(136, 49, 204, 0.2)">样品</span>, 
		- 用  $1,2,3, \cdots, n$  表示,  
		- n  是样品的 个数。
- 聚类分析的数据结构见表  7-1  。

$$\begin{array}{l}
\text { 表 7-1 聚类分析数据结构表 }\\
\begin{array}{c|cccc}
\hline {\text { 样品 }} & {\text { 变量 }} \\
 & X_{1} & X_{2} & \ldots & X_{p} \\
\hline 1 & x_{11} & x_{12} & \ldots & x_{1 p} \\
2 & x_{21} & x_{22} & \ldots & x_{2 p} \\
3 & x_{31} & x_{32} & \ldots & x_{3 p} \\
\cdots & \ldots & \ldots & \ldots & \ldots \\
n & x_{n 1} & x_{n 2} & \ldots & x_{n p} \\
\hline
\end{array}
\end{array}$$

在聚类分析中, 基本的思想是认为所研究的样品或指标 (变量) 之间存在着程度不 同的相似性 (亲疏关系)。于是根据一批样品的多个观测指标, 具体找出一些能够度量 样品（或指标) 之间相似程度的统计量, 以这些统计量为划分类型的依据, 把一些相似 程度较大的样品 (或指标) 聚合为一类, 把另外一些彼此之间相似程度较大的样品（或 指标）又聚合为另一类, 关系密切的聚合到一个小的分类单位, 关系疏远的聚合到一个 大的分类单位, 直到把所有样品 (或指标) 都聚合完毕, 把不同的类型一一划分出来, 形成一个由小到大的分类系统。最后把整个分类系统画成一张聚类图, 用它把所有样品 （或指标）间的亲疏关系表示出来。
通常根据分类对象的不同可将聚类分析分为两类: 一类是对样品进行分类处理, 叫  Q  型; 另一类是对变量进行分类处理, 叫  R  型。Q 型聚类又叫样品分类, 就是对观测对 象进行聚类，是根据被观测对象的各种特征进行分类。
聚类分析的类型  \left\{\begin{array}{l}Q \text { 型聚类: 对样品的聚类 } \\ R \text { 型聚类: 对变量的聚类 }\end{array}\right. 
在经济管理中多用 Q 型聚类方法。反映同一事物特点的变量有很多, 我们往往根据 所研究的问题选择部分变量对事物的某一方面进行研究。由于人类对客观事物的认识是 有限的, 往往难以找出彼此独立的、有代表性的变量, 而影响对问题的进一步认识和研 究。因此通常先进行变量聚类, 这样既能找出彼此独立且有代表性的自变量, 而又不丢 失大部分信息。
## 7.2 聚类统计量
聚类分析的基本原则是将有较大相似性的对象归为同一类, 而将差异较大的个体归 人不同的类。为了将样品聚类, 就需要研究样品之间的关系。一种方法是将每一个样品 看作  p  维空间的一个点, 并在空间定义距离, 距离较近的点归为一类, 距离较远的点则 归于不同的类。对变量通常计算它们的相似系数, 性质越接近的变量的相似系数越接近 1 (或 -1 ), 彼此无关的变量的相似系数越接近 0 , 以此将比较相似的变量归为一类, 不 怎么相似的变量归于不同的类。
可进行聚类的统计量有距离和相似系数:

$$聚类统计量  \left\{\begin{array}{l}\text { 距离 }\left\{\begin{array}{l}\text { 欧氏距离 } \\ \text { 马距离 } \\ \text { 兰氏距离 }\end{array}\right. \\ \text { 相似系数 }\{\text { 夹角余弦 } \\ \text { 相关系数 }\end{array}\right.$$

对样品进行聚类时, 我们将样品间的 “靠近” 程度用某种距离来刻画; 对指标的聚 类, 往往用某种相似系数来刻画。
当选用  n  个样品、  p  个指标时（数据格式见表  7-1  ）, 就可以得到一个  n \times p  的数据 矩阵  $X=\left(x_{i j}\right) n \times p$ , 该矩阵的元素  x_{i j}  表示第  i  个样品的第  j  个变量值。
对样品或变量进行分类时, 我们常用距离和相似系数对样品或变量之间的相似性进 行度量。距离常用来度量样品之间的相似性, 而相似系数常用来度量变量间的相关性。
常见的数据类型有:
（1）间隔尺度：指变量用连续的量来表示。
（2）有序尺度: 指变量度量时没有明确的数量表示, 而是划分一些有次序关系的等级。
(3) 名义尺度: 指变量度量时既没有数量表示, 也没有次序关系。 这里用得最多的还是对间隔尺度数据的聚类。
### 1.距离
距离多用于样品的分类, 令  d_{i j}  表示样品  x_{i}  和  x_{j}  的距离, 一般要求  d_{i j}  满足以下四个 条件:
(1)  d_{i j}=0 
(2)  d_{i j} \geqslant 0 \quad \Leftrightarrow  对一切  x_{i}, x_{j} 
(3)  d_{i j}=d_{j i} \quad \Leftrightarrow  对一切  x_{i}, x_{j} 
(4)  d_{i j} \leqslant d_{i k}+d_{j k} \quad \Leftrightarrow  对一切  x_{i}, x_{j}, x_{k} 
在聚类分析中, 并不严格要求定义的距离都满足这四条, 一般来说大部分是能满足 前三条的，有一些不能满足 (4), 但是在广义的角度上也称其为距离。
设  x_{i j}(i=1,2, \cdots, n ; j=1,2, \cdots, p)  为第  i  个样品的第  j  个指标的观测数据。即若每个 样品有  p  个变量, 则每个样品都可以看成  p  维空间中的一个点,  n  个样品就是  p  维空间中 的  n  个点, 定义  d_{i j}  为样品  x_{i}  与  x_{j}  的距离。于是得到一个  n \times n  的距离矩阵  D=\left(d_{i j}\right)_{n \times n}  。

$$D=\left(d_{i j}\right)_{n \times n}=\left[\begin{array}{cccc}
d_{11} & d_{12} & \cdots & d_{1 n} \\
d_{21} & d_{22} & \cdots & d_{2 n} \\
\vdots & \vdots & \vdots & \vdots \\
d_{n 1} & d_{n 2} & \cdots & d_{m n}
\end{array}\right]$$

样品聚类都是基于此距离矩阵进行的。为了叙述方便, 我们举一个简单的例子。

【例 7-1】以下列举五个观察值、两个变量数据的平面散点图。
![[Pasted image 20230411220701.png]]

由于只有两个变量, 所以从散点图上就可以直观地将这五个样品分为几类, 但当变 量较多时，这种方法显然是不行的。
为了计算平面上各点之间的距离  $d_{i j}$ , 在聚类分析中对连续变量常用的距离有:

#### （1）明氏距离（Minkowski）:

$$d_{i j}(q)=\left[\sum_{k=1}^{p}\left(x_{i k}-x_{j k}\right)^{q}\right]^{\frac{1}{q}}$$

- 当  q=1  时,  $d_{i j}(1)=\sum_{k=1}^{p}\left|x_{i k}-x_{j k}\right|$ , 称为[[绝对值距离]] (Manhattan);
- 当  q=2  时,  $d_{i j}(2)=\left[\sum_{k=1}^{p}\left(x_{i k}-x_{j k}\right)^{2}\right]^{\frac{1}{2}}$ , 称为[[欧氏距离]] (Euclidean);
- 当  $q=\infty$  时,  $d_{i j}(\infty)=\max _{1 \leqslant k \leqslant p}\left|x_{i k}-x_{j k}\right|$ , 称为[[切比雪夫距离]] (Maximun)。
#### (2) 马氏距离（Mahalanobis):

$$d_{i j}(M)=\left(x_{i}-x_{j}\right)^{\prime} \Sigma^{-1}\left(x_{i}-x_{j}\right)$$

其中，  $x_{i}$  为样品  i  的  p  个指标组成的行向量,  $\sum$  为协方差矩阵。
- 优点：马氏距离既排除了各指标间的相关性干扰, 又消除了各指标的量纲。
- 缺点: 样品协方差矩阵在聚类过程中不变, 这点不合理。
#### (3) 兰氏距离 ( Canberra)：

$$d_{i j}(L W)=\frac{1}{p} \sum_{k=1}^{p} \frac{\left|x_{i k}-x_{j k}\right|}{x_{i k}+x_{j k}} \quad\left(x_{i j}>0\right)$$

下面是欧氏和马氏距离算出的距离相似的矩阵。

#### 距离矩阵计算函数 dist() 的用法
```
dist(x , method  =  "euclidean", diag=FALSE, upper=FALSE, p=2) 
```
 
 - x:  为数据矩阵,数据框架
- method: 为计算方法,
	- 包括" euclidean" ," maximum" , manhattan" , canberra" ," binary" or "minkowski"
- diag: 为是否包含对角线元素, upper: 为是否需要上三角,  p  : 为 Minkowski 距离的幂次

![[Pasted image 20230411223918.png]]

### 2.相似系数
对两个变量之间的相似程度可用相似系数来刻画, 用 $C_{i j}$  表示第  i  个变量与第  j  个变 量之间的相似系数。  $C_{i j}$  的绝对值越接近 1 , 表示指标  i  与指标  j  的关系越密切;  $C_{i j}$  的绝对 值越接近 0 , 表示指标  i  与指标  j  的关系越疏远。常用的相似系数有：
#### （1）夹角余弦:

$$C_{i j}(1)=\frac{\sum_{k=1}^{n} x_{k i} x_{k j}}{\left[\left(\sum_{k=1}^{n} x_{k i}^{2}\right)\left(\sum_{k=1}^{n} x_{k j}^{2}\right)\right]^{\frac{1}{2}}}$$

#### (2) 相关系数:

$$$C_{i j}(2)=\frac{\sum_{k=1}^{n}\left(x_{k i}-\bar{x}_{i}\right)\left(x_{k j}-\bar{x}_{j}\right)}{\sqrt{\sum_{k=1}^{n}\left(x_{k i}-\bar{x}_{i}\right)^{2} \sum_{k=1}^{n}\left(x_{k j}-\bar{x}_{j}\right)^{2}}}$$

3. 距离和相似系数之间的转换
一般来说, 距离越小, 两样品之间关系越密切; 而相似系数越大, 两变量之间关系 越密切。为了聚类分析方便起见, 可以用下面的通用公式得到变量间的距离:

$$d_{i j}^{2}=1-C_{i j}^{2}$$

## 7.3 系统聚类法
### 7.3. 1 系统聚类法的基本思想
确定了距离和相似系数后就要进行分类。分类有许多种方法, 最常用的一种方法是 在样品距离的基础上定义类与类之间的距离。首先将  n  个样品分成  n  类, 每个样品自成 一类, 然后每次将具有最小距离的两类合并, 合并后重新计算类与类之间的距离, 这个 过程一直持续到将所有的样品归为一类为止, 并把这个过程画成一张聚类图, 参照聚类图 可方便地进行分类。因为聚类图很像一张系统图, 所以这种方法就叫系统聚类法 (hierachical clustering method)。系统聚类法是目前在实际中使用最多的一种方法。从上面的分析 可以看出, 虽然我们已给出了计算样品之间距离的方法, 但在实际计算过程中还要定义 类与类之间的距离。定义类与类之间的距离也有许多方法, 不同的方法就产生了不同的 系统聚类方法, 常用的有如下六种：
（1）最短距离法: 类与类之间的距离等于两类最近样品之间的距离。
（2）最长距离法: 类与类之间的距离等于两类最远样品之间的距离。
(3) 类平均法: 类与类之间的距离等于各类元素两两之间的平方距离的平均。
(4) 重心法: 类与类之间的距离定义为对应这两类重心之间的距离。对样品分类来 说，每一类的类重心就是该类样品的均值。
（5）中间距离法：最长距离法夸大了类间距离, 最短距离法低估了类间距离。介于 两者间的距离法即为中间距离法, 类与类之间的距离既不采用两类之间最近距离, 也不 采用最远距离，而是采用介于最远和最近之间的距离。
（6）离差平方和法 (Ward 法)：基于方差分析的思想, 如果分类正确, 同类样品之 间的离差平方和应当较小，类与类之间的离差平方和应当较大。
结合  \mathrm{R}  语言，本书只给出常用的六种方法。
### 7.3.2 系统聚类法的计算公式
#### 1.最短距离法
该法用  $D_{k}(p, q)=\min \left\{d_{i j} \mid i \in G_{p}, j \in G_{q}\right\}$  来刻画类  $G_{p}$  与类  $G_{q}$  中最临近的两个样品的 距离。
若类  $G_{p}$  与类  $G_{q}$  合并为  $G_{r}$ , 则  $G_{r}$  与其他类  $G_{s}$  的距离为:

$$D_{k}(r, s)=\min \left\{D_{k}(p, s), D_{k}(q, s)\right\}$$

![[Pasted image 20230411231032.png]]

#### 2. 最长距离法
该法用  $D_{k}(p, q)=\max \left\{d_{i j} \mid i \in G_{p}, j \in G_{q}\right\}$  来刻画类  $G_{p}$  与类  $G_{q}$  中最远的两个样品 的距离。
若类  $G_{p}$  与类  $G_{q}$  合并为  $G_{r}$ , 则  $G_{r}$  与其他类  $G_{s}$  的距离为:

$$D_{k}(r, s)=\max \left\{D_{k}(p, s), D_{k}(q, s)\right\}$$
![[Pasted image 20230411231300.png]]

#### 3.类平均法
将两类之间的距离平方定义为这两类元素两两之间的平方距离的平均来计算距 离，即

$$D^{2}(k, r)=\frac{1}{n_{k} n_{r}} \sum_{r \in C_{k} j \in G_{r}} d_{i j}^{2}=\frac{1}{n_{k} n_{r}}\left(\sum_{i \in G_{k} j \in C_{p}} \sum_{i j}^{2}+\sum_{i \in G_{k} j \in C_{q}} d_{i j}^{2}\right)$$

其递推公式为:

$$D^{2}(k, r)=\frac{n_{p}}{n_{r}} D^{2}(k, p)+\frac{n_{q}}{n_{r}} D^{2}(k, q)$$

#### 4.重心法
在样本空间中, 一个类用它的重心 (即该类样品的均值）作为代表较为合理, 类与 类之间的距离就用重心之间的距离来表示。
设样品之间的距离用欧氏距离, 若类  $G_{p}$  与类  $G_{q}$  合并为  $G_{r}$  后, 它们各有  $n_{p} 、 n_{q}  、  n_{r}\left(n_{r}=n_{p}+n_{q}\right)$  个样品, 它们的重心用  $\bar{x}_{p} 、 \bar{x}_{q}$  和  $\bar{x}_{r}$  表示, 显然,  $\bar{x}_{r}=\frac{1}{n_{r}}\left(n_{p} \bar{x}_{p}+n_{q} \bar{x}_{q}\right)$ ,
某一类  $G_{k}$  的重心为  $\bar{x}_{k} , 它与新类  G_{r}$  的距离是:

$$D^{2}(k, r)=\left(\bar{x}_{k}-\bar{x}_{r}\right)^{\prime}\left(\bar{x}_{k}-\bar{x}_{r}\right)$$

其递推公式为:

$$D^{2}(k, r)=\frac{n_{p}}{n_{r}} D^{2}(k, p)+\frac{n_{q}}{n_{r}} D^{2}(k, q)-\frac{n_{p}}{n_{r}} \cdot \frac{n_{q}}{n_{r}} D^{2}(p, q)$$

#### 5.中间距离法
该方法是对最短距离法和最长距离法的折中, 即类间距离的递推公式为:
$$(当  G_{r}=\left\{G_{p}, G_{q}\right\}  )  D_{k r}^{2}=\frac{1}{2} D_{k p}^{2}+\frac{1}{2} D_{k q}^{2}-\frac{1}{4} D_{p q}^{2}$$ 
#### 6.离差平方和法（Ward 法)
该方法是 Ward 提出来的, 所以又称为 Ward 法。该方法的基本思想来自方差分析, 如果分类正确, 同类样品的离差平方和应当较小, 类与类的离差平方和应当较大。具体 做法是先将  n  个样品各自成一类, 然后每次缩小一类, 每缩小一类, 离差平方和就会增 大, 选择使方差增加最小的两类合并, 直到所有的样品归为一类为止。
设将  n  个样品分成  k  类  G_{1}, G_{2}, \cdots, G_{k} , 用  X_{i t}  表示  G_{t}  中的第  i  个样品,  n_{t}  为  G_{t}  中样 品的个数,  \bar{X}_{t}  是  G_{t}  的重心, 则  G_{t}  的样品离差平方和为:

$$S_{t}=\sum_{i=1}^{n_{t}}\left(X_{i t}-\bar{X}_{t}\right)^{\prime}\left(X_{i t}-\bar{X}_{t}\right)
$$

如果  $G_{p}$  和  $G_{q}$  合并为新类  $G_{r}$ , 类内离差平方和分别为:

$$\begin{array}{l}
S_{p}=\sum_{i=1}^{n_{p}}\left(X_{i p}-\bar{X}_{p}\right)^{\prime}\left(X_{i p}-\bar{X}_{p}\right) \\
S_{q}=\sum_{i=1}^{n_{q}}\left(X_{i q}-\bar{X}_{q}\right)^{\prime}\left(X_{i q}-\bar{X}_{q}\right) \\
S_{r}=\sum_{i=1}^{n_{r}}\left(X_{i r}-\bar{X}_{r}\right)^{\prime}\left(X_{i r}-\bar{X}_{r}\right)
\end{array}$$

它们反映了各自类内样品的分散程度, 如果  $G_{p}  和  G_{q}$  这两类相距较近, 则合并后所 增加的离差平方和  $S_{r}-S_{p}-S_{q}$  应较小; 否则, 应较大。于是定义  $G_{p}$  和  $G_{q}$  之间的平方距 离为:

$$D_{p q}^{2}=S_{r}-S_{p}-S_{q}$$

其中,  $G_{r}=G_{p} \cup G_{q}$ , 可以证明类间距离的递增公式为:

$$D_{k r}^{2}=\frac{n_{k}+n_{p}}{n_{r}+n_{k}} D_{k p}^{2}+\frac{n_{k}+n_{q}}{n_{r}+n_{k}} D_{k q}^{2}-\frac{n_{k}}{n_{r}+n_{k}} D_{p q}^{2}$$

这六种系统聚类法的并类原则和过程完全相同, 不同之处在于类与类之间的距离定 义。当采用欧氏距离时, Lance 和 Williams 于 1967 年将这些方法统一成如下的递推公式:

$$D_{p q}^{2}=\alpha_{r} D_{r q}^{2}+\alpha_{s} D_{s q}^{2}+\beta D_{r s}^{2}+\gamma\left|D_{r q}^{2}-D_{s q}^{2}\right|$$

$$表  7-2 \quad 递推公式的参数表$$
$$\begin{array}{ccccc}
\hline \text { 方法 } & \alpha_{r} & \alpha_{s} & \beta & \gamma \\
\hline \text { (1) 最短距离法 ( single) } & 1 / 2 & 1 / 2 & 0 & -1 / 2 \\
\text { (2) 最长距离法 (complete) } & 1 / 2 & 1 / 2 & 0 & 1 / 2 \\
\text { (3) 类平均法 ( average } & \frac{n_{r}}{n_{p}} & \frac{n_{s}}{n_{p}} & 0 & 0 \\
\text { (4) 中间距离法 (median) } & 1 / 2 & 1 / 2 & -1 / 4 & 0 \\
\text { (5) 重心法 (centroid) } & \frac{n_{r}}{n_{p}} & \frac{n_{s}}{n_{p}} & -\alpha_{r} \alpha_{s} & 0 \\
\text { (6) 离差平方和法 (ward) } & \frac{n_{q}+n_{r}}{n_{q}+n_{p}} & \frac{n_{q}+n_{s}}{n_{q}+n_{p}} & -\frac{n_{q}}{n_{q}+n_{p}} & 0 \\
\hline
\end{array}$$

### 7.3.3 系统聚类法的基本步骤
(1) 计算  n  个样品两两间的距离  $\left\{d_{i j}\right\}$ , 记作  D  。
(2) 构造  n  个类, 每个类只包含一个样品。
(3) 合并距离最近的两类为一个新类。
(4) 计算新类与当前各类的距离, 若类个数为 1 , 转到步骤 (5), 否则回到 步骤 (3)。
(5) 画聚类图。
(6) 决定类的个数和类。

### 系统聚类函数 hclust() 的用法
```
helust  (d, method  =  " complete", ...) 
```

d 为相似矩阵
method 为系统聚类方法, 包括" ward" ," single" ," complete" ," average" ," mequitty" ," median" or" centroid".

下面应用例  7-1  的数据进行系统聚类。
#### 1.最短距离法（采用欧氏距离）
开始有五类, 即每个样品自成一类  \left\{G_{1}, G_{2}, G_{3}, G_{4}, G_{5}\right\} , 这五类之间的距离就等于 5 个样品之间的距离, 距离阵记为  D_{0} , 其最小元素是  D_{0}(4,5)=1.00 , 故将类  G_{4}  和  G_{5}  合 并成一新类  G_{6}=\left\{G_{4}, G_{5}\right\} , 然后计算  G_{6}  与  G_{1} 、 G_{2} 、 G_{3}  之间的距离。
应用公式  D_{1}(6, i)=\min \left\{D_{0}(4, i), D_{0}(5, i)\right\} , 求其最近相邻的距离是:

$$\begin{array}{l}
D_{1}(6,1)=\min \left\{D_{0}(4,1), D_{0}(5,1)\right\}=\{2.23,1.41\}=1.41 \\
D_{1}(6,2)=\min \left\{D_{0}(4,2), D_{0}(5,2)\right\}=\{4.12,5.09\}=4.12 \\
D_{1}(6,3)=\min \left\{D_{0}(4,3), D_{0}(5,3)\right\}=\{4.24,5.00\}=4.24
\end{array}$$

![[Pasted image 20230411232729.png]]

同理可得其他类与类之间的距离: 

![[Pasted image 20230411232815.png]]

最后将其绘成系统图。

![[Pasted image 20230411232910.png]]

#### Ward 法 (采用欧氏距离)
![[Pasted image 20230411233006.png]]

【例 7-2】( (续例 3-1) 为了研究我国 31 个省、市、自治区 2007 年的城镇居民生活 消费的分布规律, 根据调查资料作区域消费类型划分。指标及原始数据见表 3-1。
为了对系统聚类法有一个全面的了解, 我们将各种聚类方法进行对比分析, 从中确 定最好的聚类结果。
用  \mathrm{R}  语言把我国 31 个省、市、自治区消费类型进行分类, 下列图是采用欧氏距离, 分别用最短距离法、最长距离法、类平均法、中间距离法、重心法和Ward 法得出的有关 数据和系统图。
由下列图可以看到, 不同方法的分类不完全一样。这也说明目前聚类方法还不够成 熟。为了便于对照, 将六种方法的分类结果综合列于表  7-3  。
从直观上看, 最短距离法分类效果较差, 最长距离法和 Ward 法分类效果较好。总的 可以分为三类: 北京、上海、广东、浙江为一类, 视为高消费地区; 其余 26 个省份（不 包括西藏, 西藏情况比较特殊, 自成一类）归为一大类, 视为中低消费地区一-可将该 类进一步分类, 分为中等消费地区和低消费地区。除此, 最长距离法和类平均法的分析 结果基本上是相同的。
由于  \mathrm{R}  语言的系统聚类函数选项较多, 所以我们编制了一个方便的函数进行快速聚 类分析, 下面建立一个系统聚类分析的函数来进行各种距离 (distance) 和方法 ( method) 的聚类。

#### 自编系统聚类函数 H.clust() 的用法

```
H. clust  <-  function  (X, d=  " euc", m=  " comp", proc=F, plot  =T) 
```
- X : 数值矩阵或数据框,  
- d : 距离计算方法 (见上), 
-  m  : 系统聚类方法 (见上) 
- proc: 是否输出聚类过程, 
- plot: 是否输出聚类图

![[Pasted image 20230419210559.png]]

```
library (mvstats)
H. clust(d7. 2, " euclidean", "single", plot = T) #最短距离法
```

![[Pasted image 20230419214705.png]]

```
H. clust( d7. 2, " euclidean", "complete", plot = T) #最长距离法
```

![[Pasted image 20230419214804.png]]

```
H. clust ( d7. 2, "euclidean" ," median", plot = T) #中间距离法
```

![[Pasted image 20230419220836.png]]

```
H. clust ( d7. 2, "euclidean" ," median", plot = T) #中间距离法
```

![[Pasted image 20230419221903.png]]

```
H. clust(d7.2 , "euclidean", " average", plot =T) #类平均法 
```

![[Pasted image 20230419221918.png]]

```
H. clust(d7. 2, "euclidean", "centroid", plot = T) #重心法
```

![[Pasted image 20230419222007.png]]

```
H. clust(d7.2, "euclidean", "ward", plot =T) #ward 法
```

![[Pasted image 20230419222147.png]]

综合考虑以上的分析结果, 笔者认为, 从我国各省、市、自治区的消费情况来看, 分为四类较为合适。
$$表  7-3 按类整理聚类图结果$$

![[Pasted image 20230419222337.png]]
![[Pasted image 20230419222351.png]]
![[Pasted image 20230419222439.png]]

从表 7-3 可以看出, 北京、上海、浙江、广东、天津、江苏、福建七个省、市的消 费水平与其他省、市、自治区有较显著的差异，这是符合实际情况的。
关于系统聚类分析方法详细分析见本章案例代码。
## 7.4 kmeans 聚类法
### 7.4.1 kmeans 聚类的概念
系统聚类法需要计算出不同样品或变量的距离, 还要在聚类的每一步都计算 “类间 距离”, 相应的计算量自然比较大。特别是当样本的容量很大时, 需要占据非常大的计算 机内存空间, 这给应用带来一定的困难。而 kmeans 法是一种快速聚类法, 采用该方法得 到的结果比较简单易懂, 对计算机的性能要求不高, 因此应用也比较广泛。
kmeans 法 (K 均值法) 是麦奎因 (MacQueen，1967) 提出的, 这种算法的基本思想是 将每一个样品分配给最靠近中心 (均值) 的类中, 具体的算法至少包括以下三个步骤:
(1) 将所有的样品分成  k  个初始类。
(2) 通过欧氏距离将某个样品划人离中心最近的类中, 并对获得样品与失去样品的 类重新计算中心坐标。
(3) 重复步骤 (2), 直到所有的样品都不能再分类为止。
kmeans 法和系统聚类法一样, 都是以距离的远近亲疏为标准进行聚类的。但是两者 的不同之处也很明显: 系统聚类对不同的类数产生一系列的聚类结果, 而  \mathrm{K}  均值法只能 产生指定类数的聚类结果。具体类数的确定, 离不开实践经验的积累。有时也可借助系 统聚类法, 以一部分样本为对象进行聚类, 其结果作为  \mathrm{K}  均值法确定类数的参考。

![[Pasted image 20230419223240.png]]

根据聚类中的均值进行聚类划分的 kmeans 算法如下:
(1) 从  n  个数据对象中取任意  k  个对象作为初始簇中心。
(2) 循环下述流程（3）到（4），直到每个聚类不再发生变化为止。
(3) 根据每个簇中对象的均值 (中心对象), 计算每个对象与这些中心对象的距离, 并根据最小距离重新对相应对象进行划分。
(4) 重新计算每个 (有变化) 簇的均值。
#### 快速聚类函数 kmeans() 的用法

```
kmeans(x, centers, ...)
x: 数据矩阵或数据框; centers: 聚类数或初始聚类中心
```

【例 7 -3】kmeans 算法的  \mathrm{R}  语言实现及模拟分析。
本例模拟正态随机变量  x \sim N\left(\mu, \sigma^{2}\right)  。
(1) 首先, 用  R  模拟 1000 个均值为 0 、标准差为 0.3 的正态分布随机数, 再把这些 随机数转化为 10 个变量、 100 个对象的矩阵; 其次, 用同样的方法模拟 1000 个均值为 1、标准差为 0.3 的正态分布随机数, 再转化为 10 个变量、 100 个对象的矩阵; 再次, 把 这两个矩阵合并为 10 个变量、200 个样本的数据矩阵; 最后，利用 kmeans 聚类法将其聚 成两类，观察其聚类效果如何。R 程序如下:

```
x1=matrix(rnorm(1000, mean=0,sd=0.3), ncol=10)
#  均值为 1 , 标准差为 0.3 的  100 \times 10  的正态随机数矩阵,
x2=matrix(rnorm(1000, mean =1, sd=0.3), ncol=10)
x=rbind(x1, x2) 
H. clust (x,"euclidean", "complete")
```

![[Pasted image 20230419224631.png]]

```
cl=kmeans(x, 2) #kmea聚类 
```

$\mathrm{K}  - means clustering with 2 clusters of sizes 100,100$

$Cluster means:$

$\begin{array}{ccccccc} & {[, 1]} & {[, 2]} & {[, 3]} & {[, 4]} & {[, 5]} & {[, 6]} \\ 1 & 0.92509363 & 0.979628438 & 1.013173661 & 0.98166417 & 1.00774076 & 1.01366905 \\ 2 & -0.01438657 & 0.002553707 & 0.007182876 & 0.02216878 & -0.01795538 & 0.02148843 \\ & {[, 7]} & {[, 8]} & {[, 9]} & {[, 10]} & & \\ 1 & 0.958660594 & 0.953604039 & 0.98721019 & 1.00988537 & & \\ 2 & -0.003497503 & -0.002056875 & -0.03691529 & 0.02797555 & & \end{array}$ 

Clustering vector:
![[Pasted image 20230419225116.png]]

$Within cluster sum of squares by cluster:$

$[1] 94.29468 88.84025$

$(between_SS/total_SS = 72.5\%)$

$Available components :$

$[1] "cluster" "centers" "totss" withinss" "tot. withinss"$
$[6] "betweenss" " size"$


```
pch1 = rep(" 1 ", 100)
pch2=rep(" 2 ", 100)
plot(x, col=cl$cluster, pch=c(pch1, pch2), cex=0.7) 
points(cl$centers,col=3, pch=" * ", cex=3)
```

![[Pasted image 20230419225610.png]]

从聚类结果来看,  kmeans  聚类方法可以准确地把均值为 0 和均值为 1 的两类数据分 类。图中的“  * ”  分别是两类的聚类中心。
(2) 为了显示 kmeans 方法对大样本数据的优势, 我们再模拟 10000 个均值为 0 、标 准差为 0.3 的正态分布随机数, 把这些随机数转化为 10 个变量、 1000 个对象的矩阵; 然后再用同样的方法模拟 10000 个均值为 1 、标准差为 0.3 的正态分布随机数, 转化为 10 个变量、1000 个对象的矩阵; 接着把这两个矩阵合并为 10 个变量、2000 个样本的数 据矩阵, 再利用 kmeans 聚类方法聚成两类, 观察其聚类效果如何。

```
x1=matrix(rnorm(10000, mean=0, sd=0.3), ncol =10) 
#均值为1, 标准差为0.3 的1000 *10 的正态随机数矩阵 
x2=matrix(rnorm(10000, mean=1, sd=0.3), ncol=10) 
x=rbind(x1, x2) 
cl=kmeans(x, 2) # 聚类 
pch1=rep("1", 1000) 
pch2=rep("2", 1000) 
plot(x,col=cl$cluster}, pch=c(pch1,pch2), cex=0.7) 
points(cl$centers, col =3, pch=" * ", cex=3)
```

![[Pasted image 20230419232501.png]]

从聚类结果来看, kmeans 聚类方法可以完全准确地把均值为 0 和均值为 1 的两类数 据分类。图中的 “*” 分别是两类的聚类中心。这里请不要使用系统聚类法, 因为有可 能造成电脑死机。

## 7.5 聚类分析的一些问题
### 1.系统聚类分析的一些特点
系统聚类分析方法与传统的统计分组方法相比具有如下特点:
(1) 综合性：聚类分析可以利用多个变量的信息对样品进行分类, 克服单一指标分 类的弊端。
（2）形象性: 聚类分析可以利用聚类图直观地表现其分类形态以及类与类之间的内在 关系。
（3）客观性：聚类分析的结果克服了主观因素，比传统分类方法更客观、细致、全 面和合理。
### 2.关于 kmeans 算法
kmeans 算法只有在类的平均值被定义的情况下才能使用，这可能不适用于某些应 用。例如, 涉及有分类属性的数据, 要求用户必须事先给出  k  (要生成的类的数目)。这 可以算是该方法的一个缺点。另外，kmeans 算法不适合分析非凸面形状的类或者大小差 别很大的类。而且, 它对于 “噪声” 和孤立点数据是敏感的, 少量的该类数据能够对均 值产生极大的影响。
kmeans 算法有很多变种。它们可能在初始  k  个平均值的选择、相异度的计算和计算 聚类平均值的策略上有所不同。经常会产生较好的聚类结果的一个有趣策略是：首先采 用层次的凝聚算法, 决定结果类的数目, 并找到一个初始的聚类, 然后用迭代重新定位 来改进聚类结果。
### 3. 关于变量变换
在实际问题中, 不同的变量一般取的量纲不同, 为了使不同的量纲也能放在一起比 较, 通常需要对数据作一些变换, 有时即使变量用的是同一量纲, 为了使数据更适用某 种数学模型, 也需要对数据进行变换, 常用的变换有 :
(1) 平移变换：将某一个指标的数据同减去一个数，一般是减去均值。
（2）极差变换：将某一个指标的数据同除以该指标的极差。
（3）标准差变换：将某一个指标的数据同除以该指标的标准差。
（4）主成分变换： 将数据用它们的主成分代替, 有时为了简化, 只取前几个主成分, 或舍去次要的主成分。
(5) 对数变换：将数据取对数，当数据之间数量级相差较大时常采用这一变换。 以上的变换有时同时采用, 例如将数据标准化, 就是先作变换 (1), 后作变换 (3)。
### 4.聚类分析总结
(1) 聚类分析根据分类对象不同分为  Q  型和  \mathrm{R}  型聚类分析。
（2）通常测量变量有三种尺度：间隔尺度、有序尺度和名义尺度, 其中, 间隔尺度 使用得最多，本章主要讨论这种尺度。
(3) 距离和相似系数这两个概念反映了样品（或变量）之间的相似程度。相似程度 越高, 一般两个样品（或变量) 间的距离就越小或相似系数的绝对值就越大; 反之, 相 似程度越低, 一般两个样品（或变量）间的距离就越大或相似系数的绝对值就越小。
(4) 系统聚类法是最常用的一种聚类方法, 常用的系统聚类方法有最短距离法、最 长距离法、中间距离法、类平均法、重心法、离差平方和法等。


